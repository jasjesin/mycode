============================================================================================
[Day 1]  Sunday    : 01/22 : 4hrs    | 4    hrs | Core Concepts
[Day 2]  Monday    : 01/23 : 4.5hrs  | 8.5  hrs | Scheduling & Application Lifecycle Mgmt
[Day 3]  Tuesday   : 01/24 : 1.5hrs  | 10   hrs | Cluster Maintenance
[Day 4]  Wednesday : 01/25 : 4hrs    | 14   hrs | Security
[Day 5]  Thursday  : 01/26 : 1hr     | 15   hrs | Storage
[Day 6]  Friday    : 01/27 : 2hrs    | 17   hrs | Networking
[Day 7]  Saturday  : 01/28 : 1hr     | 18   hrs | Networking
[Day 8]  Sunday    : 01/29 : 1hr     | 19   hrs | Networking, Design/Install Cluster
[Day 9]  Monday    : 01/30 : 2hrs    | 21   hrs | Install Cluster wid kubeadm, Troubleshooting, Other Topics
[Day 10] Tuesday   : 01/31 : 2hrs    | 23   hrs | Mock Exams & revision & Exam in evening
[Day 11] Wednesday : 02/01 :         |          | revision & CKA Exam in evening
============================================================================================

Use the code - DEVOPS15 - while registering for the CKA or CKAD exams at Linux Foundation to get a 15% discount.
Certified Kubernetes Administrator: https://www.cncf.io/certification/cka/
Exam Curriculum (Topics): https://github.com/cncf/curriculum
Candidate Handbook: https://www.cncf.io/certification/candidate-handbook
Exam Tips: http://training.linuxfoundation.org/go//Important-Tips-CKA-CKAD
https://github.com/mmumshad/kubernetes-the-hard-way

We have created a repository with notes, links to documentation and answers to practice questions here.
Please make sure to go through these as you progress through the course:
https://github.com/kodekloudhub/certified-kubernetes-administrator-course

[] Core Concepts:
[] CA -- Cluster Architecture -- overall arch abt k8s system components in ctrl plane as well as worker nodes
1. ETCD for Beginners
ETCD -- simple,reliable key-value store, tht is simple, secure & fast (kind of NoSQL DB ?)
Key-value store -- stores info in the form of documents/files or pages, changes to 1 file doesn't impact others


jasdil@JasDiLMacAir bin % etcd --version
etcd Version: 3.5.7
Git SHA: 215b53cf3
Go Version: go1.19.5
Go OS/Arch: darwin/amd64
jasdil@JasDiLMacAir bin %

jasdil@JasDiLMacAir bin % etcdctl version
etcdctl version: 3.5.7
API version: 3.5
jasdil@JasDiLMacAir bin %

jasdil@JasDiLMacAir bin % etcdctl put key1 value1
OK
jasdil@JasDiLMacAir bin % etcdctl get key1
key1
value1
jasdil@JasDiLMacAir bin % etcdctl put jas dil srt sdk
OK
jasdil@JasDiLMacAir bin % etcdctl get jas
jas
dil
jasdil@JasDiLMacAir bin %

etcd stores info abt:
- Nodes
- Pods
- Configs
- Secrets
- Accounts
- Roles
- Bindings

each info u see, upon running kubectl get cmd, is fetched from etcd
each change made in cluster like
- adding additional nodes
- deploying pods or replicaSets etc
is noted/updated in etcd svr. Change is considered completed only once its updated in etcd svr

--advertise-client-urls is the address (IP of svr) on which etcd listens at port 2379 (default port)
https://{Internal_IP}:2379 <= should b registered in API Svr wen it tries to reach etcd

wen k8s is setup using kubeadm svr, then it deploys etcd as a pod in kube-system namespace
u can explore DB using etcdctl utility within that pod

to list all keys stored by k8s, run following cmd on master/ctrl svr >>
kubectl exec etcd-master -n kube-system etcdctl get / --prefix -

k8s stores data in specific directory structure
root dir. is registry n under that u hv various constructs like
- minions
- pods
- replicasets
- deployments
- roles
- secrets

/registry/apiregistration.k8s.io/apiservices/v1
/registry/apiregistration.k8s.io/apiservices/v1.apps
/registry/apiregistration.k8s.io/apiservices/v1.authentication.k8s.io
/registry/apiregistration.k8s.io/apiservices/v1.authorization.k8s.io
/registry/apiregistration.k8s.io/apiservices/v1.autoscaling
/registry/apiregistration.k8s.io/apiservices/v1.batch
/registry/apiregistration.k8s.io/apiservices/v1.networking.k8s.io
/registry/apiregistration.k8s.io/apiservices/v1.rbac.authorizations.k8s.io
/registry/apiregistration.k8s.io/apiservices/v1.storage.k8s.io
/registry/apiregistration.k8s.io/apiservices/v1.admissionregistration.k8s.io

Since ETCD is setup as HA, controller nodes run at port 2380 by default
  etcd nodes must know abt each other, by specifying details abt them at >>
  --initial-cluster controller-0=https://{CONTROLLER0_IP}:2380 controller-1=https:{CONTROLLER1_IP}:2380

ETCDCTL is the CLI tool used to interact with ETCD
ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to
To set the right version of API set the environment variable ETCDCTL_API command
export ETCDCTL_API=3
Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:
--cacert /etc/kubernetes/pki/etcd/ca.crt
--cert /etc/kubernetes/pki/etcd/server.crt
--key /etc/kubernetes/pki/etcd/server.key
So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:
kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key"

2. kube-api svr/ API Server:
wen kubectl cmd, like kubectl get nodes, is executed:
- kubectl utility reaches to API svr
- Kube API Svr authenticates the request & validates it
- It then retrieves the needed data from etcd cluster
- n then kube API svr responds back to requested info asked in kubectl cmd

Another way insteading of using kubectl cmd each time, can invoke APIs directly by sending POST request like this >>
- user issues API request directly to API svr wid auth details like x-et-auth etc n payload (if needed)
- API svr validates request
- retrieves data from ETCD cluster
- responds back wid action/data

Example: creation of a pod
curl -X POST /api/v1/namespaces/default.pods/ ...[other]
In this case, API svr
- creates a pod object without assigning it to a node
- updates info in etcd svr
- updates user tht pod is created
Scheduler
- continously monitors API svr n realizes tht thrs a new pod wid no node assigned
- identifies right node, based on rank, to place the new pod on
- communicates new node details for new pod, back to API svr
API svr then
- updates this new information in etcd cluster
- passes that info, abt creation of new pod, to kubelet of the new worker node
kubelet then
- creates the pod on the node
- instructs container runtime engine, like docker, to deploy the application image
- once that is done, kubelet updates the status back to api svr
API svr then
- updates data back in etcd cluster

Similar pattern is followed each time a new change is requested
kube API svr is at the center of all the diff. tasks tht need to be performed, to make the change in the cluster

To summarize, API svr is responsible for:
- authenticating n validating the request
- retrieving & updating data in etcd data store cluster
- - API svr is the only component tht interacts wid etcd data store
- - Other components like kube controller mgr, scheduler & kubelet, use API svr to perform updates in the cluster in their respective areas

If u bootstrap ur cluster using kubeadm tool but if u setup the hard way, then API svr is avl as a binary in k8s release page
in hard way, all details for API svr r avl at /etc/systemd/system/kube-apiserver.service
thru kubeadm, u can just execute kubectl get pods -n kube-system
another way is to see running process n see all options configured, by executing ps -eaf | grep kube-apiserver on master node

3. kube-controller-manager / Controller Mgr:
manages various controllers in k8s.
controller is a process tht
- continously monitors the state of various components within the system
- works towards bringing the whole system to the desired functioning state

Ex: node ctrller is reponsible for
- monitoring status of nodes
- taking necessary actions to keep the applications running
it does that thru API Svr
Node ctrller tests/chks status of nodes every 5 secs
Tht way, node ctrller can monitor health of nodes
if it stops recving heartbeat from a node, the node is marked as unreachable
node ctrller waits for 40 secs, before marking a node as unrechable
After a node is marked unreachable, node ctrller gives it 5 minutes to come back up
If node doest comes back up in 5 mins, then node ctrller
- removes the pods assigned to that node
- provisions those pods to healthy ones, if the pods r part of a replicaSet

Node Ctrller reponsibilities in a nutshell
- WatchStatus
- RemediateSituation
- NodeMonitorPeriod=5s
- NodeMonitorGracePeriod=40s
- PodEvictionTimeout=5m

Next ctrller is replication ctrller, responsible for
- monitoring status of replicaSets
- ensuring that desired no. of pods r avl at all times within replicaSet
if a pod dies, it creates another one

Many different ctrllers avl inside ctrller mgr, like
- deployment ctrller
- namespace ctrller
- endpoint ctrller
- cronjob
- job ctrller
- PV-protection ctrller
- PV Binder ctrller
- svc account ctrller
- statefulSet
- replicaSet
- Replication ctrller
- node ctrller
Whtever intelligence is built into these contructs, is implemented wid these controllers
This is kind of brain behind lot of things in k8s

All these ctrllers r pkgd as k8s Ctrller Mgr, n get installed
we can customize or add more ctllers as additional options, to specify which ctrllers to enable
By default, all ctrllers r enabled, but we can choose to enable few
if any of ctrllers do not work or seem to exist, this would be a good starting point to look at
kubectl get pods -n kube-system, look for kube-controller-manager-master
options within pod definition file r avl under /etc/kubernetes/manifests/kube-c
non-kubeadm setup, options avl under /etc/systemd/system/kube-controller/manager.service
again u can chk all options configured under running processes as ps -eaf | grep kube-controller-manager

4. kube-scheduler/ Scheduler:
only responsible for deciding which pod goes to which node, thereby, makes sure right pod gets on right node, by making sure assigned node has sufficient capacity to accommodate that pod
it doesn't actually place the pod, thats the job of the kubelet
kubelet creates the pod on the node.
pods can hv diff. resource reqmnts, can hv nodes in the cluster, dedicated to specific apps
Scheduler looks @ each pod n tries to find best node for it, based on its CPU/memory etc resource reqmnts
Scheduler goes thru 2 phases to identify best node for a pod:
- filter out node that do not fit profile for new pod
- ranks nodes to identify best fit for the pod
- - uses a priority fn to assign a score to nodes, on a scale of 0 to 10
- - this is done by calculating amt of resources tht would be free on the node, after placing new pod on it
- - higher amt of free resources after placing new pod, higher the rank

Scheduler can b customized n u can write ur own scheduler as well, based on following considerations added for decision-making:
- resource reqmnts & limits
- taints & tolerations
- node selectors/affinity
- labels & selectors
- manual scheduling
- daemonSets
- multiple schedulers
- scheduler events

kubectl get pods -n kube-system, look for kube-scheduler
options within pod definition file r avl under /etc/kubernetes/manifests/kube-scheduler.yaml
again u can chk all options configured under running processes as ps -eaf | grep kube-scheduler


5. kubelet:
responsibilities:
- registers node wid k8s cluster
- wen it recvs instructions to load a new container/pod on the node
- - requests container runtime engine, like docker, to pull reqd image n run an instance
- continues to monitor state of pod & containers in it
- reports status to kube API Svr on a timely basis

kubelet is not automatically deployed wid kubeadm, as its not a static pod or part of ctrl plane or master setup, as its needed on worker nodes only.
alwys manually install kubelet on worker nodes
d/l installer, extract it & run it as a svc

6. kube-proxy:
within k8s cluster, each pod can reach evry other pod
This is accomplished by deploying a pod n/wing solution to cluster.
A Pod n/w is an internal virtual n/w tht spans across all nodes in cluster, to which all pods can connect to
thru this n/w pods r able to communicate wid each other
thr r many solutions avl for deploying such a n/w

pod can access another pod thru IP, but pods die frequently
so better way is to access app on pod via svc
svc can't join pod n/w as svc is not actual thing, its not a container like pods, so it doesnt has any interfaces or actively listening process
its a virtual component tht only lives in k8s memory
for svc to be accessible across the cluster from any of the nodes.
kube-proxy helps accomplish that

kube-proxy
- is a process tht runs on each node in k8s cluster
- its job is to look for new svcs
- - each time a new svc is created, kube-proxy creates appropriate rules on each node, to fwd traffic to those svcs to backend pods
- - kube-proxy accomplishes this by using/creating iptables rules on each nodes in cluster to fwd traffic heading to IP of svc to IP of actual pod
So thats how kube pxy configures a svc

kubeadm deploys kube pxy as pod on each node as a DaemonSet so a single pod is alwys deployed on each node in cluster


[] Pods:
k8s doesnt deploys containers directly on worker nodes. Containers r encapsulated into a k8s object called pods
Pod
- is a single instance of an app
- smallest object tht is created in k8s

best practice is to deploy a single container in a pod, but we r not restricted frm NOT deploying multiple containers in a pod
we can setup multi-container Pod but containers shouldn't b of same type.
Other container can be a helper container tht can do some kind of supporting task like processing a user & thr r processing r files uploaded by user etc

Pods help cover application for future architectural changes, like initially app may be deployed as single container but over long run, it may need 2nd container like helper container to do backoffice tasks

u can configure k8s to pull images frm pvt registry, instead of dockerhub set as default

k8s uses yaml files as inputs for creation of resources like pods, replicas, deployments, svcs etc

a k8s definition file follows similar structure for all resources n alwys contains following root/top-lvl mandatory/reqd fields/properties:
- apiVersion: k8s api version to b used to create the object, diff. values r v1, apps/v1, extensions/v1beta etc
- kind: refers to type of object to be created, like pod, replicaSet, deployment, service, secrets, configMap etc
- metadata: data abt obj, like name, labels etc, defined in the form of dictionary, making name, labels as children of metadata. name n labels r siblings, so, should hv same indentation. name is string value, labels is dictionary, wid any kind of key-value pairs within it..labels help filter pods for specific app, wid a type, if thr r 100s of them
- spec: container n image details, spec is a dictionary, wid containers added as a list for name & image property details specified for each container

kubectl get pods
kubectl describe pod <podID>


pod.yaml file >>
========================
apiVersion: v1
kind: Pod
metadata:
  name: test
  labels:
    type: frontend
spec:
  containers:
  - name: webserver
    image: nginx
========================

# Both the following commands work same way wen a new object needs to be created
kubectl create -f pod.yaml
kubectl apply -f pod.yaml

kubectl get pods -o wide
kubectl describe pod <podName|podID>

to create a yaml file, basic template for a pod creation, execute following command >>
kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx.yaml

27. -- 100 mins done

[] ReplicaSets:
Ctrllers r brain behind k8s
These r process tht monitor k8s objects & respond accordingly
Wht is a replica n wht is a replication ctrller
replication ctrllers
- helps run multiple instances of a single pod in k8s cluster, thereby, providing HA
- helps bring up new pod if existing pod fails
- ensures specified no. of pods run all the time
- helps share/balance load among multiple pods, thereby, providing load balancing
- spans across multiple nodes in the cluster
- also helps scale app wen demand increases/decreases, thereby providing scaling

replication ctrller & replicaSet: both share same purpose, but they r not same
replication ctrller is older tech, replcaed by replicaSet

in replicaSet's yaml definition, under spec, we provide template definition to define wht kind of pod needs to be created
add metada n spec section from pod to template here

ReplicationController kind has apiVersion set to v1
ReplicaSet kind has apiVersion set to apps/v1

ReplicaSet can also manage pods, tht were not created under it, by using selector field, where u can specify label name
so, all pods wid same label name will be monitored by replicaSet
replicaSet is a process tht monitors the pods

selector:
  matchLabels:
    type: backend

Labels & Selectors: help rectify specific pods to be monitored by replicaSet, out of 100s of pods in cluster

how to scale?

1. update replicase inside the rs.yaml file, upon scaling/increasing count of replicas, execute
kubectl replace -f rs.yaml # using replace instead of apply

2. CLI: kubectl scale --replicas=6 -f rs.yaml

3. kubectl scale rs app-replicaset -- replicas=6
                  TYPE        NAME of running rs

kubectl explain replicaset # or any object, to get details abt it including apiVersion to be used for it


[] Deployments:
same yaml as rs, only diff is kind value is changed from replicaSet to Deployment
Deployment can help in performing rolling updates, rollbacks, pause, make changes n play again etc

[] Certification Tip!
Here's a tip!
As you might have seen already, it is a bit difficult to create and edit YAML files. Especially in the CLI. During the exam, you might find it difficult to copy and paste YAML files from browser to terminal. Using the kubectl run command can help in generating a YAML template. And sometimes, you can even get away with just the kubectl run command without having to create a YAML file at all. For example, if you were asked to create a pod or deployment with specific name and image you can simply run the kubectl run command.
Use the below set of commands and try the previous practice tests again, but this time try to use the below commands instead of YAML files. Try to use these as much as you can going forward in all exercises
Reference (Bookmark this page for exam. It will be very handy):
https://kubernetes.io/docs/reference/kubectl/conventions/

Create an NGINX Pod
kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment
kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.
kubectl create -f nginx-deployment.yaml
OR
In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml

kubectl get po -o wide
kubectl get rs
kubectl get deploy
kubectl get svc
kubectl get ns

[] Svcs:
- enable communication b/w various components within & outside of app
- help us connect apps together wid other apps/users
- enable loose coupling among microsvcs in our app
- is an object, just like pod, deployment, rs etc, tht listens to a port on the node and
- - fwds request on that port to a port on the Pod, running the app
- - this type of svc is called as NodePort svc cuz svc listens to a port on the node & fwds requests to the pods

[] Types of Svcs:
- NodePort: svc makes internal port accessible on a port on the node, basically, exposes internal port to external world
- ClusterIP: svc creates a virtual IP inside the cluster, to enable communication b/w diff. svcs, all internal in cluster
- LoadBalancer: provisions a LB for app in supported cloud providers.
- - This helps distribute load across svrs in frontend tier

[] nodePort:
- 3 ports involved in this, tgtPort (port of pod), svc port (called simply as port) and port of node (NodePort, that is exposed to external world)
- - nodePorts can only b in a valid range of 30,000 to 32,767.
- - if we do not specify this, a free port from the above range is picked
- - if targetPort is not defined, then that is considered same as port (svc port)
- - ports is an array, with port being only mandatory property, rest two port key-values r optional
- svc is like a virtual svr inside the node, inside the cluster it has its own IP address & tht IP address is called ClusterIP of the svc
- use labels n selectors to link svc to pod/deployment & select all pods wid same label name, for load balancing
- even if pods r created across multiple nodes, k8s automatically expands svc across nodes to make app avl at same nodePort across all nodes
- wen pods r removed/added, svc is automatically updated, making it highly flexible & adaptive
- once created, u wont hv to typically make any additional cfg changes
- gives IP:port as the URL to access svcs, wid 1 IP for each port
- - if u hv 100 nodes, then u ll end up providing 100 URLs

[] clusterIP:
full stack app contains diff. pods hosting diff. parts of app, like no. of pods hosting frontend, some hosting backend etc
- provides single interface for frontend to load balance n connect to all pods of backend or DB, all internal to cluster
- this one uses just two ports, i.e., svc port & targetPort (port of svc running in pods)

[] Loadbalancer:
- provides single URL, integrates wid native LB of cloud platforms...this works only on cloud platforms, not on-prem

[] Namespace:
- helps isolate apps n make them co-exist in same cluster
- each svc created, gets registered in DNS, as <svcName>.<ns>.svc.cluster.local
                                                                      Domain
kubectl create namespace <nsName>
# to switch to another namespace
kubectl set-context $(kubectl config current-context) --namespace=<nsName>
kubectl get pods --namespace=<nsName>
# or
kubectl get pods --n=<nsName>
kubectl get pods --all-namespaces
kubectl get ns #lists all avl namespaces

k8s creates kube-system ns for all internal components to be created in it n to keep it separate frm default ns
k8s also creates kube-public ns for all components that can be exposed to external world.

contexts r used to manage multiple clusters & multiple environments from same mgmt system

to limit resources in a ns, create a resource quota, as a kind in yaml
compute-quota.yaml >>
=========================
apiVersion:v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi
=========================

[] Imperative vs Declarative Approach: kubectl apply -f <file>.yaml is declarative approach
apply is intelligent enuff to figure out if object alrdy exists or not

[] Certification Tips - Imperative Commands with Kubectl
While you would be working mostly the declarative way - using definition files, imperative commands can help
in getting one time tasks done quickly, as well as generate a definition template easily.
This would help save considerable amount of time during your exams.
Before we begin, familiarize with the two options that can come in handy while working with the below commands:
--dry-run: By default as soon as the command is run, the resource will be created.
If you simply want to test your command , use the --dry-run=client option.
This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
-o yaml: This will output the resource definition in YAML format on screen.
Use the above two in combination to generate a resource definition file quickly, that you can then modify and
create resources as required, instead of creating the files from scratch.

POD
Create an NGINX Pod
kubectl run nginx --image=nginx
Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml

Deployment
Create a deployment
kubectl create deployment --image=nginx nginx
Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
Generate Deployment with 4 Replicas
kubectl create deployment nginx --image=nginx --replicas=4
You can also scale a deployment using the kubectl scale command.
kubectl scale deployment nginx --replicas=4
Another way to do this is to save the YAML definition to a file and modify
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml
You can then update the YAML file with the replicas or any other field before creating the deployment.

Service
Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml
(This will not use the pods labels as selectors, instead it will assume selectors as app=redis.
You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set.
So generate the file and modify the selectors before creating the service)

Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml
(This will automatically use the pod's labels as selectors, but you cannot specify the node port.
You have to generate a definition file and then add the node port in manually before creating the service with the pod.)
Or
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml
(This will not use the pods labels as selectors)
Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot
accept a node port. I would recommend going with the kubectl expose command.
If you need to specify a node port, generate a definition file using the same command and manually input the nodeport
before creating the service.

Reference:
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands
https://kubernetes.io/docs/reference/kubectl/conventions/


# Create pod n svc in 1 cmd >>
controlplane ~ ➜  kubectl run httpd --image=httpd:alpine --port=80 --expose
service/httpd created
pod/httpd created

# cmd to create a svc n expose existing app within cluster on port 6379
kubectl expose pod redis -- port 6379 --name redis-service


[] kubectl apply: how it works
- takes into consideration
- - local cfg file
- - live object definition in k8s
- - last applied cfg
- n then it makes a decision on wht changes r to be made
- if object alrdy doesnt exists, apply creates it
- - if object exists, then it updates cfg changes
- apply uses yaml format of written cfg to compare wid live cfg, makes changes n then stores it as json format as last applied cfg
- for any future updates, all 3 r compared to identify wht changes need to be made
- last applied cfg used as last known state & helps compare b/w local file changes wid live cfg
- this last applied cfg is stored in k8s live cfg in k8s memory, as an annotation names last-applied-configuration, under metadata

[] Scheduling: customize n configure how a scheduler behaves
If there's no scheduler to monitor & schedule nodes, then pods stay in pending state
- Manual Scheduling: 3 diff. ways to manually schedule a pod
- - Assigning node name to a pod during Pod Creation Time. The following is NOT applicable for existing pods.
- - nodeName: By default, this field, under spec, is not set in yaml/manifest file. Easiest way to schedule a pod manually to a node.
- - - Scheduler goes through all the pods, that does NOT has this property set. Those are the candidates for scheduling.
- - - Scheduler then identifies the right node for the pod by running scheduling algorithm
- - - Once identified, it schedules the pod on the node, by setting nodeName property to name of the node, by creating a binding object
- - For existing pod, create a binding object & send a POST request to pod's binding API, thereby, mimicking wht actual scheduler does
- - - In the binding object, specify a tgt node, wid name of node,
- - - then send a POST request to pod's binding API, wid data set to binding object in JSON format
pod-bind-definition.yaml >>>
=============================
apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: <nodeName>
=============================
Then, convert yaml file to json format and specify content in data pmtr, n then execute >>
curl --header "Content-Type:application/json" --request POST --data '{"apiVersion": "v1", "kind": "Binding" ...}' http://${SERVER}/api/v1/namespaces/default/pods/${podName}/bin

- Labels & Selectors, & Annotations: standard method to group things together
- - Labels r properties attached to each object. Labels r similar to tags, starting wid #, used in youtube videos, blogs etc for metadata search
- - - We can label objects, under metadata in key-value format, based on application, functionality, type, environment (like integration n QA in non-PRD) etc
- - - for each object, we can add multiple labels, like app, type etc to filter those based on those labels later on.
- - - the more the labels defined, the more uniquely you can filter your needs in selector
- - Selectors, defined under spec, help u filter objects based on label specified
- - - k8s objects use labels & selectors internally to connect to diff. objects together
- - Annotations: defined under metadata, used for informatory purpose like comments to describe things better for future debugging
Ex:
annotations:
  buildVersioin: 1.34

kubectl get pods --selector type=frontend #<== to filter output
kubectl get pods --no-headers | wc -l #<== gives exact count

# for filtering wid multiple labels, use comma
kubectl get pods --selector env=prod,bu=finance,tier=frontend

56. 4:03 hrs

- taints & tolerations: helps define pod-to-node definition; & restrict wht pods r placed on wht nodes
- - taints n tolerations hv nothing to do wid security or intrusion on cluster; these r used to set restrictions on wht pods can b scheduled on a node
- - taint a node helps repel all pods wid no toleration or no matching toleration
- - adding a toleration to specific pods, help those pods gain toleration against tainted node
- - taints r set on nodes & tolerations r set on pods
- - wen k8s cluster is 1st setup, a taint is set on master node automatically, tht prevents any pods frm being scheduled on master node
to see this taint, execute >>
kubectl describe kubemaster | grep Taint

cmd >>
kubectl taint nodes <nodeName> key=value:taint-effect #taint-effect defines wht will happen to pods tht do not tolerate this taint
Ex:
kubectl taint nodes node1 app=blue:NoSchedule
kubectl taint nodes node1 app=blue:NoSchedule- #<== appending "-" removes taint

3 taint effects:
- NoSchedule: pods will not be scheduled on this node
- PreferNoSchedule: system will try to avoid placing a pod on this node, but thats not guaranteed
- NoExecute: no pods wont be scheduled, & existing pods, before taint was applied, will be evicted, if those do not tolerate the taint

in pod yaml, add tolerations under spec, as >>
=======================
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
=======================

kubectl run bee --image=nginx --dry-run -o yaml > bee.yaml #<== use this rather than creating yaml frm scratch
kubectl get po --watch #<== watches output continously

[] Node Selectors:
- serve purpose wid single label & selector, for more complex reqmnt, use nodeAffinity
- label a node 1st wid key-value pair, for tht label to be used in pod later on
kubectl label nodes <nodeName> <labelKey>=<labelValue>
Ex:
kubectl label nodes node1 size=Large
Now in pod's yaml, under spec,
==============================
  nodeSelector:
    size: Large
==============================


[] Node Affinity & anti-Affinity:
- provides us wid advanced capabilities to limit pod placement on specific nodes.
- taint only specifies restriction to accept specific pods but it doesn't guarantees that pod wid toleration will land on tainted node for sure
- if u want to ensure tht a specific pod goes ONLY to tainted node, use nodeAffinity
- a reqmnt like select a node tht is not small, or select a node that is small or medium, nodeSelector cant help, as it works wid 1 label only

To use nodeAffinity, following needs to be defined now in pod's yaml, under spec,
==============================
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: #<== this is type of node affinity,
#     prefferedDuringSchedulingIgnoredDuringExecution: #<== this is useful if labels do not match or specified label doesnt exists
#     IgnoredDuringExecution means tht after scheduling, if labels r removed, pod wont b impacted
#     prefferedDuringSchedulingRequiredDuringExecution: #<== pod will be removed if label is removed later on
        nodeSelectorTerms:
        - matchExpressions:
          - key: size
            operator: In
            values:
            - Large
            - Medium
# or
          - key: size
            operator: NotIn
            values:
            - Small
# or if size is not specified for small nodes by default, n only defined for large or medium nodes
          - key: size
            operator: Exists
==============================

[] Taints & Tolerations v/s Node Affinity:
- node affinity cannot repel non-confirming pods to nodes
- taints n tolerations can repel non-confirming pods to nodes, but cannot ensure that confirming pods alwys get scheduled to tainted nodes, those can go to non-tained nodes as well
- combo of both helps ensure confirming pod alwys goes to tainted node
- 1st apply taints n tolerations, n then apply node affinity wid node selector


[] Resource Limits:
- How resource requests & limits play a role
- default assumption of k8s for each pod, 0.5 CPU, 256MB memory.
- This min. amt of CPU & memory requested by container is called Resource Request for container
- Default limit set by k8s, is 1 vCPU & 512Mi for containers
- To customize the default values, add a section resources under containers as>>
====================
  resources:
    requests:
      memory: "1Gi"
      cpu: 1
    limits:
      memory: "2Gi"
      cpu: 2
====================

0.1 CPU is 100m (milli), u can go as low as 1m, but not lower than that
1 CPU = 1vCPU in AWS or 1 core in GCP or 1 hyper-thread in Azure
 important to set limit of resource usage on pods as docker container can consume entire resources on node

if limits r exceeded by pod/container, k8s throttles CPU so tht it doesnt go beyond specified limit.
a container cannot use more resources tht its limit for CPU
BUT, it CAN use more memory than its limit

if a pod uses more memory than its limit, continously, then that pod is terminated

For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.
==============================
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/
==============================
==============================
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
==============================

A quick note on editing PODs and Deployments
Edit a POD
Remember, you CANNOT edit specifications of an existing POD other than the below.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations
For example you cannot edit the environment variables, service accounts, resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:
1. Run the kubectl edit pod <pod name> command.  This will open the pod specification in an editor (vi editor). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.
A copy of the file with your changes is saved in a temporary location as shown above.
You can then delete the existing pod by running the command:
kubectl delete pod webapp
Then create a new pod with your changes using the temporary file
kubectl create -f /tmp/kubectl-edit-ccvrq.yaml

2. The second option is to extract the pod definition in YAML format to a file using the command
kubectl get pod webapp -o yaml > my-new-pod.yaml
Then make the changes to the exported file using an editor (vi editor). Save the changes
vi my-new-pod.yaml
Then delete the existing pod
kubectl delete pod webapp
Then create a new pod with the edited file
kubectl create -f my-new-pod.yaml

Edit Deployments
With Deployments you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification,  with every change the deployment will automatically delete and create a new pod with the new changes. So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command
kubectl edit deployment my-deployment

69. -- 5:01 hrs

- DaemonSets
- - r like replicaSets, help in deploying multiple instances of pods, but runs 1 copy of pod on each node in cluster
- - wenevr a new node is added to cluster, a replica of a pod is automatically added to that node
- - ensures 1 copy of pod is alwys present in all nodes in cluster
- - use cases:
- - - monitoring agent or log collector/aggregator in each node in cluster, kube-proxy can b deployed as DaemonSet
- - - CW Agent or n/wing solutions like weave-net, requires an agent to be deployed on each node in cluster

How does DaemonSet schedules n ensure 1 copy of pod on each node?
- 1 approach, set nodeName property in node specification to bypass scheduler. This is useful before node is created
- DaemonSet uses default scheduler & node affinity rules, to schedule pods on nodes.

kubectl create deployment elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.20 -n=kube-system --dry-run=client -o yaml > daemonset.yaml
update Kind: to DaemonSet, remove replicas, strategy, status n then apply it

kubectl get ds


[] Static Pods:
- these r pods created by kubelet on its own, without intervention by API Svr or rest of k8s cluster components
- kubelet is the only component tht can run independently in absence of ctrl plane, rs, deploy, svc cannot work without ctrl plane
- kubelet can periodically chk /etc/kubernetes/manifests/ folder for presence of yaml files n can create pods n maintain them independently
- kubelet works at pod lvl n can only understand pods, which is why its able to create static pods this way
- --pod-manifest-path=<path> is the path tht kubelet looks in, to chk for manifest/yaml files
- --config=<yamlFile>, clusters setup by kubeadm tool use this option to set static pod path
- - if u r inspecting an existing cluster, inspect this option of kubelet to identify path to directory
- - this will inform whr to place definition file for static pods
- look for staticPodPath in kubeconfig.yaml or in kubelet.service, chk for --pod-manifest-file or --config value
- - this will determine the definition files tht kubelet is monitoring for creating newer pods n maintaining existing ones

without ctrl plane, no kubectl utility as well, so, use docker ps to chk for containers running

kubelet can take in requests for pod creation thru multiple inputs
1st is thru pod definition file from static pods folder
2nd is from etcd API endpoint, that is how kube API svr provides input

kubelet can create both kinds of pods, static as well as dynamic (ones from API svr) at the same time

API svr is aware of static pods created by kubelet.

if kubelet creates a static pod, it also creates a mirror object in kube API svr
wht we see in kubectl get pods output frm kube API svr is just a read-only mirror of the pod
u can view details of pod but u cannot edit or delete it like usual pods
only way to delete static pods is by modifying files from node's manifest folder
node name is automatically appended in static pod's name

Why use static pods?
- since static pods r not dependent on k8s ctrl plane, these can be used to deploy ctrl plane on the node itself

static pods vs daemon sets:
- static pod -- created by kubelet, daemonset, created by kube PAI ctrller
- in static pod, ctrl plane components can be deployed. in daemonSet, monitoring n logging agents can be deployed
both r ignored by kube scheduler

First, let's identify the node in which the pod called static-greenbox is created. To do this, run:
root@controlplane:~# kubectl get pods --all-namespaces -o wide  | grep static-greenbox
default       static-greenbox-node01                 1/1     Running   0          19s     10.244.1.2   node01       <none>           <none>
root@controlplane:~#
From the result of this command, we can see that the pod is running on node01.


Next, SSH to node01 and identify the path configured for static pods in this node.
Important: The path need not be /etc/kubernetes/manifests. Make sure to check the path configured in the kubelet configuration file.
root@controlplane:~# ssh node01
root@node01:~# ps -ef |  grep /usr/bin/kubelet
root       752   654  0 00:30 pts/0    00:00:00 grep --color=auto /usr/bin/kubelet
root     28567     1  0 00:22 ?        00:00:11 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2
root@node01:~# grep -i staticpod /var/lib/kubelet/config.yaml
staticPodPath: /etc/just-to-mess-with-you
root@node01:~#
Here the staticPodPath is /etc/just-to-mess-with-you


Navigate to this directory and delete the YAML file:
root@node01:/etc/just-to-mess-with-you# ls
greenbox.yaml
root@node01:/etc/just-to-mess-with-you# rm -rf greenbox.yaml
root@node01:/etc/just-to-mess-with-you#
Exit out of node01 using CTRL + D or type exit. You should return to the controlplane node. Check if the static-greenbox pod has been deleted:
root@controlplane:~# kubectl get pods --all-namespaces -o wide  | grep static-greenbox
root@controlplane:~#

kubectl get pods -A #<== chk if this shows frm all namespaces
chk path in /var/lib/kubelet/config.yaml, for manifests/yaml files


[] Multiple Schedulers:
- good for specific apps tht requires pods to be placed on specific nodes, after performing some additional checks
- setup own/custom scheduling algorithm to place pods on nodes, so tht u can add ur own custom conditions & checks in it
- u can write ur own scheduler program, pkg it n deploy it as default scheduler or as additional scheduler
- wen creating a cluster, u can instruct k8s to use a specific scheduler
scheduler1-config.yaml
==========================================
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: scheduler1
leaderElection:
  leaderElect: true #<== used wen u hv multiple copies of scheduler running on diff. master nodes in HA, only 1 can be active at any time
  resourceNamespace: kube-system
==========================================
download scheduler as a binary n deploy it as a svc
ExecStart=/usr/local/bin/kube-scheduler --config=/etc/kubernetes/config/scheduler1-config.yaml
Other options can b added like kubectl file to be authenticated wid api for scheduling pods in it, etc
deploy additional scheduler as a pod, scheduler1.yaml >>
==========================================
apiVersion: v1
kind: Pod
metadata:
  name: scheduler1
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.config #<== this file has auth info to connect to API Svr
    - --config=/etc/kubernetes/scheduler1-config.yaml
    image: k8s.grc.io/kube-scheduler-amd64:v1.11.3
    name: kube-scheduler
==========================================

upon creating new pod, specify under spec,
schedulerName: scheduler1
this instructs k8s to pick needed shcduler

if scheduler was not configured correctly, this pod will stay in pending state, else in running state

kubectl get events -o wide
kubectl logs scheduler1 --name-space=kube-system
kubectl sa my-scheduler -n kube-system # sa --> service account
kubectl create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system


[] Scheduler Profile:
- How to configure multiple schedulers
- How to view scheduler events
- wen pods r requested, those end up being lined up in
1. scheduling queue, n wait to be scheduled
- at this stage, pods r sorted, based on priority defined on pods
- - priority class can be created to set priorities for pods
================================
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "This priority class should be used for XYZ svc pods only"
================================
then define, under spec of pod.yaml, priorityClassName: high-priority

This is how pods wid high priority get to beginning of scheduling queue
2. Next is filtering phase,
- this is whr nodes tht cannot run pods based on resource reqmnts n leftover resources, r filtered out
3. Next is scoring phase
- this is whr nodes r scored wid diff. weights, based on free space leftover after pod will get deployed
4. Finally is binding phase
- this is whr pod is finally bound to a node, wid highest score

All these operations r achieved wid certain plugins.
Ex:
1. while in scheduling queue, its the PrioritySort plugin tht sorts the pods, based on priority configured on pods
- This is how pods wid priority class get higher priority over other pods, when scheduling
2. In Filtering phase, its the nodeResourcesFit plugin tht identified nodes tht hv sufficient resources, reqd by pod & filters out nodes tht do not hv sufficient resources
Other plugin examples for this stage will be, nodeName plugin, tht chks if a pod has a node name, mentioned in pod spec & filters out nodes tht do not match this spec
Another one is nodeUnschedulable plugin, filters out nodes tht hv unschedulable flag set to true. This is when u hv to drain nodes, using cordon cmd for maintenance
3. Scoring phase, NodeResourcesFit associates a score to each node, based on resource availability.
Single plugin can b associated to multiple phases
Another plugin is ImageLocality plugin tht associates a high score to nodes tht alrdy hv containerImage, used by pods among diff. nodes
At this phase, plugins do not really reject pod placement on a particular node
4. Binding phase, DefaultBinder plugin tht provides binding mechanism

u can write ur own plugins n attach to diff. phases
That is achieved wid help of extension points
at each stage, thrs an extension point, that a plugin can plug to

In SchedulingQueue phase, we hv queueSort extension point, to which PrioritySort plugin is plugged to.
same way for Filtering phase, filter extension point
Scoring --> score
Binding --> bind

thr r more extensions, before entering filter phase, called pre-filter extensions
and after filter phase, called post-filter
same way for others...lots of plugins r spanned across multiple extension points
highly extensible nature of k8s allows us to customize the way, tht these plugins r called & write ur own scheduling plugin
<<<extensionPoints>>

How we can change default behavior of how these plugins r called & get our own plugins in scheduler?
- 1st way is like 3 separate scheduler binaries, run wid separate cfg file associated wid each
- - problem is since each run wid a separate process, additional effort reqd to maintain these separate processes
- - these may run into race conditions while making scheduling decisions
- - Ex: 1 scheduler may schedule workload on a node without knowing thrs another scheduler scheduling workload on same node at same time
- Scheduler Profiles: 1.18 onwards, feature to support multiple profiles in single scheduler, in single scheduler cfg file, was introduced
- - each profile acts as separate scheduler & all r run in same binary
- - under each profiles: - schedulerName:, u can enable/disable own as well as default plugins, for each extension point

References
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md
https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

83. 6:01 hrs
[] Logging & Monitoring:
- How to monitor k8s cluster components as well as apps hosted on them
- - node-lvl metrics like
- - - no. of nodes in cluster & how many of them r healthy
- - - performance metrics like CPU, memory, n/w & disk utilization
- - pod-lvl metrics like
- - - no. of pods
- - - performance metrics of each pod like CPU, memory consumption

A Solution like Metrics Svr, Prometheus, Elastic Stack, Datadog, dynatrace tht will
- monitor these metrics
- store these metrics
- provide analytics around this data

Heapster was one of original solution, native to k8s, now deprecated & its slimmed-down version is Metrics svr
- we can hv 1 metrics svr per k8s cluster.
- its an in-memory monitoring solution, doesnt store metrics on disk
- due to this, we cannot see historical data
- for this we need to rely on more advanced monitoring solutions

How metrics r generated for pods on each node?
- k8s runs an agent on each node called kubelet, responsible for recving instructions frm API Svr & running pods on nodes
- kubelet contains a sub-component, called cAdvisor (container Advisor), responsible for
- - retrieving performance metrics frm pods
- - exposing those metrics thru kubelet API to make metrics avl for metrics svr

How to deploy metrics svr?
git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git # DO NOT USE this for PRODUCTION use-cases

controlplane ~ ✖ cd kubernetes-metrics-server/

controlplane kubernetes-metrics-server on  master ➜  ls -ltrh
total 32K
-rw-r--r-- 1 root root  612 Jan 24 00:08 resource-reader.yaml
-rw-r--r-- 1 root root  219 Jan 24 00:08 README.md
-rw-r--r-- 1 root root  249 Jan 24 00:08 metrics-server-service.yaml
-rw-r--r-- 1 root root 1000 Jan 24 00:08 metrics-server-deployment.yaml
-rw-r--r-- 1 root root  293 Jan 24 00:08 metrics-apiservice.yaml
-rw-r--r-- 1 root root  324 Jan 24 00:08 auth-reader.yaml
-rw-r--r-- 1 root root  303 Jan 24 00:08 auth-delegator.yaml
-rw-r--r-- 1 root root  384 Jan 24 00:08 aggregated-metrics-reader.yaml

controlplane kubernetes-metrics-server on  master ➜  kubectl create -f .
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.apps/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created

controlplane kubernetes-metrics-server on  master ➜


- How to view & manage logs for cluster components as well as apps
-- various logging mechanisms in k8s
-- for multiple containers running in a pod, specify container name as well
kubectl logs -f pod1 container2 #<== this streams live logs



[] Application Lifecycle Mgmt:
- Scale apps
- Rolling updates & rollbacks
- - default rolling strategy is Rolling Update.
- - Recreate/All-at-once is good for dev environments whr downtime is acceptable & its fastest of all, for upgrades/rollouts

kubectl rollout status deployment/webserver-deployment #<== live status
kubectl rollout history deployment/webserver-deployment #<== shows history of revisions n rollout
kubectl rollout undo deployment/webserver-deployment #<== rollback

kubectl set image deployment/webserver-deployment nginx=nginx:1.9.1 #<== imperative cmd, doesnt updates yaml cfg file
                                        <containerName>=<Imageversion>

cat curl-test.sh
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

controlplane ~ ➜  cat curl-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: curl
  namespace: kube-public
spec:
  containers:
  - image: byrnedo/alpine-curl:latest
    name: alpine-curl
    command: ["sleep", "5000"]


- Diff. ways to configure apps: Configuring applications comprises of understanding the following concepts:
- - Configuring Command and Arguments on applications
- - - EntryPoint in Dockerfile specifies default/1st command to be executed wen container starts running
- - - CMD in Dockerfile can provide default argument to command specified in Entrypoint.
- - - Ex: ENTRYPOINT ["sleep"] & CMD ["5"], becomes sleep 5, wen container starts
- - - - in k8s, ENTRYPOINT becomes command: and CMD becomes args:, under spec.containers, i.e.,
=======================
spec:
  containers:
  - name: nginx
    image: nginx
    command: ["sleep"]
    args: ["5"]
=======================

- - Configuring Environment Variables
- - - To set an environment variable, use env property under spec.containers.name, wid name/value pair as a list under it
- - - Better ways r to specify key/values in configMap n secrets

Direct env variable specified >>
=================================
env:
- name: COLOR
  value: green
=================================

Invoked from configMap >>
=================================
env:
- name: COLOR
  valueFrom:
    configMapKeyRef:
=================================

Invoked from configMap >>
=================================
env:
- name: COLOR
  valueFrom:
    secretKeyRef:
=================================

a. configMaps:
- helps work wid configuration data in k8s, by managing env variables etc config centrally
- configMaps r used to pass cfg data in the form of key-value pairs in k8s
- wen a pod is created, inject cfg map into the pod so key/value pairs r avl as env variables for app hosted inside containers in the pod
- 2 phases involved in configuring cfg maps
- - 1st, create configMap
2 ways to create configMap, just like any other object creation in K8s
i. imperative --
kubectl create configmap \
<cfgName> --from-literal=<key>=<value> #<== from-literal option is used to specify key/value pairs from teh cmd itself
Ex: app-cfg --from-literal=COLOR=green
or
kubectl create configmap \
<cfgName> --from-file=<file>
Ex:
controlplane ~ ✖ kubectl create cm webapp-config-map --from-literal=APP_COLOR=darkblue
configmap/webapp-config-map created


ii. declarative
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: cfgmap
data:
  COLOR: green
  MODE: prod
================

- - 2nd, inject configMap into the pod
- - - use the following under spec.containers.name
=================
- envFrom:
  - configMapRef:
      name: cfgmap #<== filename of cfgmap to load all specified key/value pairs
=================

kubectl edit cm webapp-config-map
kubectl replace --force -f /tmp/<file>.yaml

100. 7:04 hrs

- - Configuring Secrets
b. Secrets:
- secrets r NOT encrypted, those r only encoded.
- DO NOT checkin secrets definition files in GIT along with code
- secrets r NOT encrypted in etcd by default
- - consider enabling encryption at rest in etcd, by creating EncryptionConfiguration kind object
- - then pass this object as an option to API svr, target specific resource in definition, like encrypt only secrets etc
- consider storing secrets in separate namespace n setup RBAC wid least-privilege access to secrets
- consider 3rd party secret store providers like Vault, Helm Secrets, AWS, CyberArk etc

echo -n 'passwd'   | base64 #<== to encode a secret, before storing it in secrets file
echo -n 'cGFzc3dk' | base64 --decode #<== to decode back value

under spec.containers.image
=================
envFrom:
  - secretRef:
      name: secret-file #<== filename of secret to load all specified key/value pairs
=================

Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it.
As such the secrets can be considered as not very safe.
The concept of safety of the Secrets is a bit confusing in Kubernetes.
The kubernetes https://kubernetes.io/docs/concepts/configuration/secret/ page and a lot of blogs out there
refer to secrets as a "safer option" to store sensitive data.
They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and
other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it.
Secrets are not encrypted, so it is not safer in that sense.
However, some best practices around using secrets make it safer.
As in best practices like:
Not checking-in secret object definition files to source code repositories.
Enabling Encryption at Rest https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
for Secrets so they are stored encrypted in ETCD.
Also the way kubernetes handles secrets. Such as:
A secret is only sent to a node if a pod on that node requires it.
Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.
Read about the protections https://kubernetes.io/docs/concepts/configuration/secret/#protections and
risks https://kubernetes.io/docs/concepts/configuration/secret/#risks of using secrets
here https://kubernetes.io/docs/concepts/configuration/secret/#risks
Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as
using tools like Helm Secrets, HashiCorp Vault https://www.vaultproject.io/

controlplane ~ ➜  kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --dry-run=client -o yaml > secret.yaml
controlplane ~ ➜  kubectl get secret db-secret -o yaml #<== shows secret data

[] Encrypting Secret data at rest:
- How data is stored in etcd svr?
apt-get install etcd-client #<== get client installed 1st
encryption.yaml>>
========================
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets:
  - providers:
    - aescbc:
      keys:
        - name: key1
        secret: slkfjblekfqifuw==
    - identity: {} #<== sequence is IMPORTANT. If this goes at the top, that means NO Identity provider, so no encryption
========================

vi /etc/kubernetes/manifests/kube-apiserver.yaml
add the following right before image:
- --encryption-provider-config=/etc/kubernetes/enc/encryption.yaml
n then define mountPath for local dir. to be mounted to, i.e.,
- name: encryption
  mountPath: /etc/kubernetes/enc
  readonly: true
then below, under mounts,specify location of local dir,
- name: encryption
  hostPath:
    path: /etc/kubernetes/enc
    type: DirectoryOrCreate


- Multi-container pods:
Multi-container PODs Design Patterns
There are 3 common patterns, when it comes to designing multi-container PODs.
The first and what we just saw with the logging service example is known as a side car pattern.
The others are the adapter and the ambassador pattern.



- InitContainers
In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle.
For example in the multi-container pod that we talked about earlier that has a web application and logging agent,
both the containers are expected to stay alive at all times. The process running in the log agent container is expected
to stay alive as long as the web application is running. If any of them fails, the POD restarts.
But at times you may want to run a process that runs to completion in a container.
For example a process that pulls a code or binary from a repository that will be used by the main web application.
That is a task that will be run only  one time when the pod is first created.
Or a process that waits for an external service or database to be up before the actual application starts.
That's where initContainers comes in.
An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers
section,  like this:

=====================
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']
=====================

When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion
before the real container hosting the application starts.
You can configure multiple such initContainers as well, like how we did for multi-pod containers.
In that case each init container is run one at a time in sequential order.
If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

=====================
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
=====================

Read more about initContainers here. And try out the upcoming practice test.
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/


111. 8:10 hrs

- Primitives for self-healing app
Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers.
The replication controller helps in ensuring that a POD is re-created automatically when the application
within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.
Kubernetes provides additional support to check the health of applications running within PODs and
take necessary actions through Liveness and Readiness Probes.

[] Cluster maintenance:

- OS Upgrades: implications of losing a node from the cluster for applying patches or upgrades on OS itself
- - wen a node is taken out of cluster,
- - - k8s waits based on --pod-eviction-timeout (time it waits for pod to come back online, default set to 5 mins), set on kube-controller-manager
- - - after 5 mins, pods r terminated from that node, as master node on k8s considers them as dead
- - - if pods r part of replicaSet, then they r recreated on other nodes
- - - wen node comes back online after 5 mins, it joins cluster as blank wid no pods scheduled on it
- - Better approach
- - - drain the node by executing kubectl drain <nodeName>, so tht workloads/pods r moved to other nodes in the cluster
- - - - technically, pods r gracefully terminated and recreated on other nodes in the cluster, drain = eviction of pods + node marked as unschedulable
- - - - node is also cordoned / marked as unschedulable, so tht no newer pods can be scheduled on this node,  till restriction is removed
- - - once node is back, execute kubectl uncordon <nodeName>, so that newer pods can start getting scheduled on it again
- - - pods recreated on other nodes, don't automatically fall back, if any pods were deleted or need to be recreated, only then those land on this node
- - - kubectl cordon <nodeName> simply marks node as unscheduled, it doesnt move pods to other nodes, like drain does

- k8s Releases & Versions: Best practices arnd upgrading, like wen to upgrade, wht version to upgrade to.
- - release versions
- - - release version in the format of major.minor.patch, with alpha & beta being minor initial releases, making it to patch release later on
- - - all components in k8s can hv their own versions, as those r opereated as separated projects internally
- - - none of the components should be of version higher than API Svr as all components talk to it
- - - ctrller-mgr & scheduler can be 1 version lower, while kubelet n kube-proxy can be 2 versions lower than API svr
- - - kubectl can be 1 version lower or 1 version higher than API svr
- - when do we upgrade?
- - - we can upgrade component by component as well
- - - k8s supports only up to 3 minor versions, so plan upgrade accordingly to stay within EOL
- - - recommended approach is to upgrade 1 minor version at a time

https://kubernetes.io/docs/concepts/overview/kubernetes-api/
Here is a link to kubernetes documentation if you want to learn more about this topic
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md


- Cluster Upgrade Process:
- - kubeadm upgrade plan & kubeadm upgrade apply cmds to upgrade versions
- - alwys upgrade 1 minor version at a time
- - if cluster is deployed frm scratch, then manually upgrade each component yourself
- - 2 steps involved, 1st upgrade master node n then upgrade worker nodes
- - - while master is being upgraded, ctrl plane components go down briefly but that doesnt means workers stop operating
- - - all workloads on worker nodes continue to serve users as normal,
- - - only mgmt fns r down, like cant deploy new apps or manage existing ones
- - - u cannot access cluster using kubectl or other k8s API during that time
- - - for worker nodes, strategies
- - - - 1st strategy, upgrading all at once can lead to downtime (can b done in Dev)
- - - - 2nd strategy, perform 1 node upgrade at a time, by moving pods of that node to other node
- - - - 3rd strategy, add new nodes (up to 33%) wid latest version to cluster, easy to provision nodes in cloud
- - kubeadm has upgrade cmd for upgrading clusters, i.e., kubeadm upgrade plan
- - - provides current cluster version, kubeadm tool version, latest stable version,
- - - lists all ctrl plane components & wht version these can be upgraded to
- - - also informs that after upgrade ctrl plane components, kubelet version needs to be upgraded manually on each node
- - - provides cmd to upgrade cluster, i.e., kubeadm upgrade apply v<version>

126. 9:11 hrs

- Upgrade Steps: use k8s official documentation n cmds specified, during version upgrades
- - Step1: upgrade kubeadm tool itself before upgrading the cluster, i.e., apt-get upgrade -y kubeadm=<version>
- - - kubeadm tool follows same s/w version as k8s
- - Step2: kubeadm upgrade apply v<version> pulls necessary images n upgrades cluster components in ctrl plane
- - - kubectl get nodes may still show old version cuz it shows version of kubelet on each worker node, registered wid API svr
- - Step3: upgrade kubelet on master node, i.e., by executing following cmds
kubectl drain <masterNode>
apt-get upgrade -y kubelet=<version>
sudo systemctl daemon-reload
sudo systemctl restart kubelet
kubectl uncordon <masternode>
- - - now kubectl get nodes will show upgraded version of kubelet
- - Step4: upgrade worker nodes
- - - i. On worker node, upgrade kubeadm by executing
apt-get upgrade -y kubeadm=<version>
kubeadm upgrade node
- - - ii. On Master Node, move workloads frm planned worker node to other nodes, using kubectl drain <nodeName>
- - - - this will safely terminate all pods from node & reschedules them to other nodes; and also cordons node (marks it unscheduleable)
- - - iii. On worker node, kubelet, kubctl pkgs on tht node & node cfg for new kubelet version, i.e.,
apt-get upgrade -y kubelet=<version>
kubeadm upgrade node config --kubelet-version v<version>
systemctl restart kubelet
- - - iv. From master node, uncordon node, for it to start accepting new pod schedule requests
kubectl uncordon <nodeName>

controlplane ~ ➜  kubeadm upgrade plan
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.25.0
[upgrade/versions] kubeadm version: v1.25.0
I0125 00:12:00.493376   19258 version.go:256] remote version is much newer: v1.26.1; falling back to: stable-1.25
[upgrade/versions] Target version: v1.25.6
[upgrade/versions] Latest version in the v1.25 series: v1.25.6

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     2 x v1.25.0   v1.25.6

Upgrade to the latest version in the v1.25 series:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.25.0   v1.25.6
kube-controller-manager   v1.25.0   v1.25.6
kube-scheduler            v1.25.0   v1.25.6
kube-proxy                v1.25.0   v1.25.6
CoreDNS                   v1.9.3    v1.9.3
etcd                      3.5.4-0   3.5.4-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.25.6

Note: Before you can perform this upgrade, you have to update kubeadm to v1.25.6.

_____________________________________________________________________


The table below shows the current state of component configs as understood by this version of kubeadm.
Configs that have a "yes" mark in the "MANUAL UPGRADE REQUIRED" column require manual config upgrade or
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
upgrade to is denoted in the "PREFERRED VERSION" column.

API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
kubelet.config.k8s.io     v1beta1           v1beta1             no
_____________________________________________________________________


controlplane ~ ➜

controlplane ~ ➜  kubectl describe node01 | grep -i taint
error: the server doesn't have a resource type "node01"

controlplane ~ ✖ kubectl describe no node01 | grep -i taint
Taints:             <none>

controlplane ~ ➜  kubectl describe no controlplane | grep -i taint
Taints:             <none>

controlplane ~ ➜

controlplane ~ ➜  kubectl get deploy
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
blue   5/5     5            5           2m35s

controlplane ~ ➜  kubectl get all
NAME                        READY   STATUS    RESTARTS   AGE
pod/blue-5db6db69f7-26cvh   1/1     Running   0          2m45s
pod/blue-5db6db69f7-744ls   1/1     Running   0          2m45s
pod/blue-5db6db69f7-pwvp9   1/1     Running   0          2m45s
pod/blue-5db6db69f7-qv2zx   1/1     Running   0          2m45s
pod/blue-5db6db69f7-zr2lx   1/1     Running   0          2m45s

NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes    ClusterIP   10.96.0.1       <none>        443/TCP        57m
service/red-service   NodePort    10.105.151.92   <none>        80:30080/TCP   2m45s

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blue   5/5     5            5           2m45s

NAME                              DESIRED   CURRENT   READY   AGE
replicaset.apps/blue-5db6db69f7   5         5         5       2m45s

controlplane ~ ➜  kubectl get po -o wide
NAME                    READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
blue-5db6db69f7-26cvh   1/1     Running   0          3m1s   10.244.1.3   node01         <none>           <none>
blue-5db6db69f7-744ls   1/1     Running   0          3m1s   10.244.1.2   node01         <none>           <none>
blue-5db6db69f7-pwvp9   1/1     Running   0          3m1s   10.244.0.5   controlplane   <none>           <none>
blue-5db6db69f7-qv2zx   1/1     Running   0          3m1s   10.244.0.4   controlplane   <none>           <none>
blue-5db6db69f7-zr2lx   1/1     Running   0          3m1s   10.244.1.4   node01         <none>           <none>

controlplane ~ ➜

controlplane ~ ➜  kubectl drain controlplane
node/controlplane cordoned
error: unable to drain node "controlplane" due to error:cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-flannel/kube-flannel-ds-dxvjq, kube-system/kube-proxy-bg68t, continuing command...
There are pending nodes to be drained:
 controlplane
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-flannel/kube-flannel-ds-dxvjq, kube-system/kube-proxy-bg68t

controlplane ~ ✖ kubectl drain controlplane --ignore-daemonsets
node/controlplane already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-dxvjq, kube-system/kube-proxy-bg68t
evicting pod kube-system/coredns-565d847f94-zpg75
evicting pod default/blue-5db6db69f7-qv2zx
evicting pod kube-system/coredns-565d847f94-hf8wz
evicting pod default/blue-5db6db69f7-pwvp9
pod/blue-5db6db69f7-qv2zx evicted
pod/blue-5db6db69f7-pwvp9 evicted
pod/coredns-565d847f94-zpg75 evicted
pod/coredns-565d847f94-hf8wz evicted
node/controlplane drained

controlplane ~ ➜  kubectl get po -A -o wide
NAMESPACE      NAME                                   READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES
default        blue-5db6db69f7-26cvh                  1/1     Running   0          6m42s   10.244.1.3    node01         <none>           <none>
default        blue-5db6db69f7-744ls                  1/1     Running   0          6m42s   10.244.1.2    node01         <none>           <none>
default        blue-5db6db69f7-ndcf9                  1/1     Running   0          21s     10.244.1.8    node01         <none>           <none>
default        blue-5db6db69f7-qvkjn                  1/1     Running   0          20s     10.244.1.10   node01         <none>           <none>
default        blue-5db6db69f7-zr2lx                  1/1     Running   0          6m42s   10.244.1.4    node01         <none>           <none>
kube-flannel   kube-flannel-ds-dj4fh                  1/1     Running   0          60m     10.14.19.9    node01         <none>           <none>
kube-flannel   kube-flannel-ds-dxvjq                  1/1     Running   0          60m     10.14.19.6    controlplane   <none>           <none>
kube-system    coredns-565d847f94-cmb24               1/1     Running   0          21s     10.244.1.7    node01         <none>           <none>
kube-system    coredns-565d847f94-kvhzc               1/1     Running   0          20s     10.244.1.9    node01         <none>           <none>
kube-system    etcd-controlplane                      1/1     Running   0          60m     10.14.19.6    controlplane   <none>           <none>
kube-system    kube-apiserver-controlplane            1/1     Running   0          60m     10.14.19.6    controlplane   <none>           <none>
kube-system    kube-controller-manager-controlplane   1/1     Running   0          60m     10.14.19.6    controlplane   <none>           <none>
kube-system    kube-proxy-bg68t                       1/1     Running   0          60m     10.14.19.6    controlplane   <none>           <none>
kube-system    kube-proxy-jbwd8                       1/1     Running   0          60m     10.14.19.9    node01         <none>           <none>
kube-system    kube-scheduler-controlplane            1/1     Running   0          61m     10.14.19.6    controlplane   <none>           <none>

controlplane ~ ➜  kubectl get no
NAME           STATUS                     ROLES           AGE   VERSION
controlplane   Ready,SchedulingDisabled   control-plane   61m   v1.25.0
node01         Ready                      <none>          60m   v1.25.0

controlplane ~ ➜  apt update
Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease
Hit:3 https://download.docker.com/linux/ubuntu focal InRelease
Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease
Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
Fetched 114 kB in 1s (78.4 kB/s)
Reading package lists... Done
Building dependency tree
Reading state information... Done
26 packages can be upgraded. Run 'apt list --upgradable' to see them.

controlplane ~ ➜  apt-get install kubeadm=1.26.0-00
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages will be upgraded:
  kubeadm
1 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.
Need to get 9,730 kB of archives.
After this operation, 2,974 kB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.26.0-00 [9,730 kB]
Fetched 9,730 kB in 0s (42.7 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 18569 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.26.0-00_amd64.deb ...
Unpacking kubeadm (1.26.0-00) over (1.25.0-00) ...
Setting up kubeadm (1.26.0-00) ...

controlplane ~ ➜  kubeadm upgrade apply v1.26.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0125 00:20:30.970517   24000 configset.go:177] error unmarshaling configuration schema.GroupVersionKind{Group:"kubeproxy.config.k8s.io", Version:"v1alpha1", Kind:"KubeProxyConfiguration"}: strict decoding error: unknown field "udpIdleTimeout"
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to "v1.26.0"
[upgrade/versions] Cluster version: v1.25.0
[upgrade/versions] kubeadm version: v1.26.0
[upgrade] Are you sure you want to proceed? [y/N]: y
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.26.0" (timeout: 5m0s)...
[upgrade/etcd] Upgrading to TLS for etcd
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-01-25-00-21-02/etcd.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component "etcd" upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests3585077112"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-01-25-00-21-02/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-01-25-00-21-02/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-01-25-00-21-02/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.26.0". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.

controlplane ~ ➜

controlplane ~ ➜  apt-get install kubelet=1.26.0-00
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following packages will be upgraded:
  kubelet
1 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.
Need to get 20.5 MB of archives.
After this operation, 7,032 kB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.26.0-00 [20.5 MB]
Fetched 20.5 MB in 0s (54.8 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 18569 files and directories currently installed.)
Preparing to unpack .../kubelet_1.26.0-00_amd64.deb ...
/usr/sbin/policy-rc.d returned 101, not running 'stop kubelet.service'
Unpacking kubelet (1.26.0-00) over (1.25.0-00) ...
Setting up kubelet (1.26.0-00) ...
/usr/sbin/policy-rc.d returned 101, not running 'start kubelet.service'

controlplane ~ ➜  systemctl daemon-reload

controlplane ~ ➜  systemctl restart kubelet

controlplane ~ ➜



- Backup & restore methodologies: Practice a DR scenario, recover frm disaster to bring cluster back to previous state
- - Backup Candidates
- - - resource cfg: deployments, pod, svc cfg files
- - - - Imperative vs declarative approach: Declarative is preferred approach if u want to save cfgs
- - - - Backups for Declarative Approach:
- - - - - Good practice is to save yaml/manifests on GIT repo, n this repo should b configured with right backup solutions
- - - - - with this, even if u lose ur entire cluster, u can redeploy ur app on cluster by simply applying cfg files frm repo
- - - - Backups for Imperative Approach:
- - - - - query API svr using kubectl or by accessing api svr directly & save all resource cfgs for all objs created on cluster, as a copy
kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml #<== cmd to backup all pods/deployments/rs/svcs etc
- - - PVs: persistent storage, if apps r configured wid it n storing data there
- - - etcd cluster: stores info abt state of cluster, infor abt cluster itself, nodes & each resource created within cluster
- - - - instead of backing up resources, better to backup etcd svr itself, by backing up path specified in --data-dir=<path>
- - - - etcd also comes wid built-in snapshot solution by executing,
etcdctl snapshot save snapshot.db
etcdctl snapshot status snapshot.db
# to restore backup from this snapshot, 1st stop kube API svr, as restore process will require u to restart etcd cluster & API svr depends on it
service kube-apiserver stop
# now run restore cmd
etcdctl snapshot restore snapshot.db --data-dir <newPath>
# with this restore, etcd reinitializes cluster & configures members of etcd as new members to new cluster
# this is done to prevent a new member to accidentally join existing cluster
# now configure etcd to use newPath for data dir, by setting --data-dir=<newPath>
# then reload svc daemon n restart etcd svr n startup API svr
systemctl daemon-reload
service etcd restart
service kube-aipserver start
# Now cluster should b back in original state
# wid all etcd cmds, remember to specify cert files for auth, endpoints to etcd cluster, cacert & key
# in cloud, we do not hv access to etcd cluster, in that case, backup by querying API svr is better way

etcdctl is a command line client for etcd.
In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.
To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.
You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:
export ETCDCTL_API=3

On the Master Node:
To see all the options for a specific sub-command, make use of the -h or --help flag.
For example, if you want to take a snapshot of etcd, use:
etcdctl snapshot save -h and keep a note of the mandatory global options.
Since our ETCD database is TLS-Enabled, the following options are mandatory:
--cacert                                                verify certificates of TLS-enabled secure servers using this CA bundle
--cert                                                    identify secure client using this TLS certificate file
--endpoints=[127.0.0.1:2379]          This is the default as ETCD is running on master node and exposed on localhost 2379.
--key                                                      identify secure client using this TLS key file
Similarly use the help option for snapshot restore to see all available options for restoring the backup.
etcdctl snapshot restore -h


controlplane ~ ➜  ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \
>  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
>  --cert=/etc/kubernetes/pki/etcd/server.crt \
>  --key=/etc/kubernetes/pki/etcd/server.key \
>  snapshot save /opt/snapshot-pre-boot.db
Snapshot saved at /opt/snapshot-pre-boot.db

controlplane ~ ➜  ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db
2023-01-25 01:53:47.716747 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32

controlplane ~ ➜  vim /etc/kubernetes/manifests/etcd.yaml

controlplane ~ ➜  grep backup /etc/kubernetes/manifests/etcd.yaml
      path: /var/lib/etcd-from-backup

controlplane ~ ➜  grep -A2 -B2 backup /etc/kubernetes/manifests/etcd.yaml
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-from-backup #<== updating here is sufficient as this volume is mounted
      type: DirectoryOrCreate
    name: etcd-data

controlplane ~ ➜  grep -B1 -A1 "/var/lib/etcd" /etc/kubernetes/manifests/etcd.yaml
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
--
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data                  #<== this connects hostPath wid volumeMount, & volumeMount is used by data-dir
--
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data


Note: In this case, we are restoring the snapshot to a different directory but in the same server where we took the
backup (the controlplane node) As a result, the only required option for the restore command is the --data-dir.
Next, update the /etc/kubernetes/manifests/etcd.yaml:
We have now restored the etcd snapshot to a new path on the controlplane - /var/lib/etcd-from-backup, so,
the only change to be made in the YAML file, is to change the hostPath for the volume called etcd-data from old directory
(/var/lib/etcd) to the new directory (/var/lib/etcd-from-backup).
  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
With this change, /var/lib/etcd on the container points to /var/lib/etcd-from-backup on the controlplane
(which is what we want).
When this file is updated, the ETCD pod is automatically re-created as this is a static pod placed under the
/etc/kubernetes/manifests directory.

Note 1: As the ETCD pod has changed it will automatically restart, and also kube-controller-manager and kube-scheduler.
- Wait 1-2 to mins for this pods to restart.
- You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.
Note 2: If the etcd pod is not getting Ready 1/1, then restart it by kubectl delete pod -n kube-system etcd-controlplane
- and wait 1 minute.
Note 3: This is the simplest way to make sure that ETCD uses the restored data after the ETCD pod is recreated.
- You don't have to change anything else.

If you do change --data-dir to /var/lib/etcd-from-backup in the ETCD YAML file, make sure that the volumeMounts for
etcd-data is updated as well, with the mountPath pointing to /var/lib/etcd-from-backup
(THIS COMPLETE STEP IS OPTIONAL AND NEED NOT BE DONE FOR COMPLETING THE RESTORE)

kubectl config view #<-- to chk list of clusters deployed

student-node ~ ➜  kubectl config use-context cluster1 #<== to switch to specific cluster n then chk nodes in tht cluster
Switched to context "cluster1".

student-node ~ ➜  kubectl get no -A
NAME                    STATUS   ROLES           AGE   VERSION
cluster1-controlplane   Ready    control-plane   36m   v1.24.0
cluster1-node01         Ready    <none>          35m   v1.24.0

student-node ~ ➜

student-node ~ ➜  kubectl get no -A -o wide
NAME                    STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
cluster1-controlplane   Ready    control-plane   38m   v1.24.0   10.20.56.18   <none>        Ubuntu 18.04.6 LTS   5.4.0-1093-gcp   containerd://1.6.6
cluster1-node01         Ready    <none>          37m   v1.24.0   10.20.56.24   <none>        Ubuntu 18.04.6 LTS   5.4.0-1028-gcp   containerd://1.6.6

student-node ~ ➜  ssh 10.20.56.18

This means that ETCD is set up as a Stacked ETCD Topology where the distributed data storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by kubeadm that run control plane components.


cluster2-controlplane ~ ➜  kubectl get po -o wide -A | grep etcd

cluster2-controlplane ~ ✖ ps -eaf | grep etcd
root        1740    1355  0 06:29 ?        00:02:21 kube-apiserver --advertise-address=10.20.56.21 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem --etcd-servers=https://10.20.56.9:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root        7961    7618  0 07:13 pts/0    00:00:00 grep etcd

API svr is referencing to external etcd (--etcd-servers=https://10.20.56.9:2379) in this case

To find out no. of nodes setup for etcd cluster (external one)
etcd-server ~ ✖ ETCDCTL_API=3 etcdctl \
>  --endpoints=https://127.0.0.1:2379 \
>  --cacert=/etc/etcd/pki/ca.pem \
>  --cert=/etc/etcd/pki/etcd.pem \
>  --key=/etc/etcd/pki/etcd-key.pem \
>   member list
381d9d5a79b6660d, started, etcd-server, https://10.20.56.9:2380, https://10.20.56.9:2379, false
etcd-server ~ ➜

cluster1-controlplane ~ ✖ ETCDCTL_API=3 etcdctl snapshot save /opt/cluster1.db --endpoints=https://10.20.56.18:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
Snapshot saved at /opt/cluster1.db

cluster1-controlplane ~ ➜


On the student-node:
First set the context to cluster1:
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".
Next, inspect the endpoints and certificates used by the etcd pod. We will make use of these to take the backup.
student-node ~ ✖ kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls
      --advertise-client-urls=https://10.1.218.16:2379
student-node ~ ➜
student-node ~ ➜  kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep pki
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
    Path:          /etc/kubernetes/pki/etcd
student-node ~ ➜

SSH to the controlplane node of cluster1 and then take the backup using the endpoints and certificates we identified above:
cluster1-controlplane ~ ➜  ETCDCTL_API=3 etcdctl snapshot save /opt/cluster1.db --endpoints=https://10.20.56.18:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key
Snapshot saved at /opt/cluster1.db
cluster1-controlplane ~ ➜

Finally, copy the backup to the student-node. To do this, go back to the student-node and use scp as shown below:
student-node ~ ➜  scp cluster1-controlplane:/opt/cluster1.db /opt
cluster1.db                                                                                                        100% 2088KB 112.3MB/s   00:00
student-node ~ ➜

An ETCD backup for cluster2 is stored at /opt/cluster2.db. Use this snapshot file to carryout a restore on cluster2 to a new path /var/lib/etcd-data-new.
Once the restore is complete, ensure that the controlplane components on cluster2 are running.
The snapshot was taken when there were objects created in the critical namespace on cluster2. These objects should be available post restore.
If needed, make sure to set the context to cluster2 (on the student-node):

Step 1. Copy the snapshot file from the student-node to the etcd-server. In the example below, we are copying it to the /root directory:
student-node ~  scp /opt/cluster2.db etcd-server:/root
cluster2.db                                                                                                        100% 1108KB 178.5MB/s   00:00
student-node ~ ➜

Step 2: Restore the snapshot on the cluster2. Since we are restoring directly on the etcd-server, we can use the endpoint https:/127.0.0.1. Use the same certificates that were identified earlier. Make sure to use the data-dir as /var/lib/etcd-data-new:
etcd-server ~ ➜  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem snapshot restore /root/cluster2.db --data-dir /var/lib/etcd-data-new
{"level":"info","ts":1662004927.2399247,"caller":"snapshot/v3_snapshot.go:296","msg":"restoring snapshot","path":"/root/cluster2.db","wal-dir":"/var/lib/etcd-data-new/member/wal","data-dir":"/var/lib/etcd-data-new","snap-dir":"/var/lib/etcd-data-new/member/snap"}
{"level":"info","ts":1662004927.2584803,"caller":"membership/cluster.go:392","msg":"added member","cluster-id":"cdf818194e3a8c32","local-member-id":"0","added-peer-id":"8e9e05c52164694d","added-peer-peer-urls":["http://localhost:2380"]}
{"level":"info","ts":1662004927.264258,"caller":"snapshot/v3_snapshot.go:309","msg":"restored snapshot","path":"/root/cluster2.db","wal-dir":"/var/lib/etcd-data-new/member/wal","data-dir":"/var/lib/etcd-data-new","snap-dir":"/var/lib/etcd-data-new/member/snap"}
etcd-server ~ ➜

Step 3: Update the systemd service unit file for etcdby running vi /etc/systemd/system/etcd.service and add the new value for data-dir:
[Unit]
Description=etcd key-value store
Documentation=https://github.com/etcd-io/etcd
After=network.target
[Service]
User=etcd
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd-server \
  --data-dir=/var/lib/etcd-data-new \
---End of Snippet---

Step 4: make sure the permissions on the new directory is correct (should be owned by etcd user):
etcd-server /var/lib ➜  chown -R etcd:etcd /var/lib/etcd-data-new
etcd-server /var/lib ➜
etcd-server /var/lib ➜  ls -ld /var/lib/etcd-data-new/
drwx------ 3 etcd etcd 4096 Sep  1 02:41 /var/lib/etcd-data-new/
etcd-server /var/lib ➜

Step 5: Finally, reload and restart the etcd service.
etcd-server ~/default.etcd ➜  systemctl daemon-reload
etcd-server ~ ➜  systemctl restart etcd
etcd-server ~ ➜

Step 6 (optional): It is recommended to restart controlplane components (e.g. kube-scheduler, kube-controller-manager, kubelet) to ensure th


[] Certification Exam Tip!
Here's a quick tip. In the exam, you won't know if what you did is correct or not as in the practice tests in this course.
You must verify your work yourself.
For example, if the question is to create a pod with a specific image,
you must run the the kubectl describe pod command to verify the pod is created with the correct name and correct image.

References
https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md
https://www.youtube.com/watch?v=qRPNuT080Hk


137. 10:00 hrs

[] Security:

- Security Primitives: How does some1 gain access to k8s cluster & how r their actions controlled?
Wht risks & k8s related security measures need to be taken to secure cluster. Security Primitives for
- - Host: all access to host must b secured, by making sure that
- - - route access is disabled
- - - password based authentication is disabled
- - - only SSH key based authentication is made avl
- - API Svr: being center to all k8s ops, we interact wid it thru kubectl or by accessing API directly, to perform any operation on cluster. 1st line of defense, i.e., ctrlling access to API svr
    make 2 types of decisions:
- - - who can access the cluster?
- - - - defined by authentication mechanisms, like user IDs/passwords stored in static files or tokens, certs or integration wid external auth providers like LDAP, svc a/cs
- - - what can they do?
- - - - authorization implemented via RBAC, whr users r associated to grps wid specific perms, other permission modules like attribute-based access ctrl, node authorizers, webhooks etc
- - - all communication with API Svr should b secured wid TLS encryption
- - App to App communication within cluster: By default, all pods can access all other pods within the cluster
- - - restrict access within pods, by using n/w policies

- Authentication Mechanisms avl: How to secure cluster by securing communication b/w internal components & securing mgmt access to cluster thru authentication & authorization mechanisms.
- - all usr access is managed by API svr, by authenticating n then processing request. Diff. authentication mechanisms used. list of user names n password in a
- - - static password file or static token file
- - - - create list of users n passwords in a csv static file n use tht as src for user info. this file has 3 columns, i.e., password, username & userID. 4th column as grpName is optional,
- - - - pass filename to API svr as --basic-auth-file=<fileName> & restart API svr.
- - - - or modify kubeadm pod definition file (/etc/kubenetes/manifests/kube-apiserver.yaml) & tht will take care of API svr restart as well
curl -v -k https://master-node-ip:6443/api/v1/pods -u "<ID>:<password>" #<== supply password in plaintext for static file auth
- - - - similarily we can hv static token file, whr instead of password, u specify token
curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer <token>" --token-auth-file=<fileName> #<== supply token file, istead of password
These r NOT recommended authentication mechanisms. Consider volume mount while providing auth file in kubeadm setup.
Setup Role-based Authorization for new users.

[] Setting up Basic Authentication
Setup basic authentication on Kubernetes (Deprecated in 1.19)
Note: This is not recommended in a production environment. This is only for learning purposes.
Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases
Follow the below instructions to configure basic authentication in a kubeadm setup.
Create a file with user details locally at /tmp/users/user-details.csv
# User File Contents
password123,user1,u0001
password123,user2,u0002
password123,user3,u0003
password123,user4,u0004
password123,user5,u0005

Edit the kube-apiserver static pod configured by kubeadm to pass in the user details.
Modify the kube-apiserver startup options to include the basic-auth file
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
      <content-hidden>
    - --basic-auth-file=/tmp/users/user-details.csv

Create the necessary roles and role bindings for these users:
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
 ---
# This role binding allows "jane" to read pods in the "default" namespace.
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: user1 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
Once created, you may authenticate into the kube-api server using the users credentials
curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"

- - - connect to 3rd party auth protocols like LDAP, Kerberos etc
- - users like (k8s doesnt manage user a/cs natively. it relies on external src like file wid user details or certs or 3rd party ID svc like LDAP to manage users)
- - - admins tht access cluster to perform admin tasks
- - - developers to test or deploy apps
- - - end users tht access app, deployed in cluster. This is managed by apps themselves, internally
- - - 3rd party apps like bots, or otehr processes or svcs, accessing cluster for integration purpose. Svc a/cs used for these by executing
kubectl create serviceaccount user1;
kubectl get serviceaccount
- - - - Service Accounts are not part of CKA curriculum, instead, they are part of CKAD
- - - TLS certs: authenticate using certs: cert-based auth & how various components in k8s cluster r secured using certs


- Default settings in the cluster
- view cfgs of existing cluster

- TLS certs: How various components within cluster r secured using TLS certs?
- - TLS certs: TLS basics
- - - What r TLS certs? Why u need certs? How to configure certs to secure SSH or web svrs?
- - - - A cert is used to guarantee trust b/w 2 parties during a transaction to avoid MITM attack. Ex: wen usr accesses web svr, TLS ensures that comm. b/w usr & svr is encrypted & svr is who it says it is.
- - - How it works?
- - - - encrypt the data passed by end user, using encryption key (set of random numbers n letters), that is added to data & encrypted into a format which cant be recognized.
- - - - Data is then sent to svr. Hacker sniffing the n/w gets data but cant do anything wid it. Same way, svr also cant decrypt data without key.
- - - - So, a copy of key needs to be sent to svr, thru same n/w tht hacker can sniff n gain access to key to decrypt data. This is called symmetric encryption. To avoid this risk, use asymmetric key
- - - - Asymmetric key uses pair of keys, pvt & public key (lock). Trick is, if u lock ur data wid public key (lock), then it can b decrypter only wid associated pvt key, so ur key must alwys b secure wid u
- - - - Another example, ssh access to svr. dont want to use password as those r too risky. use key-pairs.
- - - - - generate public & pvt key using ssh-keygen on svr. secure svr by locking down all access to it, xcpt thru a door tht is locked by ur public key (lock).
- - - - - This is usually done by adding key into svr's authorized_keys file. so, now lock can only b opened wid pvt key
ssh -i id_rsa <user>@<svr> #<== lets u open lock n get in the server
- - - - - for more than 1 svr, create copies of ur public lock & place them on as many svrs as u want, n u can use same pvt key to ssh into all those svrs securely.
- - - - Back to symmetric key example, u can securely transfer ur key to svr using asymmetric encryption, for hacker unable to gain access to key
- - - - Generate public n pvt key pair on svr, in this case, use
openssl genrsa -out <keyName>.key 1024
openssl rsa -in <keyName>.key -pubout > <pubKeyName>.pem
- - - - wen usr accesses svr 1st time, it gets public lock/key frm svr, which hacker also gains access to.
- - - - now, usr's browser encrypts symmetric key using public key provided by svr. Symmetric key is now secure.
- - - - usr now sends this to svr, hacker also gets copy of it.
- - - - svr uses pvtkey to decrypt msg & retrieve symmetric key from it (hacker cant do that n now doesnt has access to symmetric key anymore)
- - - - the symmetric key is now avl to only usr & svr. Now, this symmetric key can b used to encrypt data & send to each other
- - - - So, asymmetric encryption can b used to successfully transmit symmetric key frm usr to svr; & then that symmetric key can b used to exchange all future data comm. b/w usr & svr
- - - - Now, hacker clones svr website n manages to route ur request to his svr, thereby, sending u his svrs public key, to which u encrypt it wid ur symmetric key n start exchanging data. ur data gets hacked!
- - - - To avoid this, wen key is recvd from svr, if a cert is attached to it, it gives u assurance tht its not phishing
- - - - This is TLS cert, actual cert, wid info abt who cert is issued to, public key of tht svr, location of svr etc
- - - - each cert has a name on it, person/subject to whim cert is issued to. this helps validate svr's identity. if svr is accessed wid other names, those names should also b specified in cert
- - - - hacker can also generate cert n claim himself to be real svr. How do u verify if cert is legit?
- - - - Who signed & issues cert (has to be valid Certifying Authority, that takes responsibility). if u generate cert, u sign it by urself, tht it called as self signed cert
- - - - CAs r well-known organizations like Symantec, DigiCert, Comodo, GlobalSign etc tht can sign & validate certs for u.
- - - How this works?
- - - - u generate a CSR (Cert Signing Request) using key u generated earlier & domain name of ur website.
- - - - openssl req -new -key <keyName>.key -out <keyName>.csr -subj "/C=US/ST=CA/O=<companyName>/CN=<websiteName>"
- - - - CAs verify ur details, sign cert & send it back to u. This cert is trusted by browsers
- - - - If hacker tries to get cert verified, he will fail as CAs will reject it.
- - - - CAs use diff. techniques to ensure u r actual owner of tht domain
- - - How do browsers know tht CA itself is legitimate?
- - - - CAs themselves hv a set of pvt & public key-pairs. CAs use their pvt keys to sign certs. Public keys of all CAs r built into browsers.
- - - - Browser uses public key of CA to validate tht cert is actually/legitimately signed by CA themselves
- - - pvt CAs
- - - - for hosting own internal sites like payroll or internal email apps etc, u can host ur own pvt CA.
- - - - Most of public CAs do provide pvt hosting svc, by deploying CA svr internally, get public key of internal CA svr installed on all employees' browser & establish secure connectivity with org.

- - Summary:
- - - To encrypt msgs, use asymmetric encryption, wid a pair of publ n pvt keys.
- - - Admin generates key pair (ssh-keygen) to secure SSH access to svrs.
- - - svr generates key pair (openssl) to secure HTTPS traffic.
- - - - For this, svr send CSR to CA. CA generates its own key pair & uses its pvt key to sign cert n send it back to svr. Svr then configures app wid signed cert
- - - - wenevr a usr access app over HTTPS, svr 1st sends cert wid its public key. usr's browser reads cert & uses CA's public key to validate & retrieve svr's public key
- - - - end user's browser then only generates a symmetric key to b used for all future comm.
- - - - This symmetric key is encrypted wid svr's public key & sent back to svr. Svr uses its pvt key to decrypt msg & retrieve symmetric key
- - - - symmetric key is now used for comm. once trust is establish, end user can use his creds to authenticate to svr's app.
- - - now wid svr's key-pairs, client has validated it to be legitimate.
- - - However, svr has not validated yet if client/user is legitimate. it could b a hacker as well, impersonating a client
- - - to trust client, svr can request cert frm client.
- - - So, client must generate pair of keys & submit CSR to CA, with CA to validate, sign & send back cert to client; tht can b sent to svr for legitimacy.
- - - we hv nvr generated TLS certs to access a website. Thats cuz TLS client certs r not generally implemented on web servers. EVen if they r, its all implemented under hosts
- - - a normal user doesnt need to generate & manage certs manually.
- - PKI (Public Key Infrastructure) -- whole infrastructure, including CA, svrs, ppl & process of generating, distributing & maintaining digital certs is known as PKI
- - asymmetric: u can encrypt data wid any of the keys, be it pvt or public & can decrypt data with the other. So, be vry careful to decide in which scnario data needs to be encrypted wid pvt or public.
- - - like client sending data, should be wid public key, as only svr should hv pvt key to decrypt it
- - - client trying to SSH should encrypt wid pvt key, as svr is locked wid public key for all.
- - certs wid public key r usually named as *.crt or *.pem. certs wid pvt key usually have *.key or *-key.pem.
- - - pvt key will hv 'key' word specified in its filename, either as *.key or as *-key.pem
- - Svr certs (configured on svr), Root certs (configured on CA, like CA uses pvt key to sign cert), client certs (configured on clients)

- - TLS in k8s: How do we secure k8s cluster wid TLS certs? How does k8s use certs?
- - - Primary reqmnt is for all svrs to use svr certs & all clients to use client certs.
- - - All comm.s need to be encrypted, be it frm admin -> API svr, or API svr -> scheduler -> API svr -> cluester of worker nodes
- - - svr certs for svrs
- - - - API svr exposes its svc as HTTPS, to other components as well as to external users/clients. So generate a cert & key/pair as APIserver.crt & APIserver.key
- - - - etcd svr stores all info abt cluster. so, it reqs pair of cert n key for itself. generate as etcdserver.crt & etcdserver.key
- - - - kubelet on worker nodes, also exposes as HTTPS API endpoint tht API svr talks to, to interact wid worker nodes. generate as kubelet.crt & kubelet.key
- - - client certs for clients
- - - - admins tht use kubectl thru REST API. admin reqs a client cert & keypair to authenticate to API svr. generate as admin.crt & admin.key
- - - - scheduler talks to API svr to look for pods tht require scheduling & then get API svr to schedule pods to right worker nodes. For APi svr, scheduler is just another client. so, scheduler needs to validate its identity using client TLS cert. generate as scheduler.crt & scheduler.key
- - - - kube-ctrller-mgr accesses API svr (controller-manager.crt & controller-manager.key)
- - - - kube-proxy (kube-proxy.crt & kube-proxy.key)
- - - - API svr, is the only component tht talks to etcd. For etcd, API svr is a client & needs to authenticate. API svr can use same keys tht it used earlier for serving its own API svc, or u can generate new 1 just for etcd, as apiserver-etcd-client.crt & apiserver-etcd-client.key
- - - - API svr also talks to kublet svr, to monitor worker nodes. can use original certs or generate new ones as apiserver-kubelet-client.crt & apiserver-kubelet-client.key

- - How to generate above-specified certs for cluster?
- - - we need CA to sign all these certs. k8s reqs u to hv at least 1 CA for ur cluster. better to hv 1 for all components & 1 separately just for etcd. In tht case, etcd svr certs & client certs (API svr) will b all signed by etcd svr CA.
- - - CA, has its own set of cert & key, lets call it as ca.crt & ca.key
- - TLS cert generation: Diff. tools avl to generate certs, like easyrsa, cfssl, openssl etc
- - - start wid CA's cert generation
openssl genrsa -out ca.key 2048
openssl -req -new -key ca.key -suj "/CN=KUBERNETES-CA" -out ca.csr #<== this csr is like a cert wid all ur details but wid no signature. We specify name of component tht this cert is for, in CN field.
openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt #<== we sign cert ourselves using pvt key generated above. Since this is for CA itself, itse self-signed by CA using own pvt key
- - - Next, generate client certs
openssl genrsa -out admin.key 2048
openssl -req -new -key admin.key -suj "/CN=kube-adm/O=system:masters" -out admin.csr #<== this csr is like a cert wid all ur details but wid no signature. We specify name of component tht this cert is for, in CN field.
# system:masters as grp name separates admin usr from rest of users.
openssl x509 -req -in admin.csr -signkey ca.key -out admin.crt #<== here we sign wid pvt ca.key. signed cert admin.crt is used by admin to authenticate to k8s cluster
- - - - This process is similar to creating user IDs n passwords. crt is ID and key is password
- - - - all system components must hv their name pre-fixed with keyword system:kube-<componentName> in CN.
- - - - all components need copy of CA's root cert (public key), i.e., ca.crt to verify/validate each other as client
- - - - now these certs can be used instead of providing userName/password in REST API call to API svr >>
curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt
# another way is to move all these certs in kube-config.yaml & specify API svr endpoint details for certs to be used etc. This is wht most k8s clients use.
==========================================
apiVersion: v1
clusters:
- cluster:
    certificate-authority: ca.crt
    server: https://kube-apiserver:6443
  name: kubernetes
kind: Config
users:
- name: kubernetes-admin
  user:
    client-certificate: admin.crt
    client-key: admin.key
==========================================
- - - Next, generate server side certs
- - - - etcd is deployed as a cluster, so, generate peer certs as well n specify those in etcd.yaml like >>
==========================================
- etcd
  - --advertise-client-urls=https://127.0.0.1:2379
  - --key-file=/etc/kubernetes/pki/etcd/etcdserver.key
  - --cert-file=/etc/kubernetes/pki/etcd/etcdserver.crt
  - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
  - --client-cert-auth=true
  - --data-dir=/var/lib/etcd
  - --initial-advertise-peer-urls=https://127.0.0.1:2380
  - --initial-cluster=master=https://127.0.0.1:2380
  - --listen-client-urls=https://127.0.0.1:2379
  - --lister-peer-urls=https://127.0.0.1:2380
  - --name=master
  - --peer-key-file=/etc/kubernetes/pki/etcd/etcdpeer1.key
  - --peer-cert-file=/etc/kubernetes/pki/etcd/etcdpeer1.crt
  - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
  - --peer-client-cert-auth=true
  - --snapshot-count=10000
==========================================
- - - - API svr: as API svr is center of all comm.s, it is referred by multiple names. Make sure to specify all names by defining in openssl.cnf file
==========================================
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation,
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubenetes
DNS.2 = kubenetes.default
DNS.3 = kubenetes.default.svc
DNS.4 = kubenetes.default.svc.cluster.local
IP.1 = 10.96.0.1 # Pod IP
IP.2 = 172.17.0.87 # Node IP
==========================================

openssl genrsa -out apiserver.key 2048
openssl -req -new -key apiserver.key -suj "/CN=kube-apiserver" -out apiserver.csr  --config openssl.cnf
openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt

# Now pass cert details in /etc/kubernetes/manifest/kube-apiserver.yaml
--client-ca-file=/var/lib/kubernetes/ca.pem
--tls-cert-file=/var/lib/kubernetes/apiserver.crt
--tls-private-key=/var/lib/kubernetes/apiserver.key
# also specify client details for all components like >>
--etcd-cafile=/var/lib/kubernetes/ca.pem
--etcd-certfile=/var/lib/kubernetes/apiserver-etcd-client.crt
--etcd-keyfile=/var/lib/kubernetes/apiserver-etcd-client.key

--kubelet-cafile=/var/lib/kubernetes/ca.pem
--kubelet-certfile=/var/lib/kubernetes/apiserver-kubelet-client.crt
--kubelet-keyfile=/var/lib/kubernetes/apiserver-kubelet-client.key

- - - - Kubelet svr: its HTTPS API svr, tht runs on each node, responsible for managing the node.
- - - - - Thats who API svr talks to, to monitor the node as well as info abt wht pods to schedule on which node.
- - - - - key cert pair on each node in the cluster.Name them as nodeName in CN. Once ready, specify thes in kubelet-config.yaml file
================================================
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubernetes/kubelet-node01.crt"
tlsPrivateKeyFile: "/var/lib/kubernetes/kubelet-node01.key"
================================================

- - - - client certs also need to be generated for kubelet to authenticate to API svr while responding back
- - - - - API svr needs to know which node is authenticating n give it right set of perms. So it requires nodes to have right names in right formats
- - - - - Since nodes r also system components like scheduler, ctrller-manager etc, names start wid system: keyword as prefix, i.e., system:node:<nodeName>
- - - - - How will API svr give right set of perms? By giving grp name, just like we gave it for admin user.Use grp name as SYSTEM:NODES
- - - - once certs r generated, those get into kube cfg file, as shown earlier.

- - How to view certs in existing cluster? How to perform health check of certs in entire cluster?
- - - Imp to know how cluster was setup, hard way or via kubeadm?
- - - - Hard way, all certs need to be generated urself. all native svcs r deployed as nodes in hard way
- - - - kubeadm takes care of automatically generating certs & configuring cluster for u. all native svcs r deployed as pod by kudeadm
- - - lets look at cluster provisioned by kubeadm as example, as a heath check, start by identifying all certs used in the system.
- - - - cmd used to start any component like api svr, will hv all the info abt all certs it uses.
- - - - next, lok inside each cert to find more details abt it, by executing below cmd for each cert >>
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -test -noout #<== cmd to decode cert details
# start wid CN in Subject section, then alternate names at the bottom usually, as X509v3 Subject Alternative Name:
# chk Validity section to see expiry date n then issuer of cert, this should be CA who issues this cert
- - - for troubleshooting, start looking at logs, like kubectl logs etcd-master,
- - - sometimes core components like API svr or etcd r down, kubctl wont function
- - - - in that case, go 1 lvl down, i.e., to doker, to fetch logs, list all containers using docker ps -a cmd n then view logs via docker logs <containerID> cmd
- - - How to configure certs by kubeadm tool?
- - - How to troubleshoot issues related to certs?

Resource: Download Kubernetes Certificate Health Check Spreadsheet
https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools


149. 11:18 hrs

- - Certificates API:
- - - kubeadm creates CA pair of files n stores it on master node.
- - - k8s provides certificates utility to sign/approve certs.
- - - controller mgr performs all cert-related operations. It has controllers named CSR-APPROVING, CSR-SIGINING, responsible for carrying out specific tasks
kubectl get csr
kubectl certificate approve <csrName> #<== k8s signs csr using CA keypair n generates signed cert for user. This cert can b extracted n shared wid user
kubectl get csr <csrName> -o yaml #<-- copy output under certificate:
# decode the output as thats in bas64 encoded format to get cert in plaintext format n then share wid end user
- - - ctrller mgr has 2 options whr u can specify root cert n pvt key, at /etc/kubernetes/manifests/kube-controller-manager.yaml >>
==========================
spec:
  containers:
    - kube-controller-manager
    - --address=127.0.0.1
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
==========================

Create CSR for akshay >>
Use this command to generate the base64 encoded format as following: -
cat akshay.csr | base64 -w 0 #<== prints output in single line
Finally, save the below YAML in a file and create a CSR name akshay as follows: -
---
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  groups:
  - system:authenticated
  request: <Paste the base64 encoded value of the CSR file>
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth


kubectl apply -f akshay-csr.yaml

controlplane ~ ➜  kubectl certificate deny agent-smith
certificatesigningrequest.certificates.k8s.io/agent-smith denied

controlplane ~ ➜
controlplane ~ ➜  kubectl get csr
NAME          AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
agent-smith   103s    kubernetes.io/kube-apiserver-client           agent-x                    <none>              Denied
akshay        3m31s   kubernetes.io/kube-apiserver-client           kubernetes-admin           <none>              Approved,Issued
csr-8hs5l     18m     kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   <none>              Approved,Issued

controlplane ~ ➜
controlplane ~ ➜  kubectl delete csr agent-smith
certificatesigningrequest.certificates.k8s.io "agent-smith" deleted

controlplane ~ ➜  kubectl get csr
NAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION
akshay      3m59s   kubernetes.io/kube-apiserver-client           kubernetes-admin           <none>              Approved,Issued
csr-8hs5l   19m     kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   <none>              Approved,Issued

controlplane ~ ➜


[]  Kubeconfig:
- to avoid giving cert details like svr, client-key, client-cert & CA, for each kubectl cmd, to get tht command authenticated by API svr,
- - better way is to specify all those args in kubeconfig file, as by default, kubectl looks for a file named config, under $HOME/.kube/config
- kubeconfig file is in a specific format. It has 3 sections:
- - Clusters: various k8s clusters tht u can access to
- - Users: are user accounts with which u hv access to these clusters. These users can hv diff. privileges across diff. clusters
- - Context: defines which user will be able to access which cluster. This way, no need to define user certs for each kubectl cmd
- each of the above sections is in array format, to specify multiple clusters/users within same file
- NO NEED to create any object by running kubectl apply -f <filename>, as this is directly read by kubectl cmd
Example:
========================================
apiVersion: v1
kind: Config

current-context: admin@test # default context

clusters:
- name: test
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
# or u can use certificate-authority-data: <base64encoded_content_of_ca.cert>
    server: https://kube-apiserver:6443
- name: dev
- name: prd

context:
- name: admin@test
  context:
    cluster: test
    user: admin
    namespace: default
- name: dev-user@dev
- name: prd-user@prd

users:
- name: admin
  user:
    client-certificate: /etc/kubernetes/pki/admin.crt
    client-key: /etc/kubernetes/pki/admin.key
- name: dev-user
- name: prd-user
========================================

kubectl config view #<== shows content of default kubeconfig file, located under $HOME/.kube/config file
kubectl config view --kubeconfig=<customConfigFile> #<== shows content of file passed
kubectl config use-context dev-user@dev #<== to switch to another context as default context

controlplane ~ ✖ kubectl config --kubeconfig=my-kube-config use-context research
Switched to context "research".

controlplane ~ ➜  kubectl config --kubeconfig=my-kube-config current-context
research

controlplane ~ ➜  cp ~/.kube/config ~/.kube/config-orig
controlplane ~ ➜  cp my-kube-config ~/.kube/config


[] API Groups: wht is k8s API? k8s API is grouped into multiple grps, based on needed purpose, like >>
- /metrics & /healthz APIs for monitoring health of cluster,
- /logs (for integrating wid 3rd part logging apps) etc
- /version : is for viewing version of the cluster
- APIs responsible for cluster functionality
- - /api  : core  group, under /v1; whr all core functionality exists, like, ns, po, rc, events, endpoints, no, bindings, PV, PVC, cm, secrets & svc
- - /apis : named group, more organized, with newer features avl here. It has groups under it, for
- - - API Groups : Resources, under /v1; each resource has Verbs (a set of actions) associated with it
- - - /apps : within apps it has, under /v1; /replicasets, /statefulsets, /deployments (n then under /deployments; is /list, /get, /create, /delete, /update, /watch),
- - - /extensions :
- - - /networking.k8s.io : under /v1; /networkpolicies
- - - /authentication.k8s.io :
- - - /certificates.k8s.io : under /v1; /certificatesiginingrequests
curl https://kube-master:6443/version # <== lists versions
curl https://kube-master:6443/api/v1/pods # <== provides list of pods
curl http://localhost:6443 -k # <== will list all API groups
curl http://localhost:6443/apis -k | grep "name" # <== returs all supported resource groups
- u will need to authenticate urself wid certs, to access details from APIs
- another option is kubectl proxy cmd, tht launches a proxy svc locally on port 8001 & uses creds n certs frm ur kube cfg file to access cluster
curl http://locahost:8001 -k # <== will give u details as root user, after pxy fwds ur request to API svr, picking ur creds n certs for auth
- kubectl proxy : is HTTP pxy svc, to access to API svr, by picking ur creds n certs for auth, from kube config file
- kube proxy    : is used to enable connectivity b/w pods & svcs across diff. nodes in cluster

[] Authorization Mechanisms avl: RBAC : once someone gains access, wht can they do, thats wht authorization defines
- authorization is done to provide minimum lvl of access to perform reqd operation for other users
- wen we share our cluster across orgs, or teams, by logically partitioning it using namespaces, we can restrict access to users to their ns alone
- Diff. Authorization mechanisms, supported by k8s:
- - node-based authorization (for internal access only) : any request coming frm a usr wid name system:node:<Name> & part of SYSTEM:NODES grp, is authorized by Node Authorizer & is granted these privilges, within the cluster
- - - kube API svr is accessed by users for mgmt purpose as well as by kubelets within cluster mgmt process
- - - kubelet accesses API svr to read info abt svcs & points, nodes & pods. kubelet also reports to API svr with info abt node, like its status
- - - API Requests used by kubelet Read (svcs, endpoints, no, po) & Write (no status, po status, events)
- - - These requests r handled by spl authorizer known as Node Authorizer
- - ABAC (Attribute Based Authorization Ctrl):
- - - external access: for users, attribute-based authorization is whr u associate a user or grp of users wid a set of perms to create/view pod, by writing a set of policies, defined in JSON format
{ "kind": "Policy", "spec": { "user": "dev-user", "namespace": "*", "resource": "pods", "apiGroup": "*" } }
- - - If we pass multiple policy files wid similar above contents, into API svr; then wid each future change, would require restart of API svr. To avoid this, use RBAC.
- - RBAC: instead of associating a usr or a grp wid a set of perms, we define a role wid set of perms reqd, then we associate all users like developers to tht role.
- - - This way, for any future changes, only role needs to b updated & it reflects for all users, associated wid tht role, immediately
- - - RBAC provides a more standard approach to manage access within k8s cluster.
- - webhooks: to outsource all authorization-based mechanisms, i.e., manage authorization externally.
- - - Ex: Open Policy Agent is 3rd party tool, tht helps wid admission ctrl & authorization. k8s can make an API call to OPA & have OPA decide if user should b permitted or not, based on tht, usr is granted access
- - AlwaysAllow: allows all requests without performing any authorization checks.
- - AlwaysDeny: denies all requests.
- whr do u configure these mechanisms/modes? which of them r active by default? can u hv more than one at a time? how does authorization work if multiple modes r configured?
- - modes r set using --authorization-mode option on API Svr. Default value is set to AlwaysAllow, u can provide comma-separated values, for multiple modes to be used like >>
--authorization-mode=Node,RBAC,Webhook
- - wen multiple modes r specified, request is authorized with each mode, in the sequence defined in --authorization-mode.
- - wenevr a module denies a request, it is fwded to next one in chain. As soon as 1 module approves request, no more check r done.
- - Ex: wen a usr sends a request,
- - - its 1st handled by Node Authorizer denies the request, as it handles only node requests
- - - RBAC performs its checks & grants user permission, authorization is complete n user us given access to requested object

158. 12:04 hrs

[] RBAC: Roles & Role bindings r namespaced, tht is their scope is within a single namespace, if not defined, get created in default ns
- create a role by creating a role object like below and specifying rules for specific resources >>
==================================
developer-role.yaml
==================================
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""] # <== leaving this blank, grants access to core group, for any other API groups, specify name here
  resources: ["pods"]
  verbs: ["list", "get", "create", "update", "delete"]
  resourceNames: ["blue", "orange"] # <== gives access to specific pods labelled as blue or orange only
- apiGroups: [""]
  resources: ["ConfigMap"]
  verbs: ["create"]
==================================
kubectl create -f developer-role.yaml

- Next step is to link user object to that role. For this, create another object called RoleBinding
========================================
devuser-developer-binding.yaml
========================================
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects: # <== this is whr we specify user details
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef: # <== this is whr we specify details of the role we created
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
========================================
kubectl create -f devuser-developer-binding.yaml
kubectl get roles
kubectl describe role <roleName> # <== shows details abt resources & perms for each resource
kubectl get rolebindings
kubectl describe rolebinding <bindingName>
# to chk wht access u hv in the cluster. This auth cmd gives output as yes or no
kubectl auth can-i create deployments
kubectl auth can-i describe nodes
# if u r admin, u can impersonate another user to chk perms
kubectl auth can-i create deployments --as <userName>
kubectl auth can-i create deployments --as <userName> -n test # <== to chk if specific user has access to particular ns


controlplane ~ ➜  kubectl describe po kube-apiserver-controlplane -n kube-system | grep auth
      --authorization-mode=Node,RBAC
      --enable-bootstrap-token-auth=true

controlplane ~ ➜  kubectl describe po kube-apiserver-controlplane -n kube-system | grep authorization
      --authorization-mode=Node,RBAC

controlplane ~ ➜
controlplane ~ ✖ kubectl get roles -A --no-headers
blue          developer                                        2023-01-26T03:56:25Z
kube-public   kubeadm:bootstrap-signer-clusterinfo             2023-01-26T03:48:39Z
kube-public   system:controller:bootstrap-signer               2023-01-26T03:48:38Z
kube-system   extension-apiserver-authentication-reader        2023-01-26T03:48:38Z
kube-system   kube-proxy                                       2023-01-26T03:48:40Z
kube-system   kubeadm:kubelet-config                           2023-01-26T03:48:38Z
kube-system   kubeadm:nodes-kubeadm-config                     2023-01-26T03:48:38Z
kube-system   system::leader-locking-kube-controller-manager   2023-01-26T03:48:38Z
kube-system   system::leader-locking-kube-scheduler            2023-01-26T03:48:38Z
kube-system   system:controller:bootstrap-signer               2023-01-26T03:48:38Z
kube-system   system:controller:cloud-provider                 2023-01-26T03:48:38Z
kube-system   system:controller:token-cleaner                  2023-01-26T03:48:38Z

controlplane ~ ➜  kubectl get roles -A --no-headers | wc -l
12
controlplane ~ ✖ kubectl describe role kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs  Resource Names  Verbs
  ---------   -----------------  --------------  -----
  configmaps  []                 [kube-proxy]    [get]

controlplane ~ ➜

# Which account is the kube-proxy role assigned to?
controlplane ~ ➜  kubectl get rolebindings -A
NAMESPACE     NAME                                                ROLE                                                  AGE
blue          dev-user-binding                                    Role/developer                                        7m17s
kube-public   kubeadm:bootstrap-signer-clusterinfo                Role/kubeadm:bootstrap-signer-clusterinfo             15m
kube-public   system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               15m
kube-system   kube-proxy                                          Role/kube-proxy                                       15m
kube-system   kubeadm:kubelet-config                              Role/kubeadm:kubelet-config                           15m
kube-system   kubeadm:nodes-kubeadm-config                        Role/kubeadm:nodes-kubeadm-config                     15m
kube-system   system::extension-apiserver-authentication-reader   Role/extension-apiserver-authentication-reader        15m
kube-system   system::leader-locking-kube-controller-manager      Role/system::leader-locking-kube-controller-manager   15m
kube-system   system::leader-locking-kube-scheduler               Role/system::leader-locking-kube-scheduler            15m
kube-system   system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               15m
kube-system   system:controller:cloud-provider                    Role/system:controller:cloud-provider                 15m
kube-system   system:controller:token-cleaner                     Role/system:controller:token-cleaner                  15m

controlplane ~ ➜  kubectl describe rolebinding kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  kube-proxy
Subjects:
  Kind   Name                                             Namespace
  ----   ----                                             ---------
  Group  system:bootstrappers:kubeadm:default-node-token

controlplane ~ ➜
# A user dev-user is created. User's details have been added to the kubeconfig file.
# Inspect the permissions granted to the user. Check if the user can list pods in the default namespace.
controlplane ~ ➜  kubectl auth can-i get pods --as dev-user
no

# Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.
# Copying similar role from another namespace..
controlplane ~ ✖ kubectl get role developer -n blue -o yaml > developer.yaml

controlplane ~ ➜  cat developer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2023-01-26T03:56:25Z"
  name: developer
  namespace: blue
  resourceVersion: "1064"
  uid: 3c746519-8fcc-46bc-ab8c-76d092832218
rules:
- apiGroups:
  - ""
  resourceNames:
  - blue-app
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete

controlplane ~ ➜

controlplane ~ ➜  kubectl get rolebinding dev-user-binding -n blue -o yaml > developer-binding.yaml

controlplane ~ ➜  vim developer-binding.yaml

controlplane ~ ➜  kubectl create -f developer-binding.yaml
rolebinding.rbac.authorization.k8s.io/developer-binding created

controlplane ~ ➜  kubectl get roles
NAME        CREATED AT
developer   2023-01-26T04:09:38Z

controlplane ~ ➜  kubectl describe role developer
Name:         developer
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 []              [list create delete]

controlplane ~ ➜  kubectl get rolebindings
NAME                ROLE             AGE
developer-binding   Role/developer   35s

controlplane ~ ➜  kubectl describe rolebinding developer-binding
Name:         developer-binding
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  developer
Subjects:
  Kind  Name      Namespace
  ----  ----      ---------
  User  dev-user

controlplane ~ ➜  kubectl auth can-i list pods --as dev-user
yes

# Also try to see error msg>>
kubectl get pods --as dev-user

controlplane ~ ➜

# To create a Role:-
kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods
# To create a RoleBinding:-
kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user

===========================================
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
===========================================

# A set of new roles and role-bindings are created in the blue namespace for the dev-user.
# However, the dev-user is unable to get details of the dark-blue-app pod in the blue namespace.
# Investigate and fix the issue.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2023-01-26T03:56:25Z"
  name: developer
  namespace: blue
  resourceVersion: "3259"
  uid: 3c746519-8fcc-46bc-ab8c-76d092832218
rules:
- apiGroups:
  - ""
  resourceNames:
  - blue-app
  - dark-blue-app  # <== added, as per requirment
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete

# Add a new rule in the existing role developer to grant the dev-user permissions to create deployments
# in the blue namespace.
Add the following to existing role >>
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete

controlplane ~ ➜  kubectl get pods --as dev-user
NAME                   READY   STATUS    RESTARTS   AGE
red-84c985b67c-q8k7g   1/1     Running   0          36m
red-84c985b67c-spxdz   1/1     Running   0          36m

controlplane ~ ➜  kubectl get rc --as dev-user
Error from server (Forbidden): replicationcontrollers is forbidden: User "dev-user" cannot list resource "replicationcontrollers" in API group "" in the namespace "default"

controlplane ~ ✖


[] Cluster Roles & Role Bindings: help authorize a user across cluster-scoped resources. These can b used for namedspaced as well
- cluster-wide or cluster-scoped resources like nodes cannot b associated to a specific ns
- resources r categorized as namespaced or cluster-scoped.
- - namespaced: po, rs, jobs, deploy, svc, secrets, cm, roles, rolebindings, PVC etc
- - cluster-scoped: nodes, PV, clusterroles, clusterrolebindings, certificatesiginingrequests, ns etc
kubectl api-resources --namespaced=true # <== to see list of namespaced resources
kubectl api-resources --namespaced=true # <== to see list of cluster-scoped resources
========================================
cluster-admin-role.yaml
========================================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list", "create", "get", "delete"]
========================================
kubectl create -f cluster-admin-role.yaml

========================================
cluster-admin-role-binding.yaml
========================================
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: clusteradmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
- kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
========================================
kubectl create -f cluster-admin-role-binding.yaml

# A new user michelle joined the team. She will be focusing on the nodes in the cluster.
# Create the required ClusterRoles and ClusterRoleBindings so she gets access to the nodes.
==========================================
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io
============================================

controlplane ~ ➜  diff node-admin-role-binding.yaml node-admin-role-binding1.yaml
--- node-admin-role-binding.yaml
+++ node-admin-role-binding1.yaml
@@ -1,5 +1,5 @@
-kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
+kind: ClusterRoleBinding
 metadata:
   name: michelle-binding
 subjects:
@@ -7,6 +7,6 @@
   name: michelle
   apiGroup: rbac.authorization.k8s.io
 roleRef:
-  kind: ClusterRole
+- kind: ClusterRole
   name: node-admin
   apiGroup: rbac.authorization.k8s.io

controlplane ~ ✖

# michelle's responsibilities are growing and now she will be responsible for storage as well.
# Create the required ClusterRoles and ClusterRoleBindings to allow her access to Storage.
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io


kubectl api-resources to chk what apiVersion needs to be used for each object/resource
use imperative for these temp role for faster work, like >>
kubectl create clusterrole storeage-admin --resource=persistentvolumes,storageclasses --verb=create,get,watch,list
kubectl create clusterrolebinding michelle-storage-admin --user=michelle --clusterrole=storage-admin

[] Svc a/cs: only need to know how to work wid svc a/cs for CKA
- usr a/c for admin to access cluster to perform admin tasks or a developer to deploy apps etc
- svc a/c is a/c used by an app to interact wid k8s cluster.
- - Ex: monitoring app like Prometheus uses svc a/c to pull k8s API for performance metrics
- - Automated build tool like Jenkins uses svc a/c to deploy apps on k8s cluster
- to create svc a/c
kubectl create serviceaccount <svcaccountName>
kubectl describe serviceaccount <svcaccountName>
kubectl describe secret <tokenName> # <== fetch this frm above command's output next to Tokens:
- wen svc a/c is created,
- - it also creates a token automatically, to be used by external app, while authenticating to k8s API
- - This token is then stored as a secret object.
- - This secret object is then linked to svc a/c
- so, create svc a/c, assign needed RBAC, with token generated & stored as secret & use it in 3rd part app for auth to API svr
- wht if 3rd party app is hosted on k8s cluster itself?
- - automatically mount svc token secret as a volume inside the pod hosting 3rd party app, for app to read it
- default svc a/c
- - for each ns, a svc a/c named default is automatically created
- - for each pod created, default svc a/c & its token is automatically mounted to pod as a volume mount under default location >>
/var/run/secrets/kubernetes.io/serviceaccount
- - so if u do ls to chk tht path inside pod, u will see secret mounted wid 3 files, ca.crt, namespace & token
kubectl exec -it <podName> ls /var/run/secrets/kubernetes.io/serviceaccount
- - This default svc a/c is vry restricted, has perms to run only basic k8s API queries
- - This default auto mounting can b disabled by explicitly specifying automountServiceAccountToken: false, under spec.containers:
- to use a different svc a/c, include svc a/c field serviceAccountName: <svcaccountName>, under spec:
- to decode a token, execute >>
jq -R 'split(".")' | select(length>0) | .[0],.[1] | @base64 | fromjson' <<< <tokenContent>
or copy paste token in jwt.io website to decode it. no expiry date specified means this jwt token is valid as long as svc a/c exists
- from 1.22 onwards, token is issues wid defined lifetime is generated thru tokenRequestAPI by svc a/c admission ctrller, wen pod is created
- - then this token is mounted as a projected volume on the pod like this >>
==============================
projected:
  defaultMode: 420
  sources:
  - serviceAccountToken:
      expirationSeconds: 3607
      path: token
==============================
- from 1.24 onwards, another enhancement is made, no more automatic generation of token for svc a/c
- - if u need it, request it explicitly as kubectl create token <svcAccountName>, n this gets generated wid default expiry time of 1hr
- - so if u still need token to be mounted, ass the following annotation, under metadata >>
annotations:
  kubenetes.io/service-account.name: <svcAccountName> #<== this will create a non-expiring token

controlplane ~ ➜  kubectl create sa dashboard-sa
serviceaccount/dashboard-sa created

controlplane ~ ➜  kubectl create token dashboard-sa
eyJhbGciOiJSUzI1NiIsImtpZCI6Inp0MHcxZWstQnZSNlNXNjJyR0UzT2FJZFVtVzI0QnJTLXh0blNnVzg3dkEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiLCJrM3MiXSwiZXhwIjoxNjc0NzE3OTI5LCJpYXQiOjE2NzQ3MTQzMjksImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0Iiwic2VydmljZWFjY291bnQiOnsibmFtZSI6ImRhc2hib2FyZC1zYSIsInVpZCI6ImJiZDM1OGU0LWViYmMtNDIxNy04YWExLTk2ZThkOWEyZDllMSJ9fSwibmJmIjoxNjc0NzE0MzI5LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQtc2EifQ.G5gboW7xbVSoZEkL9Yge-XY-kns-x992JyVTVdy_Ie0A7kjV-8QwyqGzi55BTlntfnVBu47lMo6YPVS7FtiUOCfUvcUoVBFAaaqgCewWAUimIfbflwd_umRH-I2XH1fFjxp_oM21snnmsYMXog2tMU1j8s94X-TmqZvyXdumkOvvKcph30aPJn8_aKLt5M9Z6ngq0EFeFC8z1Z-pYYadbFxhx2wmGdeBYHCzEdR2ntWAfN66Bc8A0q33cAKuojbluYw6_j69-EBuZCf3HbMicMuowdyqNFF0snkRuzqSf-BgmcWke1212F_EgYnvqAeD7LhClYpHM6xlussC2PUTmA

[] Image Security: Securing image repos
- basics of image names
- - image: nginx --> gets pulled by default frm docker's image repo, docker hub, avl at docker.io
- - default top lvl directory for image is library. images maintained by docker's team r loaded under library as default parent/root folder
- - pvt registry, to run a container using pvt image, 1st need to login to pvt registry, using docker login private-registry.io
- - - for this, set image: private-registry.io/apps/internal-app
- - - how to implement auth part to pvt registry? how does k8s gets creds to access pvt registry? how do u pass creds to docker runtime on worker nodes?
- - - - for this, 1st create a secret object, of built-in secret type docker-registry (built for storing docker creds) , wid creds in it like >>
kubectl create secret docker-registry regcred --docker-server=private-registry.io --docker-username=<ID> --docker-password=<passwd> --docker-email=<email>
- - - - then, specify this secret inside pod definition yaml, under spec:
imagePullSecrets:
- name: regcred

169. 13:06 hrs

[] Security contexts:
- Docker:
- - wen u run docker container, u can define set of security standards like
- - - ID of user to run container, by specifying in cmd like docker run --user=jas nginx or specify USER jas in Dockerfile (then no need to specify user in cmd)
- - - Linux capabilities using --cap-add MAC_ADMIN or --cap_drop MAC_ADMIN or --privileged MAC_ADMIN while executing docker run
- k8s: All these can b configured in k8s as well, as in k8s, containers r encapsulated in pods, u can set security settings @ container lvl or @ pod lvl
- - to set security context at pod lvl, add this under spec:
spec:
  securityContext:
    runAsUser: jas
    capabilities:
      add: ["MAC_ADMIN"]
      drop: ["WINDOWS_ADMIN"]
      privileged: [:LINUX_ADMIN]
- - to set this at container lvl, add this under spec.container:

controlplane ~ ➜  cat multi-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]

spec:
  containers:
  - command:
    - sleep
    - "4800"
  image: ubuntu
  securityContext:
    runAsUser: 1010
    capabilities:
      add: ["SYS_TIME", "NET_ADMIN"]


[] n/w policies:
- n/w solutions tht support n/w policy: kube-router, Calico, Romana, Weave-net
- n/w solutions tht DO NOT support n/w policy: Flannel
- ingress (inbound) vs egress (outbound) traffic.
- - Traffic from external world to 1st hit is ingress.
- - internal traffic frm 1st hit till DB is egress, basically outgoing request from app svr to DB is egress traffic
- k8s is by default configured wid allAllow rule, that allows traffic frm any to pod to another pod/svc within cluster, encapsulating entire setup in same virtual pvt n/w
- n/w policy needs to be setup to rectrict connectivities. n/w policy is another object in ns just like pods, rs, svc etc
- u link a n/w policy to 1 or more pods, using labels & selectors, and define rules within n/w policy n specify policy type as ingress or egress
label the pod as >>
labels:
  role: db

====================
network-policy.yaml
====================
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
  namsepace: prod
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:  # <== if this is not defined, then all pods in prod ns will be accessible from staging ns
        matchLabels:
          name: pod1
    namespaceSelector:  # <== ensures tht traffic frm 1 namespace can access to this namespace's DB
        matchLabels:
          name: staging
    - ipBlock:          # <== allows specified external IP to access DB, for DB backups
        cidr: 192168.5.10/32
    ports:
    - protocol: TCP
      port: 3306
   egress:
   - to:
     - ipBlock:          # <== in case if we have backup agent setup on DB to push backups to external svr
         cidr: 192168.5.10/32
     ports:
     - protocol: TCP
       port: 80
=====================

kubectl create -f network-policy.yaml


controlplane ~ ✖ kubectl get netpol
NAME             POD-SELECTOR   AGE
payroll-policy   name=payroll   94s

controlplane ~ ➜  kubectl describe netpol payroll-policy
Name:         payroll-policy
Namespace:    default
Created on:   2023-01-26 03:09:04 -0500 EST
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal
  Not affecting egress traffic
  Policy Types: Ingress


# Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
Note: We have also allowed Egress traffic to TCP and UDP port. This has been added to ensure that the internal DNS resolution works from the internal pod. Remember: The kube-dns service is exposed on port 53:

root@controlplane:~# kubectl get svc -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   93m
root@controlplane:~#


Kubectx and Kubens – Command line Utilities
Through out the course, you have had to work on several different namespaces in the practice lab environments.
In some labs, you also had to switch between several contexts.
While this is excellent for hands-on practice, in a real “live” kubernetes cluster implemented for production,
there could be a possibility of often switching between a large number of namespaces and clusters.
This can quickly become and confusing and overwhelming task if you had to rely on kubectl alone.
This is where command line tools such as kubectx and kubens come in to picture.
Reference: https://github.com/ahmetb/kubectx
Kubectx:
With this tool, you don't have to make use of lengthy “kubectl config” commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.

Installation:
sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx

Syntax:
To list all contexts:
kubectx

To switch to a new context:
kubectx <context_name>

To switch back to previous context:
kubectx -

To see current context:
kubectx -c

Kubens:
This tool allows users to switch between namespaces quickly with a simple command.

Installation:
sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

Syntax:
To switch to a new namespace:
kubens <new_namespace>

To switch back to previous namespace:
kubens -


[] Storage: PV, PVC, Access modes, how to configure apps wid persistent storage

- How storage works wid containers/Docker? Two concepts used:
- - Storage Drivers & Filesystems: wen docker is installed on system, it creates dir structure as below & stores all its data (files related to images & containers running on Docker host) here by default >>
/var/lib/docker/
|_______________ aufs:
|_______________ containers: all files related to container, r stored here
|_______________ images: all files related to images r stored here
|_______________ volumes: any volumes created by docker r created under this folder
- - - Filesystem: Where docker stores its files & in what format? How exactly docker stores files of an image n a container?
- - - - Lets take a look at Docker's layerd arch 1st. wen docker builds images, it builds in a layered arch. Each line of instruction in Dockerfile creates a new layer in docker image, wid just the changes frm previous layer
- - - - - Ex: for below Dockerfile, since each layer only stores changes from previous layer, it is reflected in the size as well.
=====================
Dockerfile for app1
=====================
FROM Ubuntu                                            # <== 120MB  -- 1st layer is base Ubuntu OS, followed by
RUN apt-get update && apt-get -y install python        # <== 306MB  -- 2nd layer which installs APT pkgs,
RUN pip install flask flask-mysql                      # <== 6.3 MB -- 3rd layer wid python pkgs,
COPY . /opt/source-code                                # <== 229B   -- 4th layer tht copies src code & finally
ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run # <== 0B     -- 5th layer tht updates entrypoint of image
=====================

docker build -t Dockerfile -t jas/custom-image

- - - - - Now since this app2 has 1st 3 layers same as app1, Docker is NOT going to build 1st 3 layers, instead its going to re-use 3rd layer built for app1, from its cache & only creates last 2 layers wid new src & entrypoint
- - - - - This way, docker builds images faster & efficiently saves disk space.
- - - - - This is also applicable if app code is updated, docker simply re-uses all previous layers from cache & quickly rebuilds app image by updating latest src code
- - - - - all these layers r created wen we run docker build cmd to form final docker image. So all of these r docker image layers.
- - - - - Once build is complete, these layers become immutable. These r read-only n can only b modified wid new build.
- - - - - wen u run a container, based off this image, using docker run cmd,
- - - - - - docker creates a container, based off of these layers n creates a new writable layer on top of this image layer
- - - - - - this writable layer is used to store data created by container such as logfiles written by apps, any temp files generated by container or just any file modified by user on that container.
- - - - - - life of this layer is only as long as container is alive, wen container is destroyed, this layer & all changes stored in this layer r also destroyed
- - - - - - Copy On Write Mechanism: docker copies src code files in image layer so that if needed, we can make changes to that copy of original file in the image.
=====================
Dockerfile for app2
=====================
FROM Ubuntu                                            # <== base image
RUN apt-get update && apt-get -y install python        # <== pkgs
RUN pip install flask flask-mysql                      # <== dependencies
COPY . /opt/source-code                                # <== src code
ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run # <== entry point
=====================

- - - - Add PV to container to save data in writable image layer, from getting destroyed with container destroy. How to do this?
- - - - - 1st create a volume, using docker volume create cmd, under standard docker path, in docker host & then mount it inside container by specifying location
docker create volume data_volume         # <== This creates volume under /var/lib/docker/volume/ directory
docker run -v data_volume:/var/lib/mysql mysql # <== This mounts the volume inside the container's writable layer & specify location inside container
/var/lib/docker/volumes/data_volume <== volume on docker host
/var/lib/mysql <== volume on docker container
when /var/lib/mysql points to /var/lib/docker/volumes/data_volume,
- - - - - all data, meant to get loaded on /var/lib/mysql,
- - - - - gets actually stored at /var/lib/docker/volumes/data_volume
- - - - - This way, even if container & /var/lib/mysql gets destroyed,
- - - - - data still persists at /var/lib/docker/volumes/data_volume as that is host volume, not impacted.
- - - - Volume Mounting: Docker automatically creates new volume, if not created beforehand, while running docker run command & mounts it inside container
- - - - - Volume Mounting mounts a volume from docker volume
- - - - Bind Mounting: How abt if we have data alrdy at another location, outside of /var/lib/docker/volumes/ ? In that case, specify absolute path like below & docker binds that path inside container
- - - - - Bind Mounting mounts a path frm any location on docker host
docker run -v /data/mysql :/var/lib/mysql mysql
# Recommended way to write same command >>
docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql

- - - Storage Drivers: help manage storage for images & containers; responsible for maintaining layered arch, creating writable layer, moving files across layers to enable copy & write etc
- - - - Common storage drivers: AUFS, ZFS, BTRFS, Device Mapper, Overlay, Overlay2. Selection of storage driver depends on underlying OS being used
- - - - Ubuntu uses AUFS, while AUFS is not avl wid other OS like Fedora or CentOS (Device Mapper is better option for these)
- - - - Docker will choose best storage driver automatically based on OS used.

181. 14:10 hrs

[] Volume Driver Plugins:
- Default volume driver plugin is Local, tht creates a volume on Docker host & store its data under /var/lib/docker/volumes/ directory
- Many Volume Driver Plugins allow storage on 3rd party storage solutions like EBS, Azure File Storage, Convoy, DigitalOcean Block Storage, Flocker, gce-docker, GlusterFS, NetApp, RexRay, Portworx, VMware vSphere Storage etc
- Some of these volume drivers support different storage providers, like RexRay storage driver can b used to provision storage on EBS, S3, EMC Storage Array like Isilon & ScaleIO, Google Persistent Disks, OpenStack Cinder etc
docker run -it --name mysql --volume-driver rexray/ebs --mount src=ebs-vol,target=/var/lib/mysql mysql # <== this will create container & attach volume from EBS

[] CSI: Container Storage Interface
- CRI (Container Runtime Interface): a standard tht defines how an orchestration solution like k8s will communicate with container runtimes like Docker, rkt, cri-o, etc
- - If any new CRI is developed, they can follow CRI standards, & tht new CRI will work with k8s without having to work wid k8s team of developers or touch k8s src code
- CNI (Container Network Interface) : to extend support to different n/w solutions like weaveworks, flannel, cilium etc, CNI was introduced. Any new n/w vendor can simply develop their plugin, based on CNI standards & make their solution work wid k8s
- CSI : developed to support multiple storage solutions like EBS, portworx, GlusterFS etc. With CSI, u can write ur own drivers for ur own storage to work wid k8s
- - CSI is NOT k8s specific standard, its universal standard. So, if implemented, allows any orchestration vendor like k8s, cloudfoundry, apache mesos etc to work wid any storage vendor wid a supported plugin.
- - CSI looks like, it defines a set of below RPCs (Remote Procedure Calls), that would be triggered by container orchestrator, like k8s, & these must b implemented by storage drivers >>
SHOULD call to provision a new volume
SHOULD call to delete a volume
SHOULD call to place a workload that uses volume onto a node
SHOULD provision a new volume on the storage
SHOULD decommission a volume
SHOULD make the volume avl on a node
- - Example, wen a new pod is created and requires a new volume, then k8s should call CREATE VOLUME RPC & pass a set of details like volume name
- - Then, storage driver should implement this RPC, handle tht request & provision a new volume on storage array & return results of the operation.

[] Volumes:
- Docker containers r transient in nature, i.e., they r meant to last only for short perid of time
- They r called upon wen reqd to process data & destroyed wen finished.
- in k8s, specify volume details in pod's yaml file so tht data gets saved thr.
=====================
pod.yaml
=====================
apiVersion: v1
kinds: Pod
metadata:
  name: random-number-generator
spec:
  containers:
  - image: nginx
    name: nginx
    command: ["/bin/sh", "-c"]
    args: ["shuf -i 1-100 -n 1 >> /opt/number.out;"]
    volumeMounts:            # <== mounting volume  to a directory inside the container.
    - mountPath: /opt
      name: data
  volumes:
  - name: data-volume
    hostPath:                 # <== specifies that volume is on local host
      path: /data             # <== directory on the host, to be used to store data generated by container.
      type: Directory
   - name: cloud-volume
     awsElasticBlockStore:
       volumeID: <volumeID>
       fsType: ext4
=====================

[] PV: cluster-wide pool of storage volumes, configured by admin, to be used by users to deploy apps on cluster. users can select storage frm pool using PVC
- to manage storage centrally n manage it, rather than defining in each pod n updating each pod for any minor change in storage,
- - PVs help manage storage centrally to create a pool of storage n let developers carve pieces of storage out of it, as required.
=============================
pv-definition.yaml
=============================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol01
spec:
  accessModes:       # <== define how a volume should b mounted on a host
    - ReadWriteOnce  # <== values r ReadOnlyMany, ReadWriteOnce, ReadWriteMany
  capacity:
    storage: 1Gi     # <== storage reserved for this PV
  awsElasticBlockStore:
    volumeID: <volumeID>
    fsType: ext4
  hostPath:          # <== uses storage from node's local dir, not good for PRD
    path: /tmp/data
=============================

kubectl create -f pv-definition.yaml

[] PVC: to claim a piece of volume, reserved by PV, to make the storage avl on the node
- once PVCs r created, k8s binds PVs to claims, based on request & properties set on the volume
- Each PVC is bound to a single PV. During binding process, k8s tries to find a PV tht has sufficient capacity, as requested by claim
- - and any other request properties like access modes, volume modes, storage class etc
- if thr r multiple possible matches for a single claim, & if u need to use a specific volume, use labels & selectors to bind to needed volume
- smaller claim may bind to larger volume if all the other criteria matches.
- 1:1 relation b.w PV n PVC, so no other PVC can utilize remaining capacity in the volume.
- if thr r no volumes avl, PVC remains in pending state, untill newer volumes r avl to cluster.
=============================
pvc-definition.yaml
=============================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
=============================

kubectl create -f pvc-definition.yaml
- once a PVC is deleted from a volume, we can choose what is going to happen to volume. Default is set to retain, for reuse by any other claim.
persistentVolumeReclaimPolicy: Retain # < == other values Delete (delete volume wid pvc delete, thus freeing up storage on end storage device)
                                      # < == another option is Recycle (data in data volume will be scrubbed before making it avl to other PVC)

[] Using PVCs in PODs
Once you create a PVC
use it in a POD definition file by specifying the PVC Claim name
under persistentVolumeClaim section in the volumes section like this:
============================
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
============================
The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.
Reference URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes

# Configure a volume to store these logs at /var/log/webapp on the host.
First delete the existing pod by running the following command: -
kubectl delete po webapp
then use the below manifest file to create a webapp pod with given properties as follows:
=======================================
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
=======================================

Create a Persistent Volume with the given specification.

controlplane ~ ➜  cat pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 100Mi
  hostPath:
      path: /pv/log

controlplane ~ ➜  kubectl create -f pv.yaml
persistentvolume/pv-log created

controlplane ~ ➜
controlplane ~ ➜  cat pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi

controlplane ~ ➜  kubectl create -f pvc.yaml
persistentvolumeclaim/claim-log-1 created

[] StatefulSets are out of scope for the exam

[] Storage Classes:
- Static Provisioning Volumes: each time app requests storage,
- - before PV is created, for cloud storage volumes,
- - - u hv to manually provision disk on cloud, then
- - - manually create PV file, using same name as tht of disk
- Dynamic Provisioning Volumes: Storage classes: a provisioner is defined, to ensure that
- - volume gets created automatically wen the app requires it &
- - attach tht to pods, wen a claim is made
- Storage Classes: create an object of storage class n define details, then specify it in pvc yaml, under spec: as storageClassName: google-storage
==================================
sc.yaml
==================================
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd
parameters:    # <== These pmtrs r vry specific to provisioner type
  type: pd-standard
  replication-type: none
==================================
- So, nxt time wen a PVC is created, by getting defined in app's yaml,
- - storage class associated in PVC, uses defined provisioner, to provision a new disk in cloud, with specified size
- - then storage class creates a persistent volume n binds PVC to that volume.
- So SC still creates PV, just tht we dont have to create it manually anymore, its created automatically by SC
- u can created diff. SCs specifying diff. storage  types, like
- - Silver SC wid standard disks & no replication
- - Gold SC wid SSD & no replication
- - Platinum SC wid SSD & regional replication
- Thats y its called Storage Class, as u can create different classes of svc for storage

=============================
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
=============================
The Storage Class called local-storage makes use of VolumeBindingMode set to WaitForFirstConsumer.
This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.

Create a new pod called nginx with the image nginx:alpine.
The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html.
=============================
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc
=============================

Create a new Storage Class called delayed-volume-sc that makes use of the below specs:
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

195. 15:02 hrs

[] Networking:

[] Linux n/wing basics: Switch Routing
- Switching & Routing
- - Switching: 2 nodes connect to each other thru a switch. switch creates a n/w containing 2 sys.
- - - To connect them to a switch, we need an interface (like eth0) on each host.
- - - To see interfaces of each host, type ip link cmd. This cmd is to list & modify interfaces on a host
- - - Ex: eth0 interface will be used to connect to the switch
- setting n chking IP addresses on any given system, configuring interfaces & IP addresses
- - We then assign systems wid IP addresses on same n/w, by executing ip addr add 192.168.1.10/24 dev eth0
- - once the links r up & IP addresses r assigned, nodes can now communicate wid each other thru switch.
- - Switch can only enable communication within a n/w, i.e., it can recv pkts frm 1 host on n/w & deliver it to other hosts within same n/w
- knowledge on configuring gtwys & routes
- - Routing: To connect hosts of 1 n/w wid hosts of another n/w, router is used. Router helps connect 2 n/w together
- - - Since it connects to 2 diff. n/ws, it gets 2 IPs assigned, 1 on each n/w, to enable communication b/w them
- - - Router is just one of the many devices on the n/w.
- - Default Gtwy: helps specify whr router is on n/w, so tht host 1 n/w can send pkts to host on the other n/w
- - - If n/w is a room, gtwy is the door to outside world, to other n/ws or to internet. Systems need to know whr tht door is, to go thru tht.
- - - To see existing routing cfg on a system, run route cmd, it displays the kernel's routing table
route
Kernel IP routing table
Destination     Gateway   Genmask   Flags Metric Ref    Use Iface
# to configure a gtwy on system B, to reach systems on other n/w, execute >>
ip route add 192.168.2.0/24 via 192.168.1.1
#             other n/w CIDR     gtwy IP
route
Kernel IP routing table
Destination     Gateway       Genmask         Flags Metric Ref    Use Iface
192.168.2.0     192.168.1.1   255.255.255.0   UG    0      0      0   eth0
# this has to b configured on all the systems.
ip route add default via 192.168.1.1 # <== any n/w tht u dont know route to, use this router's IP address as default gtwy
# instead of default, 0.0.0.0 can also b specified
# if u hv two gtwys, one for internal pvt n/w, other for internet, then u need 2 separate entries for each n/w
ip route add 192.168.1.0/24 via 192.168.2.2 # <== one entry for internal pvt n/w
ip route add default via 192.168.1.1        # <== second entry wid default gtwy for all other n/ws, including public n/ws

- How to setup linux host as a router?
- - if a svr has two n/w interfaces setup, like eth0 & eth1, by default pkts r not fwded eth0 <-> eth1, due to security concerns, like in case if eth0 points to pvt n/w & eth1 points to internet
cat /proc/sys/net/ipv4/ip_forward # <== a setting in this file governs whether a host can fwd pkgt b/w interfaces, default value is set to 0, meaning no fwd
0
echo 1 > /proc/sys/net/ipv4/ip_forward # <== set this to 1 & pings should start working
# modify same value in /etc/sysctl.conf file for entry net.ipv4.ip_forward = 1 as well

- Commands to remember:
ip link # <== to list & modify interfaces on a host
ip addr # <== to see IP addresses assigned to those interfaces
ip addr add 192.168.1.10/24 dev eth0 # <== to set IP addresses on interfaces
# Changes made by these cmds r only valid till a restart
# to persist these changes, update these changes in /etc/network/interfaces file
ip route or route # <== to view routing table
ip route add 192.168.1.0/24 via 192.168.2.1 # <== to add entries into routing table
cat /proc/sys/net/ipv4/ip_forward # <== to chk if IP fwding is enabled on a host. (default 0, means NO, set to 1, to enable it)


- DNS: /etc/hosts is local src of truth for a host. But tht may not be actual src of truth. Host doesnt chk if 192.168.1.11's actual name is db.
cat /etc/hosts
192.168.1.11 db             # <== system B
192.168.1.12 www.google.com # <== system B

ping db             # <== routes to system B
ping www.google.com # <== routes to system B
# so we hv 2 names pointing to same host, we can hv any no. of names for same IP.
- knowledge on DNS & configuring DNS svrs etc, basics of name resolution
- - Name Resolution: translating host name to IP address (by setting it in /etc/hosts)
- - if thr r 100 svrs in a n/w, we'll need to add 100 entries in each of 100 svrs to connect to each other & if IP needs to be changed for 1 svr, it needs to be updated at 100 different places
- - - better to dedicate a svr just to manage those entries in 1 place. This is called DNS svr
- - - point all hosts to lookup DNS svr for to resolve a hostname to IP address, instead of its own /etc/hosts file.
- - How do we point a host to DNS svr? DNS resolution file is located at /etc/resolv.conf
- - - Add an entry into it, specifying address of DNS svr like below. Now each time a host comes up wid hostname, it looks it up from DNA svr
nameserver  192.168.1.100
- - if thrs same hostname entry wid 2 diff. IPs, in /etc/hosts & in DNS svr, /etc/hosts being local takes precedence & overrides entry in DNS svr
- - - This default sequence can b changed in /etc/nsswitch.conf by updating the following line >>
hosts:  files dns # <== flip this, i.e., set it to dns files, and then DNS will take precedence over /etc/hosts
- - 8.8.8.8 is a public name svr avl on internet, hosted by google, tht knows abt all websites on internet.
- - - u can set this in DNS svr to route any unknown hostnames to this one, for google to respond to it.
Forward All to 8.8.8.8 # <== add this entry in DNS svr
- - Domain Names: .com, .net, .edu, .org, .io <== top-lvl domains, tht represent intent of the website
- - - . is root domain, com is top-lvl domain, google is domain name assigned to it, www is sub-domain, tht helps further grouping under google
- - - Ex: www.google.com has multiple apps running as maps.google.com, drive.google.com, apps.google.com, mail.google.com etc
- - - Ex: if u hit apps.google.com,
- - - - 1st request goes to ur org's DNS to chk if its known/registered site/domainName wid them, if not, then it fwds request to internet
- - - - next hop to root DNS svr, that looks at ur request & points to DNS svr serving
- - - - .com DNS, then this .com DNS svr looks at ur request & fwds u to
- - - - Google DNS, provides u IP of svr/LB serving the app's applications
- - - In order to speed up all future resolves, org's DNS may choose to cache this IP with TTL, typically few secs to few mins
- - - Similar setup can be present inside ur org as well, tht can b handled by ur org's internal DNS svr itself
- - - for internal similar site names inside company, like, mail.mycompany.com, web.mycompany.com, db.mycompany.com etc
- - - - add below entry in /etc/resolv.conf & specify domain name tht u want to append, for DNS to search for specified word as shortcut, in your company's sitemap
search mycompany.com prod.mycompany.com # <== host is intelligent enuff to exclude search domain if u specify domain in ur query, ur host will try searching all specified domain names
- - How r records stored in DNS? Record types, stored in DNS
- - - A-record :  storing IPv4 to hostnames
- - - AAAA-record (quad-A record) :  storing IPv6 to hostnames
- - - CNAME : Mapping 1 name to another name. EX: u may hv multiple aliases for same app, like eat.webserver, hungry.webserver point to food.webserver
- - Tools to test DNS resolution:
- - - ping, nslookup to query a hostname from DNS svr. nslookup doesnt conisers entries in local /etc/hosts file
- - -  dig: returns more details in similar form

[] Prerequisite - CoreDNS
In the previous lecture we saw why you need a DNS server and how it can help manage name resolution in
large environments with many hostnames and Ips and how you can configure your hosts to point to a DNS server.
In this article we will see how to configure a host as a DNS server.
We are given a server dedicated as the DNS server, and a set of Ips to configure as entries in the server.
There are many DNS server solutions out there, in this lecture we will focus on a particular one – CoreDNS.
So how do you get core dns? CoreDNS binaries can be downloaded from their Github releases page or as a docker image.
Let’s go the traditional route. Download the binary using curl or wget. And extract it. You get the coredns executable.
Run the executable to start a DNS server. It by default listens on port 53, which is the default port for a DNS server.
Now we haven’t specified the IP to hostname mappings. For that you need to provide some configurations.
There are multiple ways to do that. We will look at one.
First we put all of the entries into the DNS servers /etc/hosts file.
And then we configure CoreDNS to use that file. CoreDNS loads it’s configuration from a file named Corefile.
Here is a simple configuration that instructs CoreDNS to fetch the IP to hostname mappings from the file /etc/hosts.
When the DNS server is run, it now picks the Ips and names from the /etc/hosts file on the server.
CoreDNS also supports other ways of configuring DNS entries through plugins.
We will look at the plugin that it uses for Kubernetes in a later section.

Read more about CoreDNS here:
https://github.com/kubernetes/dns/blob/master/docs/specification.md
https://coredns.io/plugins/kubernetes/

[] n/w namespaces: used by containers like docker, to implement n/w isolation. ns is like individual room of a house
- n/w namespaces & how docker uses n/w ns internally
- each host has its own n/w interfaces, routing & ARP table, wid info abt rest of n/w. intent is to seal all these details from container
- wen container is created, we create a n/w ns for it, tht way it has no visibility, to any n/w-related info on the host.
- - within its ns, container can hv its own virtual interfaces, routing & ARP tables.
- to create a new n/w ns on a linux host, execute >>
ip netns add red
ip netns add blue
ip netns             # <== lists ns
ip link              # <== shows list of interfaces, lo: is loopback interface
# to run ip link cmd in specific n/w ns, execute >>
ip netns exec red ip link
# or
ip -n red link
# inside the container using ns, it lists only loopback interface, cannot see eth0 interface of the host
arp                   # <== shows list of entries wen executed on host
Address       HWtype    HWaddress           Flags Mask    Iface
172.17.0.21   ether     02:42:ac:11:00:15   C             eth0

ip netns exec red arp  # <== shows no entries inside container
Address       HWtype    HWaddress           Flags Mask    Iface

# same for route
route                  # <== lists entries wen executed on host
Kernel IP routing table
Destination     Gateway       Genmask         Flags Metric Ref    Use Iface
192.168.2.0     192.168.1.1   255.255.255.0   UG    0      0      0   eth0

ip netns exec red route # <== no entries wen executed inside container
Kernel IP routing table
Destination     Gateway       Genmask         Flags Metric Ref    Use Iface

- by default, n/w ns hv no n/w connectivity, no interfaces of their own & they cant see underlying hosts' n/w.
- how to establish connectivity b/w 2 n/w ns?
- - just like we connect 2 hosts using a virtual ethernet pair to an ethernet interface on each machine
- - - same way we can connect 2 n/w ns virtual ethernet pair, wid virtual interfaces on each ns, often called as pipe
- - to create the cable/pair, execute below
ip link add veth-red type veth peer name veth-blue  # create cable n specify two ends
ip link set veth-red netns red                      # attach each interface to their appropriate ns
ip link set veth-blue netns blue                    # attach each interface to their appropriate ns
ip -n red addr 192.168.15.1 dev red                 # assign ip-address, within each ns
ip -n blue addr 192.168.15.2 dev blue               # assign ip-address, within each ns
ip -n red link set veth-red up                      # bring up the interfaces, for them to connect/link to each other
ip -n red link set veth-blue up                     # bring up the interfaces, for them to connect/link to each other
ip netns exec red ping 192.168.15.2                 # ping from inside of red ns to blue ns
ip netns exec red arp                               # arp table now identifies blue neighbor ns
Address       HWtype    HWaddress           Flags Mask    Iface
192.168.15.2  ether     ba:b0:6d:68:09:e9   C             veth-red

ip netns exec blue arp                              # arp table now identifies red neighbor ns
Address       HWtype    HWaddress           Flags Mask    Iface
192.168.15.1  ether     7a:9d:9b:c8:3b:7f   C             veth-blue

arp                                                  # chk host now n host has no idea abt new ns & new interfaces created at ns lvl
Address       HWtype    HWaddress           Flags Mask    Iface
192.168.1.3   ether     02:42:ac:11:00:15   C             eth0
192.168.1.4   ether     14:83:b6:a3:65:12   C             eth0

- how all ns can communicate wid each other?
- - create a virtual switch within the host & connect all ns to it
- - how to create a virtual switch within a host? multiple solutions avl like linux bridge or Open vSwitch
- - - to create a new virtual bridge n/w, create a new virtual interface to the host, by executing >>
ip link add v-net-0 type bridge # just another interface for host wid type bridge
ip link # <== this will display new v-net-0, along wid eth0 & lo (loopback one)
ip link set dev v-net-0 up # <== by dfeault, new interface is down, we need to bring it up manually
- - - for ns, this new interface is like a switch that they can connect with
- - - think of this as interface to the host & switch to the ns, inside the host
- - - nxt step is to connect these ns to this new virtual n/w switch
- - - 1st delete pipe created earlier, delete one side & other side gets deleted automatically since they r a pair
ip -n red link del veth-red
- - - create new cables to connect ns to the bridge
ip link add veth-red type veth peer name veth-red-br   # keep -br for easy identification of bridge entries
ip link add veth-blue type veth peer name veth-blue-br # keep -br for easy identification of bridge entries
ip link set veth-red netns red # now tht we hv cables ready, connect them to ns to attach 1 end of this interface to red ns, execute
ip link set veth-red-br master v-net-0 # to the other end to the bridge n/w, run & set master for this -br to newly created bridge v-net-0
ip link set veth-blue netns blue # now tht we hv cables ready, connect them to ns to attach 1 end of this interface to blue ns, execute
ip link set veth-blue-br master v-net-0 # to the other end to the bridge n/w, run & set master for this -br to newly created bridge v-net-0
ip -n red addr 192.168.15.1 dev veth-red                 # assign ip-address, within each ns
ip -n blue addr 192.168.15.2 dev veth-blue               # assign ip-address, within each ns
ip -n red link set veth-red up                           # turn the devices up
ip -n red link set veth-blue up                          # turn the devices up
- - - containers can now reach each other over the n/w, as we now hv all 4 ns connected
ping 192.168.15.1 # <== ns IP still not reachable from host
- - host can still not reach ns, as host is on 1 n/w & ns r on another n/w.
- - how to establish connectivity b/w host & ns ?
- - - bridge switch is actually an interface to the host as v-net-0, so just assign IP address to it
ip addr add 192.168.15.5/24 dev v-net-0
ping 192.168.15.1 # <== starts responding now
- - this entire n/w is still pvt n restricted within this host. Only door to outside is ethernet port on the host
- - how do we configure this bridge to align on/w to ethernet port? how can i reach another host frm ns?
ip netns exec blue ping 192.168.1.3 # <== no response, blue ns sees this IP is diff. frm its CIDR range & chks its routing table. RT comes back saying it has no info & issues n/w unreachable
- - - add an entry in RT to provide a door/gtwy to outside world. how do we define a gtwy?
- - - gtwy is a system on local n/w tht connects to other n/w
- - wht is a system that has 1 interface on n/w local to blue ns which is 192.168.15.2 n/w & also connected to LAN n/w ?
- - localhost has all the ns specified, so u can ping ns.
- - localhost has an interface to attach pvt n/w so u can ping ns
- - localhost is gtwy tht connects 2 n/w together, we can now add a row entry in blue ns to route all traffic like below >>
ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5
- - host has 2 IP addresses, one on bridge n/w at 192.168.15.5 & another at external n/w at 192.168.1.2
- - - can u use any in the route? No, cuz blue ns can only reach gtwy in its local n/w 192.168.15.5
- - - default gtwy should b reachable frm ns wen u add it to ur route
ip netns exec blue ping 192.168.1.3 # <== u can ping now but no response comes back
- - home n/w has internal pvt IP tht destination n/w doesnt knows abt, so destination cannot reach back
- - - for this we need NAT enabled on host, acting as a gtwy here so tht it can send msgs to LAN in its own name with its own address
- - - How do we add NAT functionality to our host?
- - - - add IP address as a new rule in NAT IP table in POSTROUTING chain to masquerade or replace frm address on all pkts coming frm src n/w
iptables -y nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE
- - - now, anyone reving these pkts outside n/w will think tht they r coming frm the host & not frm within ns
ip netns exec blue ping 192.168.1.3 # <== should b able to reach outside world now
- - If LAN is connected to internet, how to make sure tht ns reach the internet?
ip netns exec blue ping 8.8.8.8 # <== no response
ip netns exec blue route # <== shows tht we hv routes to following two destinations but not to anything else
Destination     Gateway       Genmask         Flags Metric Ref    Use Iface
192.168.15.0    0.0.0.0       255.255.255.0   U     0       0      0  veth-blue
192.168.5.0     192.168.15.5  255.255.255.0   UG    0       0      0  veth-blue

ip netns exec blue ip route add default via 192.168.15.5 # <== for any other network/IP, talk to host as gtwy, tht talks to external world

ip netns exec blue route
Destination     Gateway       Genmask         Flags Metric Ref    Use Iface
192.168.15.0    0.0.0.0       255.255.255.0   U     0       0      0  veth-blue
192.168.5.0     192.168.15.5  255.255.255.0   UG    0       0      0  veth-blue
Default         192.168.15.5  255.255.255.0   UG    0       0      0  veth-blue

ip netns exec blue ping 8.8.8.8 # <== works now

- - connectivity frm outside world (like 2nd host) to webapp hosted in ns of 1st host? 2 options
- - - 1st to give away identity of pvt n/w to 2nd host. Basically add IP route entry to 2nd host, telling the host tht n/w 192.168.15.2 can b reached thru 192.168.1.2 host, but wht if thr r 100s of hosts tht try to connect to it?
- - - 2nd, better option, port fwding role using IP Tables to say any traffic coming from port 80 on localhost to b fwded to port 80 on IP assigned to blue ns
iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT

[] While testing the Network Namespaces, if you come across issues where you can't ping one namespace from the other,
- make sure you set the NETMASK while setting IP Address. ie: 192.168.1.10/24
ip -n red addr add 192.168.1.10/24 dev veth-red
- Another thing to check is FirewallD/IP Table rules.
- Either add rules to IP Tables to allow traffic from one namespace to another.
- Or disable IP Tables all together (Only in a learning environment).

[] Docker n/wing:
- Diff. n/wing options:
- - none n/w: docker container is not attached to any n/w. container cannot reach outside world & no one from outside world can reach container.
- - - if u hv multiple containers inside same host, they cannot talk to each other as well
docker run --network none nginx
- - host n/w: container is attached to host n/w, no n/w isolation b/w host & container. no port mapping needed. if app is deployed on port 80 of container, app is avl on port 80 of host
- - - if u run another instance of same container, it wont work as they share host n/wing and port 80 will b alrdy in use
docker run --network host nginx
- - bridge: internal pvt n/w is created, to which docker host & containers attach to. n/w has an address 172.17.0.0 by default, & each device connecting to this n/w, get their own internal pvt n/w address on this n/w.
- - - wen docker is installed on a host, it creates an internal pvt n/w called Bridge, by default. Docker calls the n/w by name bridge but on the host, n/w is created by name Docker0.
- - - docker uses a technique, similar to n/w ns. interface Docker0 is assigned an IP address 172.17.0.1. wenevr a container is created, docker creates a n/w ns for it
- How does docker attach a container or its n/w ns to bridge n/w? (consider container or n/w ns as same thing here)
- - docker creates a virtual cable wid two interfaces on each end, with one end attached to local bridge Docker0
ip link                         # <== shows 1 end of interface attached to localhost as master docker0
- - run same cmd again this time for n/w ns for specific n/w,
ip -n <nameSpace_created_by_docker_earlier> link # <== it lists other end of interface within container ns, something like eth0@if8
- - the interface also gets an IP assigned within the n/w. View this by running below cmd within container ns
ip -n <nameSpace_created_by_docker_earlier> addr # <== container gets assigned 172.17.0.3/16, under eth0@if8
- - app deployed can only b accessed from within one of the containers but not frm outside yet, use port mapping for this to fwd traffic frm container port 80 to host port 8080
- - how does docker fwd traffic frm 1 port to another?
- - - NAT rule is created for this. Using IP tables, an entry is created into NAT tables to append the rules to pre-routing chain to changedestination port from 8080 to 80.
iptables -t nat -A PREROUTING -j DNAT --dport 8080 --to-destination 80           # <== standard n/w way
iptables -t nat -A DOCKER -j DNAT --dport 8080 --to-destination <containerIP>:80 # <== docker does it same way, docker add the rule to Docker chain & sets destination to include container's IP as well.
iptables -nvL -t nat # <== u can see new rule wen u list rules in IP tables.
Chain DOCKER (2 references)
target    prot  opt  source   destination
RETURN    all   --   anywhere anywhere
DNAT      tcp   --   anywhere anywhere      tcp dpt:8080 to 172.170.2:80

docker network ls               # <== shows bridge, bridge n/w is like an interface to host, BUT a switch to ns or container within the host
ip link                         # <== shows docker0 as interface, alongwith lo, eth0 etc
ip link add docker0 type bridge # <== executed internally by docker to set docker0 n/w of type bridge
ip addr                         # <== shows IP address assigned to each interface
ip netns                        # <== u can chk same n/w ID by doing docker inspect <containerID> n chk initial letters of SandboxID, under NetworkSettings


- n/w ns;
- - How to create an isolated /w ns environment within our sys
- - How to connect multiple n/w ns thru a bridge n/w
- - How to create vEth (virtual cables or pipes) wid virtual interfaces on either end
- - How to attach each one end of vEth to n/w ns & other end of vEth to the bridge
- - How to assign IP & bring interfaces up
- - How to enable NAT or IP masquerade for external communication


[] CNI:
- since docker, k8s, rkt, mesos containerizer & all other n/wing solutions, tht work wid container & require to configure n/w b/w them, do it same way except it uses diff. naming patterns
- - then why code & develop same solution multiple times? why not create a single standard approach tht evry1 can follow?
- - so all ideas frm diff. solutions r taken & move all n/wing portions of it into a single program or code.
- - Since this is for bridge n/w, we call it bridge. So, a program or script is created tht performs all reqd tasks to get container attached to bridge n/w
- - Ex: u can run below program named 'bridge' & specify tht u want a particular container to be added to a particular n/w ns; and
- - - bridge takes care of the rest, so tht container runtime environment is relieved of those tasks
bridge add <containerID> /var/run/netns/<n/w_ns_name>
- so wht if u need to create a program for urself, maybe for a new n/wing type, wht arguments & commands should it support?
- - How u make sure tht the program u create, will work correctly wid all those runtimes (docker, rkt, k8s etc) ?
- - how u know tht container runtimes will invoke ur program correctly? Thats whr we need some standards defined
- A standard tht defines how a program should look, how container runtimes will invoke it so tht evry1 can adhere to single set of standards & develop solutions tht works across runtimes
- - This is wht CNI comes in. CNI is a set of standards, tht defines how programs should b developed to solve n/wing challenges in a container runtime environment & how container runtime environment should invoke them
- - These programs r referred to, as plugins. In this case, Bridge program is a plugin for CNI
- CNI defines a set of responsibilities for container runtimes & plugins
- - For container runtimes, CNI specifies that it is responsible for
- - - creating a n/w ns for each container.
- - - identify n/w that container must attach to
- - - container runtime to invoke n/w plugin (bridge) wen container is created, using, ADD cmd
- - - container runtime to invoke n/w plugin (bridge) wen container is deleted, using DEL cmd
- - - how to configure a container plugin on container runtime environment using a JSON file
- - On the plugin side, it defines that the plugin
- - - must support CLIs like ADD/DEL/CHECK
- - - must accept pmtrs container id, network ns etc
- - - should take care of assigning IP addresses to pods & any associated routes, required for containers to reach other containers in n/w
- - - must return results in a specific format
- As long as container runtimes adhere to these standards, they can all live together in harmony. Any runtime should be able to run wid any plugin
- CNI comes wid a set of supported plugins alrdy, like, bridge, vlan, ipvlan, macvlan, windows as well as IPAM (IP Address Mgmt) plugin like DHCP & host-local
- thr r other plugins avl frm 3rd party orgs as well. Ex: weave(weaveworks), flannel, cilium, vmware nsx, calico, infoblox etc
- - all these container runtimes implement CNI standards. So any of them can work wid any of these plugins.
- Docker doesn't implement CNI. It has its own set of standards, known as CNM (Container N/w Model), similar to CNI but wid some differences.
- - Due to the differences, these plugins do not natively integrate wid docker. Thr r workarounds like create a docker container in none n/w n manually invoke plugins urself
- - tht is how k8s does it. k8s creates docker containers on none n/w & then invokes configured CNI plugins tht take care of rest of cfg


204. 16:01 hrs


[] Cluster n/wing: n/wing cfgs reqd on master & worker nodes
- IP & FQDN
- - each node should hv at least 1 n/w interface connected to n/w. Each interface must hv an IP address configured.
- - each node must hv a unique hostname set as well as unique MAC address.
- - on each node, some ports must be opened as well, as those r used by components like kubectl
- - - master node should accept connections at
- - - - port 2379 (for etcd svr)
- - - - port 2380 (for etcd clients for multiple master nodes wid etcd HA setup)
- - - - port 6443 (for API svr): worker nodes, kubctl tool, external users & all ctrl plane components access API svr via this port
- - - - port 10250(for kubelets)
- - - - port 10257 (for ctrller mgr)
- - - - port 10259 (for scheduler)
- - - Kubelets on master & worker nodes listen on port 10250 (kubelets can b present on master node as well)
- - - works nodes:
- - - - port 10250(for kubelets)
- - - - ports 30000-32767: worker nodes expose svcs for external access on these ports

[] useful cmds:
ip link
ip addr
ip addr add <CIDR> dev eth0
ip route
ip route add <CIDR> via <BridgeIP>
cat /proc/sys/net/ipv4/ip_forward  # <== should be 1 to enable fwding
arp
nestat -plnt
route
- explore existing k8s cluster & view info abt interfaces, IPs, hostnames, ports etc

[] Important Note about CNI and CKA Exam
An important tip about deploying Network Addons in a Kubernetes cluster.
In the upcoming labs, we will work with Network Addons. This includes installing a network plugin in the cluster.
While we have used weave-net as an example,
please bear in mind that you can use any of the plugins which are described here:
https://kubernetes.io/docs/concepts/cluster-administration/addons/
https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model

In the CKA exam, for a question that requires you to deploy a network addon, unless specifically directed,
you may use any of the solutions described in the link above.
However, the documentation currently does not contain a direct reference to the exact command to be used
to deploy a third party network addon.
The links above redirect to third party/ vendor sites or GitHub repositories which cannot be used in the exam.
This has been intentionally done to keep the content in the Kubernetes documentation vendor-neutral.
At this moment in time, there is still one place within the documentation where you can find the exact command
to deploy weave network addon:
https://v1-22.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node (step 2)

What is the MAC address of the interface on the controlplane node?
ifconfig ens3 | grep inet # <== MAC address is specified nxt to ether in this output
ip link show eth0
or
What is the MAC address assigned to node01?
ssh node01 ifconfig en3

We use Containerd as our container runtime. What is the interface/bridge created by Containerd on this host?
ip link
cni0 # <== for bridge

What is the state of the interface cni0?
controlplane ~ ➜  ip link show cni0
3: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether e6:cd:0c:d6:99:be brd ff:ff:ff:ff:ff:ff

If you were to ping google from the controlplane node, which route does it take?

controlplane ~ ➜  ip route show # or use ip r
default via 172.25.1.1 dev eth1 # <== default will b used for google
10.18.32.0/24 dev eth0 proto kernel scope link src 10.18.32.3
10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1
10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink
172.25.1.0/24 dev eth1 proto kernel scope link src 172.25.1.103
controlplane ~ ➜

What is the port the kube-scheduler is listening on in the controlplane node?

controlplane ~ ✖ netstat -nplt # or netstat -natulp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:42751         0.0.0.0:*               LISTEN      1042/containerd
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      3736/kubelet
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      4285/kube-proxy
tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      2644/etcd
tcp        0      0 10.18.32.3:2379         0.0.0.0:*               LISTEN      2644/etcd
tcp        0      0 10.18.32.3:2380         0.0.0.0:*               LISTEN      2644/etcd
tcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      2644/etcd
tcp        0      0 127.0.0.11:43917        0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      1040/ttyd
tcp        0      0 127.0.0.1:10257         0.0.0.0:*               LISTEN      2746/kube-controlle
tcp        0      0 127.0.0.1:10259         0.0.0.0:*               LISTEN      2776/kube-scheduler  # <==
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      610/systemd-resolve
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1051/sshd: /usr/sbi
tcp6       0      0 :::10250                :::*                    LISTEN      3736/kubelet
tcp6       0      0 :::6443                 :::*                    LISTEN      2715/kube-apiserver
tcp6       0      0 :::10256                :::*                    LISTEN      4285/kube-proxy
tcp6       0      0 :::22                   :::*                    LISTEN      1051/sshd: /usr/sbi
tcp6       0      0 :::8888                 :::*                    LISTEN      3959/kubectl
controlplane ~ ➜

Notice that ETCD is listening on two ports. Which of these have more client connections established?

controlplane ~ ➜  netstat -anp  | head -3
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:42751         0.0.0.0:*               LISTEN      1042/containerd

controlplane ~ ✖ netstat -anp  | grep etcd | grep 2379 | wc -l
67

controlplane ~ ➜  netstat -anp  | grep etcd | grep -v 2379 | wc -l
3

controlplane ~ ➜  netstat -anp  | grep etcd | grep -v 2379
tcp        0      0 10.18.32.3:2380         0.0.0.0:*               LISTEN      2644/etcd
tcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      2644/etcd
unix  2      [ ]         DGRAM                    30207452 2644/etcd            @051e1
controlplane ~ ➜

wht is n/w interface configured for cluster connectivity on master node?
ifconfig -a
# or
cat /etc/network/interfaces
# or
ip link


[] Pod Networking:
- we hv setup several configured n/w b/w k8s master & worker nodes, so they r all on a n/w tht can reach each other
- we also made sure tht firewall & n/w security grps r configured correctly to allow for ctrl plane components to reach each other
- n/w @ pod layer is crucial to cluster's functioning.
- k8s cluster has large no. of pods & svcs running on it
- how r pods addressed? how do they communicate wid each other?
- how u acess svcs running on these pods, internally within cluster & externally?
- k8s doesnt comes wid built-in solution to these challenges n expects u to implement a n/wing solution
- k8s has laid out clearly, reqmnts for pod n/wing. k8s expects:
- - each pod to hv its own unique IP address
- - each pod should b able to reach evry other pod within same node & on other nodes as well, using IP address
- k8s doesnt cares wht IP address that is or wht range of subnet it belongs to
- as long as u can implementing the solution to automatically assign IP addresses & establishing connectivity, without having to configure any NAT rules
- how do u implement a model tht solves these reqmnts?
- useful cmds to solve this challenge >>
# setup bridge interface, bring it up & assign IP address to it
ip link add v-net-0 type bridge
ip link set dev v-net-0 up
ip addr add 192.168.15.5/24 dev v-net-0
# create a n/w ns virtual interface, link it up n add IP address, n bring it up
ip link add veth-red type veth peer name veth-red-br
ip link set veth-red netns red
ip -n red addr add 192.168.15.1 dev veth-red
ip -n red link set veth-red up
# set bridge to master
ip link set veth-red-br master v-net-0
# set default gtwy for virtual interface in n/w ns
ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5
# setup return path in POSTROUTING
iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE

[] Practical scenario of n/w setup frm scratch:
- consider a 3-node cluster, wid nodes part of external n/w
- wen containers r created, k8s creates n/w ns for them
- to enable comm. b/w them, attach these ns to n/w, but wht n/w?
- - bridge n/w on each node, tht can b created within nodes to attach n/w ns, & bring them up
- - then assign IP address to bridge addresses or networks. But wht IP address? Consider each bridge n/w will be on its own subnet
ip link add v-net-0 type bridge
ip link set dev v-net-0 up
ip addr add 10.244.1.1/24 dev v-net-0
- so we hv build our base across each node.
- Remaining steps needs to be performed for each container & each time a new container is created, we write a script for it
- - basically just a file tht will hv all the n/w specific cmds needed for each container & we can run this multiple times for each container
===============
net-script.sh
===============
# create veth pair: to attach a container to n/w, we need a pipe or virtual n/w cable
ip link add ...
# Attach veth pair: then attach 1 end to container & other end to the bridge
ip link set ...
ip link set ...
# Assign IP address to veth & add a route to default gtwy. But wht IP do we add?
# we either manage IP ourselves or store tht info in some kind of DB
ip -n <ns> addr add ...
ip -n <ns> route add ...
# Bring up the interface
ip -n <ns> link set ... up
===============
- Run this same script for each container n get each container connected to the n/w
- This way, now each pod gets its own unique IP address & is able to comm. with each n/w in same node.
- nxt part is to enable pods to reach other pods on other nodes
- - For this, add a route to node 1's routing table to route request/traffic to other node's pod IP via other node's IP
ip route add 10.244.2.2 via 192.168.1.12
- - as infra increases, it will become tough to manage
- - better solution is to do same on a router & point all hosts to use that as default gtwy
- - - tht way its easy to manage routes to all n/ws in routing table on router.
- - - wid that, the individual virtual n/ws created on each node, now form a single, consolidated large n/w across all nodes.
- wid larger infra, not recommended to run tht script manually, to connect container to a n/w
- how to run tht script automatically?
- - wen a pod is created on k8s, thts whr CNI comes in, acting as middleman.
- - CNI tells k8s, tht this is how u should call a script, as soon as u create a container
- - And CNI tells us, this is how ur script should look like, so modify script lil bit to meet CNI standards
- - - it should hv an ADD section to take care of adding a container to n/w
- - - DEL section to delete container interfaces from n/w, freeing IP addresses, etc
- kubelet on each node, is responsible for creating containers.
- - wenevr a container is created, kubelet looks at CNI cfg, passed as a cmd line arg wen it was run & identifies our script's name
kubelet --> --cni-conf-dir=/etc/cni/net.d & --cni-bin-dir=/etc/cni/bin
- - kubelet then looks into CNI's bin directory to find our script & then executes our script wid ADD command, as >>
./net-script.sh add <containerID> <ns>
- - n then our script takes care of the rest


[] CNI in k8s: how k8s is configured to use CNI plugins
- n/w ns in Linux, n/wing in Docker, why & wht is CNI, CNI plugins
- CNI plugin must b invoked by k8s component, kubelet, responsible for creating containers, to invoke appropriate n/w plugin after container is created.
- CNI plugin is configured in kubelet svc on each node in cluster, wid kubelet svc file specifying >>
--network-plugin=cni
--cni-bin-dir=/etc/cni/bin
--cni-conf-dir=/etc/cni/net.d
- CNI bin dir. has all the supported plugins as executables
- CNI cfg dir. has a set of cfg files. This is whr kubelet looks to find out which plugin needs to be used.
ls -ltrh /etc/cni/net.d
10-bridge.conf
- in this case, it find bridge cfg file. If thr r multiple files here, kubelet will choose files in alphabetical order
- 10-bridge.conf file looks like this >>
================
10-bridge.conf
================
{
 "cniVersion" : "0.2.0",
 "name": "mynet",
 "type": "bridge",  # <== can b set to DHCP to config external DHCP svr
 "isGateway": true, # <== defines whether bridge n/w should get an IP assigned so tht it can act as a gtwy
 "isMasq": true,    # <== defines if a NAT rule should be added for IP masquerading
 "ipam": {          # <== IP Address Mgmt
    "type": "host-local", # <== defines tht IPs r managed locally on this host, unlike DHCP svr in which IPs r mainated remotely
    "subnet": "10.22.0.0/16",
    "routes": [
        { "dst": "0.0.0.0/0"
        }
    ]
 }
}
================


[] CNI weave:
- for large infra, routing table may not be able to hold a long list of entries cleanly
- WeaveWorks deploy its agent/svc on each node. These agents communicate wid each other to exchange info regarding nodes & n/ws & pods within each
- Each agent or peer, stores topology of entire setup. That way they know the pods & their IPs on other nodes
- Weave creates its own bridge on nodes & names it Weave, then assigns IP to each n/w.
- single pod can b attached to multiple bridges like Weave, Docker0 etc.
- Wht path a pkt takes to reach destination depends on route configured on container
- Weave makes sure that pods gets correct route, configured to reach the agent & then the agent takes care of other pods.
- When a pkt is sent from 1 pod to another pod on another node,
- - Weave intercepts the pkt & identifies tht its on a separate n/w
- - Weave then encapsulates pkt into a new one wid new src & destination & sends it across the n/w
- - Once on the other side, the other Weave agent, retrieves the pkt, decapsulates it, & routes the pkt to the correct pod.
- How do we deploy weave on k8s cluster?
- - Weave & Weave peer can b deployed as svcs or Daemons on each nodes in the cluster, manually.
- - or, if k8s is setup alrdy, easier way is to deploy it as pods in the cluster
- once the base k8s system is ready wid nodes & n/wing configured correctly b/w nodes & basic ctrl plane components r deployed,
- - Weave can b deployed in cluster wid a single kubectl cmd >>
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-versions=$(kubectl version | base64 | tr -d '\n')"
- - This deploys all the necessary components reqd for Weave in the cluster.
- - Weave peers r deployed as DaemonSets. A DaemonSet ensures that 1 part of the given kind is deployed on all nodes in the cluster.
- - This works perfectly for the Weave peers.
- If u deployed ur cluster wid kubeadm & weave plugin, u can see weave peers as parts deployed on each node
kubectl get nodes -n kube-system
NAME            READY STATUS  RESTARTS AGE IP          NODE
weave-net-5gcmb 2/2   Running 0        18m 172.17.0.30 node01 <none>
weave-net-5r9n9 2/2   Running 0        18m 172.17.0.11 master <none>

kubectl logs weave-net-5gcmb weave -n kube-system


identify kubelet svc & n/w plugin configured for k8s
ps aux | grep kubelet | grep network

All CNI supported plugins r loaded under /opt/cni/bin/ by default


[] IPAM - IP Address Mgmt:
- how r virtual bridge n/ws in the nodes are assigned an IP subnet
- how r pods assigned an IP address?
- whr is this info stored
- who is responsible for ensuring thr r no duplicate IPs assigned? CNI plugin, as per CNI standards
- - CNI provides host-local & DHCP plugins tht can do IPAM
- - Weave, by default, allocates IP range 10.32.0.0/12 for entire n/w, thts like million IPs for pods on n/w
- - - frm this range, peers decide to split IPs equally b/w them & assign 1 portion to each node

[] Pod n/wing:
- how bridge n/w r created within each node,
- how pods get a n/w ns created for them,
- how interfaces r attached to those n/w ns,
- how pods get an IP address assigned to them within a subnet assigned to tht node
- thru routes or other overlay techniques, we can get pods in diff. nodes to talk to each other, forming a large virtual n/w, whr all pods can reach each other


[] Svc n/wing:
- u would rarely configure ur pods to communicate directly wid each other
- if u want a pod to access svcs, hosted on another pod, u would alwys use svcs
- to make 2 pods accessible to each other within same node, we create a svc on each pod, wid a name & IP address assigned to each svc., thru which each pod can b accessed via svc name or svc IP
- how to make 2 pods accessible to each other over different nodes?
- - wen a svc is created, it is accessible from all pods in the cluster, irrespective of wht nodes, the pods r on
- while a pod is hosted on a node, a svc is hosted across a cluster, not bound to a specific node
- Diff. kind of svcs:
- - Cluser IP: svc that is only accessible from within the cluster, good for pods tht host DBs, tht should b accessible only within the cluster
- - NodePort: svc that makes app, on pod, accessible outside the cluster
- how r svcs getting IP addresses
- how r svcs made avl across all the nodes in the cluster?
- how svc is made accessible to external users thru a port on each node? who is doing that; how & whr do we see it?
- Starting frm scratch on a 3-node cluster:
- - kubelet: process runs on each node, tht is responsible for creating pods.
- - - each kubelet svc on each node, watches changes in the cluster thru API svr
- - - each time a new pod is to be created, kubelet creates that pod on the node
- - - kubelet then invokes CNI plugin to configure n/wing for tht pod.
- - kube-proxy: runs on each node, watches the changes in the cluster thru API svr
- - - each time a new svc is to b created, kube-pxy gets into action. Unlike pods, svcs r NOT created or assigned to each node, svcs r cluster-wide concept, these exist across all nodes in the cluster
- - - these svcs do not exist at all, thrs no svr or svc really listening to IP of the svc.
- - - pods & containers hv ns wid interfaces & IPs assigned to those interfaces
- - - But svcs, nothing like that exists. Thr r no processes or ns or interfaces for a svc. svc is just a virtual object
- - - So, how does a svc gets an IP address?
- - - how r we able to access an app on the pod, thru a svc?
- - - - wen we create a svc object, it is assigned an IP frm a pre-defined range.
- - - - kube-pxy component running on each node gets that IP address & creates fwding rules on each node in the cluster, making sure any traffic coming to this IP, should go to IP of the pod.
- - - - once this is in place, wenevr a pod tries to reach IP of a svc, it is fwded to its associated pod's IP address, which is accessible frm any node in the cluster.
- - - - its not just IP, its IP & port combo. wenevr svcs r created/deleted, kube-pxy creates/deletes these rules.
- - - how r these rules created?
- - - - kube-pxy supports diff. ways, such as
- - - - - userspace, whr kube-pxy listens on a port for each svc & proxies connections to the pods
- - - - - by creating IPVS rules.
- - - - - 3rd & default option, using IP tables
- - - - pxy mode can b set by using --proxy-mode option, while configuring kube-pxy svc, if its not set, it defaults to IP tables.
kube-proxy --proxy-mode [userspace|iptables|ipvs] ...
- - - how IP tables r configured by kube-pxy & how u can view them on nodes?
- - - - Range for IP, assigned to new svc, is defined in --service-cluster-ip-range in kube API svr, which is default set to 10.0.0.0/24
kube-api-server --service-cluster-ip-range ipNet
- - - - rules creates for kube-pxy can b seen by executing
iptables -L -t nat | grep <svcName>
cat /var/log/kube-proxy.log # <== will show wht prxy kind is used, like iptables, userspace or ipvs


What network range are the nodes in the cluster part of?
controlplane ~ ➜  ip addr | grep eth0
3197: eth0@if3198: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default
    inet 10.14.81.3/24 brd 10.14.81.255 scope global eth0

controlplane ~ ➜

What is the range of IP addresses configured for PODs on this cluster?
controlplane ~ ✖ ip addr | grep weave
4: weave: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1376 qdisc noqueue state UP group default qlen 1000
    inet 10.244.0.1/16 brd 10.244.255.255 scope global weave

controlplane ~ ✖ kubectl logs -n kube-system weave-net-tbjvj  weave | grep -i ipalloc-range
INFO: 2023/01/29 04:51:16.622723 Command line options: map[conn-limit:200 datapath:datapath db-prefix:/weavedb/weave-net docker-api: expect-npc:true http-addr:127.0.0.1:6784 ipalloc-init:consensus=0 ipalloc-range:10.244.0.0/16 metrics-addr:0.0.0.0:6782 name:12:a8:47:12:a9:79 nickname:controlplane no-dns:true no-masq-local:true port:6783]
controlplane ~ ➜

What is the IP Range configured for the services within the cluster?
controlplane ~ ➜  cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range
    - --service-cluster-ip-range=10.96.0.0/12
controlplane ~ ➜

What type of proxy is the kube-proxy configured to use?
controlplane ~ ✖ kubectl logs -n kube-system kube-proxy-zpfz4
I0129 04:51:05.612483       1 node.go:163] Successfully retrieved node IP: 10.14.81.3
I0129 04:51:05.612612       1 server_others.go:109] "Detected node IP" address="10.14.81.3"
I0129 04:51:05.612635       1 server_others.go:535] "Using iptables proxy"
I0129 04:51:05.635160       1 server_others.go:176] "Using iptables Proxier"

How does this Kubernetes cluster ensure that a kube-proxy pod runs on all nodes in the cluster?
controlplane ~ ✖ kubectl describe po kube-proxy-zpfz4 -n kube-system
Name:                 kube-proxy-zpfz4
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      kube-proxy
Node:                 controlplane/10.14.81.3
Start Time:           Sat, 28 Jan 2023 23:51:04 -0500
Labels:               controller-revision-hash=78545cdb7d
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   10.14.81.3
IPs:
  IP:           10.14.81.3
Controlled By:  DaemonSet/kube-proxy

controlplane ~ ➜  kubectl get ds -n kube-system
NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-proxy   2         2         2       2            2           kubernetes.io/os=linux   66m
weave-net    2         2         2       2            2           <none>                   66m

controlplane ~ ➜

221. 17:03  hrs

[] ClusterDNS: DNS cfg on Linux
- Alrdy done:
- - What is DNS, /etc/hosts n nslookup, dig utility, recorded types A & CNAME, Domain Name Hierarchy
- - How to setup own DNS svr, using coreDNS?
- What names r assigned to what objects?
- svc DNS records
- pod DNS records
- wht r different way u can reach 1 pod to another?
- 3-node cluster: wid pods n svcs deployed on each node. Each nodes has node name, IP assigned to it
- Node names & IP in the cluster r registered in DNS svr in org
- DNS resolution within cluster, b/w diff. components in cluster, like pods & svcs
- k8s deploys a built-in DNS svr, by default, wen a cluster is setup
- How DNS helps pods resolve other pods & svcs in cluster?
- Ex: consider 2 pods & a svc. To make pod1 accessible to pod2, create a svc tht gets an IP.
- - wenevr svc is created, k8s DNS svc creates a record for the svc, it maps svc name to IP address
- - so, within cluster, any pod can reach this svc, using its svc name
- - - if svc n calling pod r in same ns, curl http://<svcName>. if both r in diff. ns, curl http://<svcName>.<ns>
- - all svcs r grouped together into another sub-domain called svc.
- - - so now, svc should b accessible as curl http://<svcName>.<ns>.svc
- - finally, all svcs n pods r grouped together into a root domain for cluster, set to cluster.local, by default
- - - so now, final FQDN URL for svc to b accessible will be curl http://<svcName>.<ns>.svc.cluster.local
- So, thats how svcs r resolved within cluster
- wht abt pods? records for pods r not created by default, but tht can be enabled explicitly.
- - once enabled, pods can't be accessible by name. k8s generates name for pod by replacing its associated UP wid dashes, ns remains same, type is set to pod instead of svc, root domain stays same
- - So FQDN URL for pod will become http://<IPwidDashes>.<ns>.pod.cluster.local

[] coreDNS: coreDNS intro, how to get started wid one of DNS solutions, i.e., coreDNS
- How k8s implements DNS in the cluster?
- core concept of DNS: add entries of pod name & IP address in DNS svr, and then add nameserver <IP> entry in each pod's /etc/resolv.conf file
- k8s does this for svcs but not for pods. for pods, k8s forms hostnames by replacing dots wid dashes in IP of pod.
- k8s implements DNS in the same way. it deploys a DNS svr within the cluster. prior to version 1.12, DNS svr was known as kube-dns. from v1.12 onwards, recommended DNS svr is coreDNS.
- How is coreDNS setup in the cluster?
- - coreDNS is deployed as a pod in kube-system ns in k8s cluster.
- - - deployed as 2 pods, for redundancy as part of replicaSet. They r actually a replicaSet within a deployment.
- - This pod runs coreDNS executable. coreDNS requires a cfg file, located under /etc/coredns/Corefile
- - within this file, we hv a no. of plugins (1st word in each line is a plugin name) configured, for handling errors, reporting health, monitoring metrics, cache etc
- - - the plugin tht makes coreDNS work wid k8s , is kubernetes plugin & tht is whr top-lvl domain name of the cluster is set, i.e., cluster.local. So,each record in coreDNS, falls under this domain.
- - - wid kubernetes plugin, thr r multiple options. Pods option is responsible for creating a record for pods in the cluster.
- - This Corefile is passed in to the pod as a configMap object. Tht way if u need to modify this cfg, u can edit configMap object
- - wid coredns pod up n running, it watches k8s cluster for new pods/svcs & each time a new pod/svc is created, it adds a record for it in its DB.
- - nxt step is for pods to point to coreDNS svr. Wht address do pods use to reach DNS svr?
- - - wen we deploy coreDNS solution, it also creates a svc to make it avl to other components within the cluster. svc is named as kube-dns by default. IP of this svc is configured as name svr on pods' /etc/resolv.conf
- - - DNS configurations on pods r done by k8s automatically wen pods r created by kubelet.
- - - - in the cfg file of kubelet, u'll see IP of DNS svr & domain in it as
....
clusterDNS:
- 10/96.0.10
clusterDomain: cluster.local
...
- - - once pods r configured with correct name svr, u can now resolve other pods n svcs. resov.conf also has a search entry as
search default.svc.cluster.local svc.cluster.local cluster.local
- - - this allows u to find svc using any name, i.e., web-svc or web-svc.default or web-svc.default.svc etc
- - - resolv.conf only has search entries for svc, u wont b able to reach a pod the same way. For pod, need to specify full FQDN of the pod.

=====================
/etc/coredns/Corefile
=====================
.:53 {
  errors
  health
  kubernetes cluster.local in-addr.arpa ip6.arpa {
    pods insecure
    upstream
    fallthrough in-addr.arpa ip6.arpa
  }
  prometheus :9153
  proxy . /etc/resolv.conf
  cache 30
  reload
}
=====================

Where is the configuration file located for configuring the CoreDNS service?
controlplane ~ ✖ ps -eaf | grep coredns
root        5854    5584  0 02:25 ?        00:00:01 /coredns -conf /etc/coredns/Corefile
root        5888    5638  0 02:25 ?        00:00:01 /coredns -conf /etc/coredns/Corefile
root       10204    8009  0 02:32 pts/1    00:00:00 grep coredns
controlplane ~ ➜

controlplane ~ ➜  kubectl describe deploy coredns -n kube-system | grep -A2 -i arg
    Args:
      -conf
      /etc/coredns/Corefile

controlplane ~ ➜

What is the root domain/zone configured for this kubernetes cluster?
controlplane ~ ➜  kubectl describe cm coredns -n kube-system
Name:         coredns
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
Corefile:
----
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}


BinaryData
====

Events:  <none>

controlplane ~ ➜


[] ingress n/wing:
- helps app to be accessed using a single external ly accessible URL, tht can be configured to route to diff. svcs, within ur cluster, based on URL path or host path; alongwith implementing SSL security as well
- Consider ingress to be layer 7 LB, built into k8s cluster, tht can b configured using native k8s primitives, just like any other object in k8s
- even wid ingress, application still needs to be exposed to make it accessible outside the cluster. So, we still need to publish it as a nodePort (on-prem) or as a Loadbalancer type (to leverage Cloud's native LB), which is just 1-time cfg
- perform all ur LB authentication, SSL & URL based routing cfg on ingress ctrller.
- How does Ingress work? Where is it? How can u see it? How can u configure it? How does it load balance? How does it implement SSL?
- Without ingress, how would u do all of this?
- - reverse proxy will need to be used or a load-balancing solution like nginx or HA-Proxy or Traefik;
- - deploy it on k8s cluster & configure it to route traffic to other svcs
- - - cfg involves defining URL routes, configuring SSL cert etc
- ingress is implemented by k8s in kind of same way.
- - Ingress ctrller: its basically one of the three supported solutions that is deployed, i.e., nginx,HA-Proxy, GCE (Google's layer-7 HTTP LB), Contour, Istio or Traefik.
- - - k8s cluster does NOT come wid an ingress ctrller by default, u must deploy one 1st
- - - GCE & nginx r currently supported & maintained by k8s project.
- - - These ingress ctrllers r not just another LBs or nginx svrs. LB components r just part of it.
- - - ingress ctrllers hv additional intelligence built into them to monitor k8s cluster for new definitions or ingress resources & configure nginx svrs accordingly.
- - - - for ingress ctrller to do this, create a svc a/c wid right set of permissions (correct roles & role bindings)
- - - nginx svr is deployed as just another deployment in k8s.
=====================================================================================
nginx-ingress-controller.yaml
=====================================================================================
apiVersion: extensions/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      name: nginx-ingress
    spec:
      containers:
      - name: nginx-ingress-controller
        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 # The nginx image version used, is a spl build of nginx, built specifically to b used as an ingress ctrller in k8s.


      args:
        - /nginx-ingress-controller                # So it has its own set of reqmnts. Within this image, nginx program is stored at location /nginx-ingress-controller. So, this location must b passed as an arg.
        - --configmap=<pod_ns>/nginx-configuration # in order to decouple these cfg data frm nginx ctrller image, create a configMapobject & pass that in.
      env:                                         # also pass 2 env variables tht contain pod's name & ns its deployed to. nginx svc requires these to read cfg data from within the pod
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
      ports:                                       # specify ports used by ingress ctrller, 80 & 443
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
=====================================================================================

=====================================================================================
nginx-configuration.yaml # nginx has a set of cfg options like path to store logs, keep alive threshold, ssl settings, session timeout etc.
=====================================================================================
apiVersion: v1
kind: ConfigMap # this configMap need not hv any entries at this point, a blank object will do. Creating one makes it easy to modify cfg in future.
metadata:
  name: nginx-configuration
=====================================================================================

=====================================================================================
nginx-ingress.yaml # we then need a svc, of type nodePort, to expose ingress ctrller to external world, by using nginx's label selector to link svc to deployment.
=====================================================================================
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
  selector:
    name: nginx-ingress
=====================================================================================

=====================================================================================
nginx-ingress-sa.yaml
=====================================================================================
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
=====================================================================================

[] To summarize, wid the following 4 accomplished, we should b ready wid ingress ctrller in its simples form.
- wid a deployment of nginx ingress image,
- a svc to expose it
- a configMap to feed nginx cfg data
- svc a/c wid right perms to access all these objects

- - Ingress resources: set of rules tht r specified to configure ingress ctrller. These r created using definition files, like ones used to create po, deploy & svc
- - - u can configure rules to say like
- - - - fwd all incoming traffic to single app
- - - - route traffic to different apps, based on URL or domain name itself
- - - - - u can mix n match domain-name n then path-specific rules under it.
- - - once ingress resource is created, it routes all incoming traffic based on defined rules.
=====================================================================================
ingress-wear.yaml
=====================================================================================
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    serviceName: wear-service # for single backend
    servicePort: 80
=====================================================================================

=====================================================================================
ingress-wear-watch.yaml
=====================================================================================
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http: # under HTTP rule, we can specify different paths, wid backend details under each path
      paths:
      - path: wear
        backend:
          serviceName: wear-service
          servicePort: 80
      - path: watch
        backend:
          serviceName: watch-service
          servicePort: 80
  - host: ab1.abc.com # domanin name based routing
      http:
        paths:
          backend:
            serviceName: ab1-service
            servicePort: 80
    - host: ab2.abc.com
        http:
        paths:
          backend:
            serviceName: ab2-service
            servicePort: 80
=====================================================================================

kubectl create -f ingress-wear.yaml
kubectl get ingress

- - - default backend: if a user tries to access a URL tht doesn't match any of the rules, then user is routed to the svc specified as default backend, named as default-http-backend
- - - must remember to deploy a such a svc to handle this scenario to display customized 404 PNF error page

Article: Ingress
In this article, we will see what changes have been made in previous and current versions in Ingress.
Like in apiVersion, serviceName and servicePort etc.
Now, in k8s version 1.20+ we can create an Ingress resource from the imperative way like this:-
Format - kubectl create ingress <ingress-name> --rule="host/path=service:port"
Example - kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"
Find more information and examples in the below reference link:-
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-
References:-
https://kubernetes.io/docs/concepts/services-networking/ingress
https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types

Ingress - Annotations and rewrite-target
Different ingress controllers have different options that can be used to customise the way it works.
NGINX Ingress controller has many options that can be seen here.
I would like to explain one such option that we will use in our labs.
The Rewrite target option.
Our watch app displays the video streaming webpage at http://<watch-service>:<port>/
Our wear app displays the apparel webpage at http://<wear-service>:<port>/
We must configure Ingress to achieve the below.
When user visits the URL on the left, his request should be forwarded internally to the URL on the right.
Note that the /watch and /wear URL path are what we configure on the ingress controller so we can forwarded users to the appropriate application in the backend.
The applications don't have this URL/Path configured on them:
http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/
http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/

Without the rewrite-target option, this is what would happen:
http://<ingress-service>:<ingress-port>/watch --> http://<watch-service>:<port>/watch
http://<ingress-service>:<ingress-port>/wear --> http://<wear-service>:<port>/wear

Notice watch and wear at the end of the target URLs.
The target applications are not configured with /watch or /wear paths.
They are different applications built specifically for their purpose, so they don't expect /watch or /wear in the URLs.
And as such the requests would fail and throw a 404 not found error.
To fix that we want to "ReWrite" the URL when the request is passed on to the watch or wear applications.
We don't want to pass in the same path that user typed in.
So we specify the rewrite-target option.
This rewrites the URL by replacing whatever is under rules->http->paths->path which happens to be /pay in this case with the value in rewrite-target.
This works just like a search and replace function.
For example: replace(path, rewrite-target)
In our case: replace("/path","/")
====================================================
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /pay
        backend:
          serviceName: pay-service
          servicePort: 8282
====================================================

In another example given here, this could also be:
replace("/something(/|$)(.*)", "/$2")
====================================================
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
  name: rewrite
  namespace: default
spec:
  rules:
  - host: rewrite.bar.com
    http:
      paths:
      - backend:
          serviceName: http-svc
          servicePort: 80
        path: /something(/|$)(.*)
====================================================

in which ns ingress resource is deployed?
controlplane ~ ✖ kubectl get ingress -A
NAMESPACE   NAME                 CLASS    HOSTS   ADDRESS          PORTS   AGE
app-space   ingress-wear-watch   <none>   *       10.108.128.125   80      6m5s

controlplane ~ ➜  kubectl describe ingress ingress-wear-watch -n app-space
Name:             ingress-wear-watch
Labels:           <none>
Namespace:        app-space
Address:          10.108.128.125
Ingress Class:    <none>
Default backend:  <default>
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /wear    wear-service:8080 (10.244.0.4:8080)
              /watch   video-service:8080 (10.244.0.5:8080)
Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /
              nginx.ingress.kubernetes.io/ssl-redirect: false
Events:
  Type    Reason  Age                    From                      Message
  ----    ------  ----                   ----                      -------
  Normal  Sync    6m58s (x2 over 6m58s)  nginx-ingress-controller  Scheduled for sync

controlplane ~ ➜

You are requested to change the URLs at which the applications are made available.
Make the video application available at /stream.

controlplane ~ ➜  kubectl edit ingress ingress-wear-watch -n app-space
ingress.networking.k8s.io/ingress-wear-watch edited

controlplane ~ ➜

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282

229. 18:13 hrs

ingress ctrller tasks for setup:
- 1 ns
- 2 sa
- 2 roles, 1 role for each sa
- 2 rolebindings, to attach each role to associated sa
- deploy ingress ctrller
- ingress resource object
Let us now deploy the Ingress Controller. Create the Kubernetes objects using the given file.
The Deployment and it's service configuration is given at /root/ingress-controller.yaml. There are several issues with it. Try to fix them
=========================
ingress-controller.yaml
=========================
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: k8s.gcr.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
=========================

Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.
Also, make use of rewrite-target annotation field: -
nginx.ingress.kubernetes.io/rewrite-target: /
Ingress resource comes under the namespace scoped, so don't forget to create the ingress in the app-space namespace.

======================
ingress-resource.yaml
======================
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port:
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080
======================


[] k8s Cluster Design, Installation, Configuration & Validation:

- Design a Cluster: Questions to be asked before designing a cluster:
- - Purpose: whts the purpose of designing this cluster?
- - - Education or Learning?
- - - - Solution based on Minikube or,
- - - - single node cluster deployed using kubeadm on local VMs or cloud providers like GCP/AWS
- - - Dev & Testing
- - - - Multi-node cluster wid single Master & multiple workers
- - - - Setup using kubeadm tool on-prem or quickly provision a cluster on GCP/AWS/AKS
- - - Hosting PRD Apps
- - - - Highly Avl multi-node cluster wid multiple master nodes
- - - - kubeadm or GCP or kOps on AWS or other supported platforms
- - - - Max supported up to
- - - - - 5k nodes
- - - - - 150k pods in the cluster
- - - - - 300k total containers
- - - - - 100 pods per node
- - - - Depending on size of cluster, resource reqmnt of node varies on AWS automatically select right-sized node based on no. of nodes in the cluster
=========================================
Nodes   EC2 ins type automatic selection based on no. of nodes.
                      These no.s can be used as base for on-prem
=========================================
1-5     m3.medium  wid 1  vCPU x 3.75 GB
6-10    m3.large   wid 2  vCPU x 7.5  GB
11-100  m3.xlarge  wid 4  vCPU x 15   GB
101-250 m3.2xlarge wid 8  vCPU x 30   GB
251-500 c4.4xlarge wid 16 vCPU x 30   GB
>501    c4.8xlarge wid 32 vCPU x 60   GB
=========================================
- - Cloud or On-prem? platform to be managed by cloud provider or self-hosted?
- - - Cloud providers provide 1-click cluster upgrade feature, which makes it vry easy to maintain cluster
- - - kOps in AWS, is used to deploy cluster in AWS
- - Workloads: wht kind of workloads r going to run on this cluster?
- - - How many? how many apps to be hosted on the cluster? Few or many?
- - - What kind? What kind of apps to be hosted on the cluster?
- - - - Web
- - - - Big Data / Analytics
- - - Depending on the workloads configured, ur nodes & disconfigurations will differ.
- - - - Storage:
- - - - - For High performance workloads, rely on SSD Backed Storage
- - - - - For Multiple concurrent connections, use n/w based storage
- - - - - Use Persistent shared volumes for shared access across multiple pods
- - - - - Consider defining diff. classes of storage & allocating right class to the right apps.
- - - - - label nodes wid specific disk types
- - - - - use node selectors to assign apps to nodes wid specific disk types
- - - - Nodes forming k8s cluster:
- - - - - can be virts or physical machines
- - - - - min. of 4-node cluster (size-based workload)
- - - - - master vs worker nodes. master can host workloads but as a best practices, limit master nodes to hosting ctrl plane components only, splly in PRD Environment
- - - - - - Deployment tools like kubeadm, prevent workloads from being hosted on master nodes, by adding a taint to master node.
- - - - - use 64-bit OS with Linux x86_64 arch on nodes
- - - App Resource Requirments: Depending on kind of app, resource reqmnts vary
- - - - CPU Intensive
- - - - Memory Intensive
- - - Traffic: wht kind of n/w traffic is expected for these apps?
- - - - Heavy Traffic: continous heavy traffic
- - - - Burst Traffic
- In large clusters, separate etcd frm master node to its own cluster nodes

[] Choosing k8s infrastructure: Diff. choices avl for infra to host k8s cluster
- k8s can be deployed in various systems in diff. ways, starting from ur laptop to physical or virts within org as well as in cloud.
- Depending on requirements, ur cloud ecosystem & kindn of apps to deploy, choose one of these solutions.
- - Laptop / Local Machine: install binaries manually & setup local cluster, but tht can b tedious.
- - - - Rely on solution tht automates entire setup & sets up cluster within minutes
- - - - - Minikube deploys a single node cluster easily. It relies on 1 of virtualization s/w like Oracle VirtualBox to create VMs tht runs k8s cluster components
- - - - - kubeadm tool can b used to deploy single node or multi-node cluster real quick. But for this, u must provision reqd hosts wid supported cfg urself.
- - - - - Diff. b/w Minikube & kubeadm is tht Minikube provisions a VM wid supported cfgs by itself, whereas kubeadm expects VMs to be provisioned alrdy
- - - On Windows, k8s cant b setup as thr r no windows binaries for k8s.
- - - - must rely on virtualization s/w like hyperV or VMWare WorkStation or VirtualBox to create Linux VMs, on which k8s can b run.
- - - - solutions r avl to run k8s components as docker containers on Windows VMs,
- - - - but even then, docker images r Linux-based & under the hood, they run on a small Linux OC, created but HyperV for running Linux Docker containers
- - Production Cluster: main ways to to get started wid k8s cluster, both in pvt & public cloud environments are:-
- - - Turnkey Solutions: u provision reqd VMs & use tools/scripts to configure k8s clusters on them. you r responsible for
- - - - maintaining those VMs & patching them, upgrading them etc
- - - - - provision VMs
- - - - - configure VMs
- - - - - use scripts to deploy cluster
- - - - - maintain VMs urself
- - - - - Cluster mgmt & maintenance r mostly made easy using these tools & scripts
- - - - - Ex: deploying a k8s cluster on AWS using kOps tool
- - - - k8s certified solutions: All these solutions make it easy to deploy & manage k8s cluster privately within org. u must hv few VMs wid supported cfgs in place.
- - - - - Openshift (open-src container app platform & is built on top of k8s) is a popular on-prem k8s platform by RedHat.
- - - - - - It provides a set of additional tools & UI, to create & manage k8s cluster; & easily integrate it wid CI/CD pipelines etc
- - - - - CloudFoundry Container Runtime is another open-src project tht helps in deploying & managing highly avl k8s clusters, using their open-src tool called bosh
- - - - - VMware Cloud PKS: good for evaluation to leverage existing VMware environment
- - - - - Vagrant provides set of useful scripts to deploy k8s cluster on diff. cloud svc providers
- - - Hosted/Managed Solutions:
- - - - k8s as a svc solution, whr cluster alongwith reqd VMs r deployed by provider & k8s is configured by provider
- - - - - K8s-As-A-Svc
- - - - - provider provisions VMs
- - - - - Provider installs k8s
- - - - - Provider maintains VMs
- - - - - Ex: EKS
- - - - EKS, AKS, GKE, OpenShift Online whr u can gain access to fully functional k8s cluster online

[] Configure High Availability:
- If u lose master node in cluster, as long as worker nodes r up & containers r alive, apps still continue to run & users can still access app.
- Issues witnessed wid loss of master:
- - No replicaSet to re-create pod if any pod in worker node dies
- - ctrllers absent to ensure current state = desired state n launch new nodes
- - schedulers absent to provide details on which node to launch new pod
- - API svr not avl, cant access cluster externally thru kubectl tool or thru API for mgmt purposes
- To avoid all this, consider multiple master nodes in HA in PRD Environment
- HA gives u redundancy across each component in cluster to avoid single point of failure
- HA: running multiple instances of same ctrl plane components,
- - are the ctrl plane components going to do same thing twice?
- - how do they share work among themselves?
- API svr is responsible for recving requests & processing them or providing info abt cluster.
- - API svr works on 1 request at a time.
- - Active-Active mode: So, API svrs on HA cluster, can b alive & running at same time in Active-Active mode
- kubectl utility talks to API svr to get things done. We point kubectl to reach master node at port 6443.
- - Thats whr API svr listens n this is configured in kubeconfig file
- - wid 2 masters, whr to point kubectl to?
- - - we can send request to either one of API svrs but NOT to both API svrs.
- - - - Better to hv LB configured in front of master nodes tht splits traffic b/w API svrs.
- - - - Use nginx or HA Proxy or any other LB for this purpose
- - - Then point kubectl utility to tht LB.
- scheduler & ctrller mgr: These r ctrllers tht watch state of cluster n take actions.
- - Ex: ctrller mgr consists of ctrllers like replication ctrller that is constantly watching state of pods & taking necessary actions like creating a pod wen 1 pod fails, etc
- - Active-Standby mode: if multiple instances of these run in parallel, then they might duplicate actions, resulting in more pods than actually needed. So, these both MUST run in Active-Standby mode
- - - who decides which among the two is active & which is passive?
- - - - This is done thru leader election process. How does that work?
- - - - - wen ctrller mgr is configured, --leader-elect is set to true, by default. Wid this option,
- - - - - - Wen ctrller mgr process starts, it tries to gain a lease or lock on an endpoint object in k8s, named as kube ctrller mgr endpoint.
- - - - - - Whichever process 1st updates the endpoint wid this info, gains the lease & becomes active of the two. It holds the lock for --leader-elect-lease-duration default set to 15s, lease duration specified
- - - - - - Active process then renews lease evry 10 secs, wid --leader-elect-renew-deadline default set to 10s. Both processes try to become leader, evry 2 secs, set by --leader-elect-retry-period default set to 2s.
- - - - - - tht way, if 1 process fails if 1st master crashes, 2nd process can acquire the log & become master.
- - scheduler follows same approach wid same set of options.
- etcd: two topologies to configure etcd in k8s
- - Stacked ctrl plane nodes topology: etcd part of master nodes. easier to setup, manage, wid fewer svrs but risks during failures.
- - - if 1 master node goes down, both etcd member & ctrl plane instances r lost, redundancy is compromised.
- - External etcd svrs topology: etcd is separated frm ctrl plane nodes & runs on its own set of svrs. less risky, harder to setup & requires twice no. of svrs for etcd nodes.
- - - failure of 1 master node doesn't impact etcd cluster & data it stores.
- - API svr is the only component tht talks to etcd svrs, need to make sure API svr is pointing to right address of etcd svrs, irrespective of topology used.
- - etcd is a distributed system. So, API svr can reach etcd svr at any of its instances. u can read or write data thru any of avl etcd svr instances.
- - - this is why we specify a list of etcd svrs in API svr cfg under --etcd-servers wid comma-separated values.

[] ETCD in HA:
- - What is ETCD?
- - - Distributed, reliable key-value store, tht is simple, secure n fast
- - What is Key-Value Store?
- - - a DB tht stores valyues in the form of documents or pages, each individual gets a document n all info abt tht individual is store in tht file.
- - - These files can be in any format or structure. Changes to 1 file, doesn't affect others. wen data gets complex, u transact in data formats like json or yaml
- - How to get started quickly?
- - - To install etcd on a svr, download latest binary, extract it, create reqd dir. structure, copy cert files generated for etcd, then configure etcd svc
wget -q --https-only "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.targ.gz"
tar -xvf etcd-v3.3.9-linux-amd64.tar.gz
mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
mkdir -p /etc/etcd /var/lib/etcd
cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
# while configuring etcd svc, whr we pass peer's info; thts how etcd knows tht it is part of cluster n whr its peers are
--initial-cluste peer-1=https://${PEER1_IP}:2380,peer-2=https://${PEER2_IP}:2380
- - How to operate ETCD?
- - - Once installed & configured, use etcdctl utility to store n retrieve data. etcdctl utility has 2 API versions, v2 n v3. cmds work diff. in each version. Set env variable
export ETCDCTL_API=3
etcdctl put name jas              # <== name is key, jas is value
etcdctl get name                  # <== use key as name to fetch value
etcdctl get / --prefix --key-only # <== to get list of all keys
- - What is a distributed system? Data Store across multiple svrs, maintaining identical copu of DB. u can read or write data to any instance.
- - - How does it ensures tht data on all nodes is consistent?
- - - - only 1 node is responsible for processing writes. Internally, 2 nodes elect a leader among them. Of the total instances, 1 node becomes leader & others become followers.
- - - - if write comes thru leader, leader processes writes. leader makes sure tht other nodes r sent a copy of changed data.
- - - - if writes came thru any of follower nodes, then they fwd the write to leader node internally & then leader processes writes & sends a copy back to follower nodes.
- - - a write is only considered complete, once leader gets consent from majority of follower nodes in the cluster.
- - RAFT Protocol
- - - etcd implements distributed consensus using RAFT protocol. Lets see how it works in a 3-node cluster.
- - - How do nodes elect a leader among themselves?
- - - - wen cluster is setup, 3 nodes r setup but they do not hv a leader elected.
- - - - RAFT algo uses random timers for initiating requests. 1st one to finish out timer, sends out a request to other nodes, requesting permission to be the leader.
- - - - The other mgrs, upon receiving request, respond wid their vote & the node assumes the leader role.
- - - - Now tht it is elected as leader, it sends out notifications @ regular intervals to other masters, informing them tht it is continuing to assume the role of leader.
- - - - in case if other nodes do not recv communication frm leader, nodes initiate a re-election process among themselves & a new leader is identified.
- - - - so, write is processed by leader & replicated to other nodes in the cluster.
- - - How does leader ensures that a write is propagated across all the instances?
- - - - write is considered complete if its written on majority (N/2 + 1) of nodes in cluster.
- - - - Quorum = N/2 + 1, minimum no. of nodes tht must b avl for cluster to function properly or make a successful write. For fractions/decimals, whole no. is considered.
- - - - - Quorum of 1 is 1 n 2 is 2. So, having 2 instances is same as having 1 instance. It doesnt offer u any real value as quorum cannot b met.
- - - Fault Tolerance = Total no. of instances - Quorum [i.e., no. of instances u can afford to lose while keeping cluster alive]
- - Best practices on no. of nodes
- - - This is why it is recommended to hv at least a 3-node cluster. This helps offer fault tolerance of at least 1 node. Recommended to hv odd. no. of nodes.
- - - wid even no. of nodes, thr is possibility of cluster failing during n/w segmentation.
- - - Having 5 nodes gives u sufficient fault tolerance. Anything beyond 5 is unnecessary.

237: 19:04 hrs

[] Important Update: Kubernetes the Hard Way
Installing Kubernetes the hard way can help you gain a better understanding of putting together the different components manually.
An optional series on this is available at our youtube channel here:
https://www.youtube.com/watch?v=uUupRagM7m0&list=PL2We04F3Y_41jYdadX55fdJplDvgNGENo
The GIT Repo for this tutorial can be found here: https://github.com/mmumshad/kubernetes-the-hard-way


[] Deployment steps with kubeadm tool to bootstrap k8s cluster:
- multiple machines should be provisioned for configuring a cluster
- designate 1 node as master & others as worker nodes
- install container runtime (docker) on all nodes
- install kubeadm tool on all nodes, tht helps us bootstrap k8s solution by installing n configuring all reqd components in right nodes, in right order
- initialize master svr. During this process, all reqd components r installed & configured on master svr
- Ensure n/w pre-requisites r met, once master is initialized & before joining worker nodes to master. normal n/w connectivity between mast n worker nodes is NOT sufficient.
- - k8s requires a spl n/wing solution b/w master & worker nodes, called as Pod n/w.
- join worker nodes to master node.

[] Resources
The vagrant file used in the next video is available here:
https://github.com/kodekloudhub/certified-kubernetes-administrator-course
Here's the link to the documentation:
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

[] Demo of k8s setup using kubeadm, provision VMs wid VirtualBox & Vagrant (both alrdy deployed is pre-requisite):
- use a vagrant file to automate provisioning of VMs, reqd for master & worker nodes.
- - vagrant status, vagrant up, vagrant ssh masternode; vagrant ssh node01; vagrant ssh node02 #(to test ssh connectivity)
- use kubeadm documentation page to setup k8s cluster, by 1st making sure tht the following pre-requisites r met :-
- - must hv supported OS wid min. 2GB RAM, wid n/w connectivity b/w VMs
- - Firewall & n/w security settings r in place, to avoid any connectivity issues.
- - - on all nodes, ensure that iptables is able to see bridged traffic, by making sure tht br_filter module is loaded before this step, by executing
lsmod | grep br_netfilter  # <== verifies if br_netfilter module is loaded
sudo modprobe br_netfilter # <== to load it manually.
- - Run below cmds to create new kernel pmtrs.
cat <<EOF | sudo tee /etc/sysctl.dk8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system
- - install container runtime (docker) on all nodes. Use documentation link to install docker
# setup s/w repos
sudo su # switch to root user
apt-get update && apt-get install -y \
apt-transport-https ca-certificate curl software-properties-common gnupg2
# setup docker's official GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
# Add docker APT repo
add-apt-repository \
"deb [arch=amd64] https://download.docker.com/linux/ubuntu \
$(lsb_release -cs) \
stable"
# install docker runtime, use latest version, exmple below may hv stale version
apt-get update && apt-get install -y \
containerd.io=1.2.13-1 \
docker-ce=5:19.03.8~3-0~ubuntu-$(lsb_release -cs) \
docker-ce-cli=5:19.03.8~3-0~ubuntu-$(lsb_release -cs)
# setup docker daemon
# for this, create a daemon.js file & then create a dir. for docker svc
cat > /etc/docker/daemon.js << EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
mkdir -p /etc/systemd/system/docker.service.d
# Execute systemctl reload n restart t o load & start docker svc
systemctl daemon-reload
systemctl restart docker
systemctl status docker.service # <== to verify if docker svc is running now
- - on all nodes, install k8s specific pkgs for kubeadm, kubelet, kubectl
- - - kubeadm tool is to bootstrap the cluster
- - - kubelet is the process thats responsible to manage pods & containers on the nodes
- - - kubectl utility is k8s CLI tool
# Execute below cmds to update repo n install reqd pkgs
sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubenetes.list
deb https://apt.kubernetes.io/kubenetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
- - configure kubeadm cluster creation process
- - - initialize ctrl plane node
- - - - 1st step is only applicable if HA cluster is being deployed.
- - - - choose a pod n/w add-on, decide wht IP-range to use for pod.
- - - - - use --pod-network-cidr=10.244.0.0/16 CIDR range
- - - - multiple n/w options avl for n/wing like calico, celium, weave net, contiv-CVPP, kube-router etc b/w pods in k8s cluster.
- - - - - choose n/w CIDR range (192.168.56.2), tht doesnt conflict wid n/w of nodes.
- - - - 3rd step can b ignored as well, if Docker is used as container runtime
- - - - 4th step: specify k8s API svr's IP as --apiserver-advertise-address=<IP>, for API svr to listen & be accessible to worker nodes or any other clients.
# the below cmd should be run ONLY on Master node, as thts whr all cluster components for ctrl plane need to be installed & initialized
kubeadm init --pod-network-cidr=10.244.0.0./16 --apiserver-advertise-address=192.168.56.2 #chk IP address by running ifconfig cmd n chk inet
# the above cmd runs a set of pre-flight checks to ensure all prerequisites r in place
# then it proceeds wid creation of security certs & installation of various components
- - - post installation tasks: run below cmds as regular user, NOT as root
# create a dir. under user's home dir & copy admin.conf file
# admin.conf file has necessary info & creds reqd to access k8s cluster using kubectl.
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube.config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl get no
NAME        STATUS    ROLES   AGE   VERSION
kubemaster  NotReady  master  114s  v1.18.3
# this is cuz pod n/w solution is not yet deployed
- - - deploy a pod n/w to cluster: use weave to create svc a/c, roles, role bindings, DaemonSets etc
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
- - - make worker nodes join the cluster
# Run this cmd ONLY on Worker nodes
kubeadm join 18=98.168.56.2:6443 --token <tokenID> \
-- discovery-token-ca-cert-hash sha256:<tokenContent>

Install the kubeadm and kubelet packages on the controlplane and node01.
Use the exact version of 1.26.0-00 for both.

These steps have to be performed on both nodes.
set net.bridge.bridge-nf-call-iptables to 1:

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system
The container runtime has already been installed on both nodes, so you may skip this step. Install kubeadm, kubectl and kubelet on all nodes:
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet=1.26.0-00 kubeadm=1.26.0-00 kubectl=1.26.0-00
sudo apt-mark hold kubelet kubeadm kubectl


Initialize Control Plane Node (Master Node). Use the following options:
apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
apiserver-cert-extra-sans - Set it to controlplane
pod-network-cidr - Set to 10.244.0.0/16

controlplane ~ ✖ ifconfig
cni0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 10.244.0.1  netmask 255.255.255.0  broadcast 10.244.0.255
        ether 4e:b4:fa:cb:23:4d  txqueuelen 1000  (Ethernet)
        RX packets 790  bytes 64690 (64.6 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 784  bytes 95964 (95.9 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
        inet 10.70.212.9  netmask 255.255.255.0  broadcast 10.70.212.255

controlplane ~ ➜  kubeadm init --apiserver-advertise-address=10.70.212.9 --apiserver-cert-extra-sans=controlplane --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.26.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'

[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.70.212.9]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [10.70.212.9 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [10.70.212.9 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key

[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file

[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet

[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"

[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"

[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s

[apiclient] All control plane components are healthy after 14.002237 seconds

[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace

[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster

[upload-certs] Skipping phase. Please see --upload-certs

[mark-control-plane] Marking the node controlplane as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]

[bootstrap-token] Using token: 9cauzd.o40hlpk9km0kcxdg
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace

[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key

[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:
  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 10.70.212.9:6443 --token 9cauzd.o40hlpk9km0kcxdg \
        --discovery-token-ca-cert-hash sha256:064fd799258befef46861bdb01291490176157a48b7094e3c99d16bfcdd1dc13

controlplane ~ ➜
controlplane ~ ➜  mkdir -p $HOME/.kube
controlplane ~ ➜  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
controlplane ~ ➜  sudo chown $(id -u):$(id -g) $HOME/.kube/config
controlplane ~ ➜

controlplane ~ ➜  ssh node01
Last login: Mon Jan 30 13:08:32 2023 from 10.70.212.9

root@node01 ~ ➜  kubeadm join 10.70.212.9:6443 --token 9cauzd.o40hlpk9km0kcxdg \
>         --discovery-token-ca-cert-hash sha256:064fd799258befef46861bdb01291490176157a48b7094e3c99d16bfcdd1dc13
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

root@node01 ~ ➜

Install a Network Plugin. As a default, we will go with flannel
On the controlplane node,
run the following command to deploy the network plugin:
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml


controlplane ~ ➜  kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.20.2/Documentation/kube-flannel.yml
namespace/kube-flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created

controlplane ~ ➜

Important Update: End to End Section
As per the CKA exam changes (effective September 2020), End to End tests is no longer part of the exam and hence it has been removed from the course.
If you are still interested to learn this, please check out the complete tutorial and demos in our YouTube playlist:
https://www.youtube.com/watch?v=-ovJrIIED88&list=PL2We04F3Y_41jYdadX55fdJplDvgNGENo&index=18


[] Troubleshooting:
- Application Failures:
- - Consider 2-tier app wid web & DB svr. Users report issues in accessing the app.
- - Start wid app front-end. Use standard ways of testing if ur app is accessible.
- - if its a web app, chk if web svr is accessible on IP of node port, using cURL
curl http://<websvr_IP>:<port>
- - Nxt, chk the svc. Has it discovered endpoints for the web pod?
kubectl describe svc web-service
- - - If not, chk the svc to pod discovery. compare selectors configured on the svc to the ones on the pod. make sure they match.
- - chk the pod itself to make sure its in running state. status of pod & no. of restarts, gives clues abt whether app is running or getting restarted.
- - chk events related to pod, using describe cmd
kubectl describe po web
kubectl logs web # <== chk logs of app, using logs cmd
- - if the pod is restarting due to failure, then logs in current version of pod, thats running current version of container,  may not reflect why it failed last time
- - - so, either watch these logs using -f command & wait for app to fail again or use previous option to view logs of previous pod
kubectl logs web -f --previous
- - chk status of DB svc & DB pod itself, chk logs of DB pod & look for any errors in DB

A simple 2 tier application is deployed in the alpha namespace.
It must display a green web page on success.
It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

Troubleshooting Test 1:
The service name used for the MySQL Pod is incorrect. According to the Architecture diagram, it should be mysql-service.
To fix this, first delete the current service: kubectl -n alpha delete svc mysql
Then create a new service with the following YAML file (or use imperative command):
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: alpha
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql
Create the new service: kubectl create -f <service.yaml>

Troubleshooting Test 2:
If you inspect the mysql-service in the beta namespace, you will notice that the targetPort used to create this service is incorrect.
Compare this to the Architecture diagram and change it to 3306. Update the mysql-service as per the below YAML:
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: beta
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql

Troubleshooting Test 3:
Service:
root@controlplane:~# kubectl -n gamma describe svc mysql-service | grep -i selector
Selector:          name=sql00001
root@controlplane:~#

Pod:
root@controlplane:~# kubectl -n gamma describe pod mysql | grep -i label
Labels:       name=mysql
root@controlplane:~#
As you can see the selector used is name=sql001 whereas it should be name=mysql.
Update the mysql-service to use the correct selector as per the below YAML:
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: gamma
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql

Troubleshooting Test 4:
Try accessing the web application from the browser using the tab called app. You will notice that it cannot connect to the MySQL database:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=sql-user; DB_Password=paswrd; 1045 (28000): Access denied for user 'sql-user'@'10.244.1.9' (using password: YES)
According to the architecture diagram, the DB_User should be root but it is set to sql-user in the webapp-mysql deployment.
Use the command kubectl -n delta edit deployments.apps webapp-mysql and update the environment variable as follows:
spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
This will recreate the pod and you should then be able to access the application.

Troubleshooting Test 5:
If you inspect the environment variable called MYSQL_ROOT_PASSWORD, you will notice that the value is incorrect as compared to the architecture diagram:
root@controlplane:~# kubectl -n epsilon describe pod mysql  | grep MYSQL_ROOT_PASSWORD
      MYSQL_ROOT_PASSWORD:  passwooooorrddd
root@controlplane:~#
Correct this by deleting and recreating the mysql pod with the correct environment variable as follows:
spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd
Also edit the webapp-mysql deployment and make sure that the DB_User environment variable is set to root as follows:
spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
Once the objects are recreated, and you should be able to access the application.

kubectl replace --force -f /tmp/kubectl-edit-2138433585.yaml
# instead of following 2 cmds
controlplane ~ ✖ kubectl delete pod/mysql -n zeta
pod "mysql" deleted
kubec
controlplane ~ ➜  kubectl create -f /tmp/kubectl-edit-2138433585.yaml
pod/mysql created
controlplane ~ ➜
controlplane ~ ➜  kubectl get pod/mysql -n epsilon -o yaml | grep -i pas
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd
controlplane ~ ➜  kubectl get pod/webapp-mysql-7cc9dcdffd-dbmfr -n epsilon -o yaml | grep -i pas
    - name: DB_Password
      value: paswrd
controlplane ~ ➜

Troubleshooting Test 6:
There are a few things wrong in this setup:
1. If you inspect the web-service, you will see that the nodePort used is incorrect.
This service should be exposed on port 30081 and NOT 30088.
root@controlplane:~# kubectl -n zeta get svc web-service
NAME          TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web-service   NodePort   10.102.190.212   <none>        8080:30088/TCP   3m1s
root@controlplane:~#

To correct this, delete the service and recreate it using the below YAML file:

apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: zeta
spec:
  ports:
  - nodePort: 30081
    port: 8080
    targetPort: 8080
  selector:
    name: webapp-mysql
  type: NodePort
2. Also edit the webapp-mysql deployment and make sure that the DB_User environment variable is set to root as follows:
spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
3. The DB_Password used by the mysql pod is incorrect. Delete the current pod and recreate with the correct environment variable as per the snippet below:
spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd
Once the objects are recreated, and you should be able to access the application.

kubectl config set-context --current --namespace=gamma

250. 20:21 hrs

- Ctrl Plane Failures:
- - start by chking status of nodes on cluster to make sure they r all healthy.
- - then chk status of pods to be running in kube-system
- - if components r deployed as svcs, then chk status of svcs
- - chk logs of crtl plane components
- - if ctrl plane components r hosted as svc, use journalctl utility to chk svc logs
sudo journalctl -u kube-apiserver

The cluster is broken. We tried deploying an application but it's not working. Troubleshoot and fix the issue.

Run the command: kubectl get pods -n kube-system and check the status of kube-scheduler pod. We need to check the kube-scheduler manifest file to fix the issue.
The command run by the scheduler pod is incorrect. Here is a snippet of the YAML file.
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    ....
Once this is corrected, the scheduler pod will be recreated.

- Worker Node Failures:
- - chk status of worker node, each node has a set of conditions tht can point us in a direction as to why a node may hv failed.
- - - Depending on status, Type status is set to True or False or Unknown
kubectl describe no <workerNode>
- - chk last heartbeat time field to find out time wen node might hv crashed.
- - chk status of kubelet
service kubelet status
sudo journal -u kubelet # <== chk kubelet logs for possible issues.
openssl x509 -in /var/lib/kubelet/worker-1.crt -text # <== chk kubelet certs to ensure those r not expired or tampered & they r part of correct group, & certs r issues by correct CA



- Networking Failures: Network Troubleshooting
[] Network Plugin in Kubernetes
--------------------
Kubernetes uses CNI plugins to setup network.
The kubelet is responsible for executing plugins as we specify the following parameters in kubelet configuration.
- cni-bin-dir:   Kubelet probes this directory for plugins on startup
- network-plugin: The network plugin to use from cni-bin-dir. It must match the name reported by a plugin probed from the plugin directory.
There are several plugins available and these are some.
1. Weave Net:
To install,
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
You can find details about the network plugins in the following documentation :
https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy
2. Flannel :
To install,
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
Note: As of now flannel does not support kubernetes network policies.
3. Calico :
To install,
   curl https://docs.projectcalico.org/manifests/calico.yaml -O
Apply the manifest using the following command.
      kubectl apply -f calico.yaml
Calico is said to have most advanced cni network plugin.
In CKA and CKAD exam, you won't be asked to install the CNI plugin. But if asked you will be provided with the exact URL to install it.
Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.

[] DNS in Kubernetes
-----------------
Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.

Memory and Pods
In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the
- number of Pods and Services in the cluster. Other factors include the
- size of the filled DNS answer cache, and
- rate of queries received (QPS) per CoreDNS instance.

Kubernetes resources for coreDNS are:
- a service account named coredns,
- cluster-roles named coredns and kube-dns
- clusterrolebindings named coredns and kube-dns,
- a deployment named coredns,
- a configmap named coredns and a
- service named kube-dns.

While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.
Port 53 is used for for DNS resolution.
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
This is the backend to k8s for cluster.local and reverse domains.
proxy . /etc/resolv.conf
Forward out of cluster domains directly to right authoritative DNS server.

Troubleshooting issues related to coreDNS
1. If you find CoreDNS pods in pending state first check network plugin is installed.
2. coredns pods have CrashLoopBackOff or Error state
- If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting.
- To solve that you can try one of the following options:
a)Upgrade to a newer version of Docker.
b)Disable SELinux.
c)Modify the coredns deployment to set allowPrivilegeEscalation to true:
kubectl -n kube-system get deployment coredns -o yaml | \
sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
kubectl apply -f -
d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.
- There are many ways to work around this issue, some are listed here:
- - Add the following to your kubelet config yaml:
resolvConf: <path-to-your-real-resolv-conf-file>
- - - This flag tells kubelet to pass an alternate resolv.conf to Pods.
- - For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.
- - Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.
- - A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS,
- - - for example forward . 8.8.8.8.
- - But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.
3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.
              kubectl -n kube-system get ep kube-dns
If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.

Kube-Proxy
---------
kube-proxy is a network proxy that runs on each node in the cluster.
kube-proxy maintains network rules on nodes.
These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.
In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.
kubeproxy is responsible for watching services and endpoint associated with each service.
When the client is going to connect to the service using the virtual IP, the kubeproxy is responsible for sending traffic to actual pods.
If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container.
Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and
we can override the hostname with the node name of at which the pod is running.
In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

Troubleshooting issues related to kube-proxy
1. Check kube-proxy pod in the kube-system namespace is running.
2. Check kube-proxy logs.
3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.
4. kube-config is defined in the config map.
5. check kube-proxy is running inside the container
# netstat -plan | grep kube-proxy
tcp        0      0 0.0.0.0:30081           0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 172.17.0.12:33706       172.17.0.12:6443        ESTABLISHED 1/kube-proxy
tcp6       0      0 :::10256                :::*                    LISTEN      1/kube-proxy
References:
Debug Service issues: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/
DNS Troubleshooting:  https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/

**Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.
Do the services in triton namespace have a valid endpoint? If they do, check the kube-proxy and the weave logs.
Does the cluster have a Network Addon installed?
Install Weave using the link: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
For example: curl -L https://github.com/weaveworks/weave/releases/download/latest_release/weave-daemonset-k8s-1.11.yaml | kubectl apply -f -

**Troubleshooting Test 2:**
There seems to be an issue with the Service Proxy. Inspect and Fix the kube-proxy daemonset.
Check logs of the kube-proxy pod. It appears that the daemonset is using a wrong configuration file.
Compare the configuration file used in the daemonset with the configmap for kube-proxy.
Edit the kube-proxy daemonset to correct the configuration file using kubectl -n kube-system edit ds kube-proxy.
The kube-proxy pod is not running. As a result the rules needed to allow connectivity to the services have not been created.
Check the logs of the kube-proxy pod as follows: -
# kube-proxy pod
kubectl -n kube-system logs <name_of_the_kube_proxy_pod>
The configuration file /var/lib/kube-proxy/configuration.conf is not valid. The configuration path does not match the data in the ConfigMap.
kubectl -n kube-system describe configmap kube-proxy shows that the file name used is config.conf which is mounted in the kube-proxy daemonset pod at the path /var/lib/kube-proxy/config.conf.
However in the DaemonSet, for kube-proxy, the command used to start the kube-proxy pod makes use of the path /var/lib/kube-proxy/configuration.conf.
Correct this path to /var/lib/kube-proxy/config.conf as per the ConfigMap and recreate the kube-proxy pod.
Here is the snippet of the correct command to be run by the kube-proxy pod:
spec:
    containers:
    - command:
        - /usr/local/bin/kube-proxy
        - --config=/var/lib/kube-proxy/config.conf
        - --hostname-override=$(NODE_NAME)
This should get the kube-proxy pod back in a running state.
You can list the config file within a kube-proxy pod as follows: -
kubectl exec -n kube-system -it kube-proxy-hb7n9 -- ls  /var/lib/kube-proxy/
Note: - In your lab, the kube-proxy pod name could be different.

root@controlplane ~ ✖ kubectl replace --force -f /tmp/kubectl-edit-1293023610.yaml
pod "kube-proxy-n7lv4" deleted
pod/kube-proxy-n7lv4 replaced


[] Pre-Requisites - JSON PATH
In the upcoming lecture we will explore some advanced commands with kubectl utility.
But that requires JSON PATH.
If you are new to JSON PATH queries get introduced to it first by going through the lectures and practice tests available here.
https://kodekloud.com/p/json-path-quiz
Once you are comfortable head back here:
I also have some JSON PATH exercises with Kubernetes Data Objects.
Make sure you go through these:
https://mmumshad.github.io/json-path-quiz/index.html#!/?questions=questionskub1
https://mmumshad.github.io/json-path-quiz/index.html#!/?questions=questionskub2

[] Advanced kubectl cmds:
- How kubectl works?
- - each time a kubectl cmd is triggered, it interacts wid API svr.
- - API svr talks in json language, i.e., returns info in json format.
- - kubectl converts returned info into human readable format
- - during this conversion, a lot of info, returned by API svr, is hidden, to make output readable, by showing only necessary details.
- - -o wide provides more details, but not all.
- - - Ex: resources capacity avl on nodes, taints on nodes, condition of node, h/w arch, images avl on node etc.
- - all this info can be seen in describe cmd, but wht if u want to see it like a report?
- - - Ex: to see nodes & their CPU counts in a tabular format, or list of nodes & taints set on them, the arch or list of pods & images used on nodes.
- - With JSONpath queries, u can filter & format output of cmd as u like
- How use JSONpath in kubectl? Follow these 4 steps
- need to know cmd tht will give reqd info in raw format.
- - Ex: if info is needed, regarding nodes, then use kubectl get nodes cmd; for pods, kubectl get pods.
- inspect output in json format, by adding -o json to cmd
- look thru structure of json doc and form JSONpath query to retreieve reqd infor for you
- - Ex: to get image, use .items[0].spec.containers.image ($ not mandatory, kubectl adds it automatically)
- use JSONpath query developed, along with same kubectl cmd, by using -o=jsonpath='{<query>}'
kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'
kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.architecture}'
kubectl get nodes -o=jsonpath='{.items[*].status.capacity.cpu}'
# all these cmds can be merged into single query cmd as well
kubectl get nodes -o=jsonpath='{.items[*].metadata.name} {"\n"} {.items[*].status.capacity.cpu} {"\t"} {.items[*].status.nodeInfo.architecture}'
# to print something like for each node, print nodeName \t print cpuCount \n
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name} {"\t"} {.status.capacity.cpu} {"\n"} {end}'
# u can also print custom columns liek >>
kubectl get nodes -o=custom-columns=<COLUMN_NAME>:<JSON_PATH>
# re-writing above composed jsonpath cmd as >>
kubectl get nodes -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu --sort-by=.metadata.name


261. 21:10 hrs

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
     controlplane ~ ✖ k get no -o=jsonpath="{.items[*].status.addresses[?(@type=='InternalIP')].address}"

Lightning Lab Introduction
This section has been created to give you hands-on practice
in solving questions of mixed difficulty in a short period of time.
This environment is valid for 60 minutes,
challenge yourself and try to complete all 5-8 questions within 30 minutes.
You can toggle between the questions but make sure that you
click on END EXAM before the timer runs out.
To pass, you need to secure 80%.


1. Upgrade the current version of kubernetes from 1.25.0 to 1.26.0 exactly using the kubeadm utility. Make sure that the upgrade is carried out one node at a time starting with the controlplane node. To minimize downtime, the deployment gold-nginx should be rescheduled on an alternate node before upgrading each node.
Upgrade controlplane node first and drain node node01 before upgrading it. Pods for gold-nginx should run on the controlplane node subsequently.

2. Print the names of all deployments in the admin2406 namespace in the following format:
DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE
<deployment name> <container image used> <ready replica count> <Namespace>
. The data should be sorted by the increasing order of the deployment name.
Example:
DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE
deploy0 nginx:alpine 1 admin2406
Write the result to the file /opt/admin2406_data.

controlplane ~ ➜  kubectl get deploy -n admin2406 -o=custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:spec.template.spec.containers[0].image,READY_REPLICAS:.spec.replicas,NAMESPACE:.metadata.namespace
DEPLOYMENT   CONTAINER_IMAGE   READY_REPLICAS   NAMESPACE
deploy1      nginx             1                admin2406
deploy2      nginx:alpine      1                admin2406
deploy3      nginx:1.16        1                admin2406
deploy4      nginx:1.17        1                admin2406
deploy5      nginx:latest      1                admin2406
controlplane ~ ➜

3. A kubeconfig file called admin.kubeconfig has been created in /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it.

4. Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update.
Image: nginx:1.16
Task: Upgrade the version of the deployment to 1:17

5. A new deployment called alpha-mysql has been deployed in the alpha namespace. However, the pods are not running. Troubleshoot and fix the issue. The deployment should make use of the persistent volume alpha-pv to be mounted at /var/lib/mysql and should use the environment variable MYSQL_ALLOW_EMPTY_PASSWORD=1 to make use of an empty root password.
Important: Do not alter the persistent volume.

6. Take the backup of ETCD at the location /opt/etcd-backup.db on the controlplane node.
controlplane ~ ➜  export ETCDCTL_API=3
controlplane ~ ➜  etcdctl snapshot save /opt/etcd-backup.db
Error: rpc error: code = Unavailable desc = transport is closing
controlplane ~ ✖

7. Create a pod called secret-1401 in the admin1401 namespace using the busybox image. The container within the pod should be called secret-admin and should sleep for 4800 seconds.
The container should mount a read-only secret volume called secret-volume at the path /etc/secret-volume. The secret being mounted has already been created for you and is called dotfile-secret.





268. 22:23 hrs

270. 23:07 hrs
